# Ruvector Domain Analysis

> **Priority**: HIGH | **Coverage**: ~11.1% (249/~2,200 DEEP est.) | **Status**: In Progress
> **Last updated**: 2026-02-17 (Session R101)

## 1. Current State Summary

The ruvector domain is a 76-crate Rust monorepo with 50+ npm packages providing distributed vector database infrastructure with self-learning capabilities. Actual codebase is ~400K LOC Rust (not the claimed 2M), representing 81 days of human-AI co-development at 10.3 commits/day — 6-20x faster than sustainable human-only velocity.

**Top-level verdicts:**

- **Hash-based embeddings are systemic across Rust + JS.** 7+ files default to character-sum or FNV-1a hash, not semantic embeddings. All "semantic search" using defaults is character-frequency matching.
- **Best code:** temporal-tensor (95%, production-ready), ruQu QEC (91→89% revised with subpoly_decoder drag), ruvllm kernels (90%, NEON SIMD), cognitum-gate-kernel (93%, rivals neural-network-impl), postgres SIMD (95-98%), ruqu-core (noise 96-98%, mitigation 95-98%, transpiler 95-98%).
- **Worst code:** ruvector-graph distributed module shows a new "transport-absent distributed protocol" pattern — algorithm state machines are correctly designed but no socket I/O exists (15-80% range). ruvector-graph Cypher parser has NO executor (30-35% overall). Postgres HNSW `connect_node_to_neighbors()` completely empty. Speculative decoding 2x SLOWER than vanilla. index_bench.rs (42%) theatrical benchmarking. subpolynomial/mod.rs (45-50%) false complexity claims. **subpoly_decoder.rs (35-40%) 3rd FALSE subpolynomial** — O(n²) greedy under "provable" claims.
- **Edge AI confirmed production-grade** — lora.rs (90-95%) real dual-SIMD LoRA with Q4/Q8 quantization, federated.rs (95-98%) BEST federated learning in project (Byzantine-robust, differential privacy, TopK compression).
- **ruvector-core advanced features confirmed genuine (85-93%):** product_quantization.rs (88-92%) real k-means++ and Lloyd's with ADC. conformal_prediction.rs (88-93%) valid split-conformal with Vovk et al. quantile. hyperbolic.rs (85-90%) genuine bipartite incidence and k-hop BFS citing HyperGraphRAG (NeurIPS 2025). tda.rs (60-70%) MISLABELED — implements graph metrics, not persistent homology (11th mislabeled file).
- **ruvector-graph distributed module (15-80% gradient):** shard.rs (70-80%) has real EdgeCutMinimizer multilevel Kernighan-Lin and xxh3/blake3 hashing. gossip.rs (45-55%) correct SWIM state machine, no transport. federation.rs (40-50%) real merge logic, execute_on_cluster always empty. coordinator.rs (30-35%) 2PC types defined, state machine frozen. rpc.rs (15-20%) all 4 RPC methods stubs, gRPC feature-gated out.
- **TWO independent LoRA implementations**: micro_lora.rs (training-focused, EWC++, sona) vs edge-net lora.rs (inference-focused, quantized, WASM).
- **Core HNSW wraps hnsw_rs** — vendored upstream v0.3.3 (NOT patched), but adds real value (SIMD, REDB, concurrency).
- **Attention crate is real** — 18+ implementations (Flash, Hyperbolic, MoE, Graph, Sheaf, OT, PDE) across 66 files, ~9,200 LOC. SIMD/Rayon features are no-ops.
- **Three distinct HNSW implementations:** ruvector-core (wrapper), micro-hnsw-wasm (novel `#![no_std]` <12KB), hyperbolic-hnsw (Poincare geometry).
- **TWO independent SIMD codebases:** ruvector-core (distance metrics for HNSW) and edge-net (NN inference — matmul, activations, quantization). Zero code sharing.
- **DUAL query languages confirmed:** Cypher (parser only, no executor) AND SPARQL (93-95% parser + 92% executor). Both property graphs and RDF triple stores supported.
- **AI co-authored explicitly** — commits credit "Claude Opus 4.5/4.6". Scope (GNN, quantum, FPGA, Raft, graph DB, 39 attention types, postgres ext) would take 2-3 years for experienced team.
- **R91 additions (5 files, ~3,935 LOC):** mmap.rs (88-92%) genuine memmap2 file-backed mmap + AtomicBitmap, for GNN training only (no HNSW integration). compress.rs (55-65%) 12TH MISLABELED FILE — named "graph compression" but implements embedding/tensor quantization with fake IEEE f16. speculative.rs (88-92%) EAGLE-style tree speculative decoding with novel lambda-guided confidence; sequential path verification (not true parallel). rope.rs (88-92%) correct RoPE with NTK-aware scaling and partial YaRN. kv_cache/legacy.rs (82-88%) RotateKV with FWHT; per-head scale stomping bug; no eviction policy.
- **R92 Hyperbolic HNSW confirmed GENUINE (88-95%):** First DEEP reads on the 63-file ruvector-hyperbolic-hnsw crate. hnsw.rs (88-93%) is a NATIVE implementation (NOT wrapping hnsw_rs), with Poincaré ball distance, tangent-space two-phase pruning, DualSpaceIndex with Reciprocal Rank Fusion. poincare.rs (90-95%) has mathematically correct Möbius operations, geodesic distance (Ganea et al. 2018), exponential/logarithmic maps, comprehensive numerical stability. lorentz_cascade.rs (90-93%) genuine Lorentz model attention using Minkowski inner product and Busemann functions — one of the most mathematically rigorous files in the ecosystem. hyperbolic-hnsw-wasm/lib.rs (88-92%) = **17th GENUINE WASM**. shard.rs (82-85%) confirms R90 "transport-absent" pattern but adds hyperbolic-aware radius partitioning.
- **R92 AIDefence PARTIALLY REAL (82-88%):** 28 genuine regex patterns for prompt injection/jailbreak/PII detection. Zero ML — the "AI" in the name is misleading. Behavioral analysis is a 4-feature stub. Policy verification config flag has ZERO implementation. Standalone — no integration with AgentDB, hooks, MCP, or claude-flow.
- **R92 CUDA flash_attention.rs MISPLACED (88-92% algorithm, 0% CUDA):** Lives in cuda-wasm crate but is a pure CPU Flash Attention v2 reference implementation. Zero CUDA kernels, device memory, or GPU patterns. Likely algorithmic ground truth for the GPU version.
- **R93+R94 MinCut-Gated-Transformer fully characterized (19 DEEP files, ~10K LOC, ~84% weighted avg):** energy_gate.rs (88-93%) **CONFIRMS R34 "MOST NOVEL"** — genuine 3-component EBT gate. Computational kernels (qgemm.rs, ffn.rs, spike_driven.rs, q15.rs) all 88-93% with real AVX2+NEON SIMD. R94 extends: packets.rs (90-95%) BEST — pure coherence interface with novel QuarantineUpdates isolation mode. state.rs (88-92%) genuine zero-alloc buffer layout. quantized_store.rs (88-92%) two-tier KIVI (4-bit/2-bit). mod_routing.rs (72-78%) is MoD (Mixture-of-Depths), NOT MoE — deterministic lambda-delta routing with duplicate functions and stride heuristic. quant4.rs (72-78%) RTN only, fully scalar. Infrastructure consistently 72-82%.
- **R97 additions — 9 files, ~3,055 LOC across 3 clusters:** Prime-Radiant Hyperbolic: hyperbolic/mod.rs (88-92%) has genuine Poincare geometry but brute-force O(n) search (no real HNSW back-end) and zero connection to the sheaf substrate. hyperbolic/energy.rs (82-87%) is a pure data container — all Riemannian math lives in adapter.rs, and merge() has a silent curvature compatibility bug. hyperbolic/adapter.rs (78-83%) is Poincare-only despite the "adapter" name (no Lorentz/hyperboloid conversion), HNSW is a documented brute-force stub, and exp_map uses Euclidean addition instead of Mobius addition (approximate geodesic stepping). SQL Attention: attention/multi_head.rs (88-92%) is genuine Rayon parallel MHA but has no W_Q/W_K/W_V projections (retrieval-focused). attention/scaled_dot.rs (90-93%) has correct QK^T/sqrt(d_k), numerically stable softmax, real simsimd SIMD with fallback. attention/mod.rs (82-87%) inflates "39 mechanisms" — AttentionType enum defines only 10 variants. MinCut remaining: spike.rs (88-92%) is a scheduling layer (NOT neuron dynamics), with FNV-1a novelty hashing and Q15 rate-tiering; config.rs (88-92%) covers only ~40% subsystem surface (missing energy gate, spike, quantization, KV cache, and MoD configs); kv_cache/tier.rs (90-93%) cleanly defines Hot/Warm/Archive=FP16/4-bit/2-bit but uses age-based (not access-frequency) tier assignment.
- **R96 MinCut KV Cache layer + SQL Attention + Trace (8 files, ~3,545 LOC, ~85% weighted avg, 27 DEEP files total in crate):** arena.rs (92%) GENUINE bump allocator with 64B cache-line alignment; aliasing UB hazard from multiple &mut slices off same backing Vec. squat.rs (78-84%) GENUINE SQuAt 2024 paper — Hadamard basis + Gram-Schmidt math correct, 2/4-bit packing complete, but calibrate() STUB ignoring calibration data entirely. kivi.rs (72-78%) GENUINE FWHT (correct butterfly recursion), but per-channel quantization claim FALSE — global min/max used regardless of QuantScheme; SIMD dequantization TODO placeholder. policy.rs (82-87%) age-based tier graduation only — NO H2O/attention-sink/LRU; RematerializationPolicy evaluate/tracker pressure diverge. hot_buffer.rs (82-87%) FIFO ring buffer for "FP16" (actually f32); pop_oldest() BUG (assumes oldest_pos=0, broken after first pop). SQL Attention: operators.rs (88-92%) GENUINE pgx extension — CORRECTS R91 AttentionService.ts skepticism; 5 #[pg_extern] functions real; only 3/10 attention types SQL-dispatchable. flash.rs (72-78%) algorithm GENUINE (KV-tiling, correct online softmax), NOT a PostgreSQL function — zero #[pg_extern]; false O(sqrt(N)) space claim; block_size_q dead code. trace.rs (88-92%) gate-decision diagnostics with stack-allocated [T;64] circular buffer; feature gate claim documentation-only (no #[cfg(feature="trace")] applied).
- **R98 additions — 6 files, ~1,385 LOC:** SQL Hyperbolic (ruvector-postgres) arc complete with poincare.rs (88-92%) and lorentz.rs (87-92%) — genuine geometry in Postgres, pure Rust library exposed via operators.rs, but lorentz.rs has CRITICAL missing manifold validation (off-hyperboloid points accepted silently) and no pgx annotations. MinCut lib.rs (90-95% EXCELLENT) confirms all 22+ DEEP modules are publicly accessible via 23-module crate root with 60+ exported items and 10 feature flags — MinCut crate now 24 DEEP files. norm.rs (55-65%) is the WEAKEST MinCut kernel — correct LayerNorm + RMSNorm math but ALL implementations are pure scalar despite SIMD siblings qgemm.rs and ffn.rs. Prime-Radiant Hyperbolic module COMPLETE (5/5 files, avg ~81%): depth.rs (78-82%) correct depth formula but curvature parameter stored yet never modulates calculations (effectively hardcoded -1.0), dead weight_multiplier(). config.rs (75-82%) HNSW params are dead config — confirmed never used (R97 brute-force search), Euclidean defaults (M=16, ef_construction=200) uncorrected for Poincare geometry.
- **R99 additions — 3 files, ~1,192 LOC:** ruvector-hyperbolic-hnsw crate now COMPLETE — tangent.rs (88-92%) implements genuine two-phase Poincare pruning (cheap Euclidean filter in tangent space + exact Poincare re-rank), TangentCache uses correct Fréchet mean centroid, crate internally consistent. GNN bindings both genuine: gnn-node/lib.rs (88-92%) real napi-rs #[napi] macros with solid FFI error handling, inference-only API surface; gnn-wasm/lib.rs (90-94%) **18th GENUINE WASM** — all exports delegate to real ruvector_gnn core algorithms via serde_wasm_bindgen. Both binding layers share the same inference-only surface (forward/compress/decompress/differentiable_search/hierarchical_forward): training APIs are absent from FFI. GNN node bindings have serde per-call overhead from JSON deserialization of layer configs in hierarchical_forward(). WASM bindings have inline cosineSimilarity() (95% correct numerics) and proper console_error_panic_hook initialization.
- **R100 additions — 12 files, ~874 LOC across 3 clusters:** ruvector-attention hyperbolic module COMPLETE (4/4 DEEP, 90-93% avg): hyperbolic_attention.rs (88-92%) fully working Poincare-ball attention with Fréchet mean aggregation; adaptive_curvature is a dead field (HIGH). mixed_curvature.rs (90-94%) MOST ARCHITECTURALLY NOVEL — splits embeddings into [euclidean || hyperbolic] halves, independent attention paths, ad-hoc softmax blend (imprecise but rescued by renormalization). poincare.rs (93-96%) mathematically rigorous Gyrovector operations — best-quality shared math foundation in the attention crate. mod.rs (100% wiring) flat pub re-export of all 4 submodules confirming lorentz_cascade is first-class. ruvector-hyperbolic-hnsw: lib.rs (ID 3276, 211 LOC, 90-93%) and error.rs (43 LOC, 95%+) add the library entry point and clean thiserror error enum — crate structurally complete with 5 submodules including tangent-space pruning and DualSpaceIndex. MinCut-Gated-Transformer: kv_cache/mod.rs (~90%) confirms ADR-004 Three-Tier KV cache (FP16 hot → 4-bit KIVI warm → 2-bit archive) with backward-compat legacy re-export; error.rs (~92%) hard no-panic contract with is_recoverable()/is_config_error() caller retry logic — HIGH quality; **attention/linear.rs (15-20%) LINEAR ATTENTION IS A PLACEHOLDER** — struct exists, docstring cites Katharopoulos 2020, but NO forward pass, NO ELU+1 kernel math, hidden behind `linear_attention` feature gate, deflates O(n) attention novelty claims; attention/mod.rs (~88%) feature-gated architecture confirms default = SlidingWindowAttention only, MinCut sparse path real but non-default, 4 academic citations. ruvector-postgres: hyperbolic/mod.rs (~92%) completes the SQL hyperbolic arc — DEFAULT_CURVATURE=-1.0, EPSILON=1e-8, MAX_NORM=1.0-1e-5, operators.rs is internal only.
- **R101 additions — 9 files, ~2,618 LOC, 72 findings (0C, 10H). DEEP: ~1,421→~1,430. v4-priority domain CLEARED.** Three arc completions in one session: **postgres hyperbolic/ COMPLETE (4/4 DEEP, 92-95%)** — operators.rs adds 8 pg_extern SQL functions (poincare_distance, lorentz_distance, mobius_add, exp_map, log_map, coordinate conversions) all immutable+parallel_safe with 11 pg_tests; combined with R98 H70 (lorentz.rs accepts off-hyperboloid points), the full postgres hyperbolic SQL API has ZERO manifold validation at any layer (H85). **postgres healing/ COMPLETE (7/7 DEEP, 88-93%)** — functions.rs adds 17 pg_extern SQL functions across 5 groups (health/history/triggers/config/strategy); healing/mod.rs confirms OnceLock global singleton (H87), 4-stage pipeline (detect→diagnose→repair→learn), and OutcomeTracker diverging-state risk (H88). **MinCut kernel/ COMPLETE** — bench_utils.rs (80-85%) is PURE MEASUREMENT SCAFFOLDING with zero SIMD kernel invocations; three timer bugs (non-serialized RDTSC H43-adjacent, hardcoded 3 GHz, no black_box); genuine no_std/AArch64 design. **GNN additional files** — ruvector-gnn: error.rs (95%+) standard thiserror enum, mmap_fixed.rs AtomicBitmap GENUINE but MmapManager+MmapGradientAccumulator ABSENT despite module doc advertisement (H86). **Postgres GNN layer OPENED (NEW)** — ruvector-postgres/gnn/ (3 files, 82-92% range) is a SELF-CONTAINED reimplementation of GNN, NOT wrapping ruvector-gnn crate: operators.rs (4 genuine SQL operators + 1 FACADE — ruvector_message_pass returns a format string, H90), gcn.rs (independent Kipf & Welling GCN, missing self-loops A not A+I, H93, deterministic pseudo-random weights H94), message_passing.rs (genuine MessagePassing trait with rayon propagate). TWO parallel GNN ecosystems now confirmed: ruvector-gnn (native Rust, 11+ DEEP files) vs ruvector-postgres/gnn (SQL-side, 3 DEEP files, zero cross-crate imports, H91). DEEP count: ~1,430.

## 2. File Registry

### ruvector-core & HNSW

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| simd_intrinsics.rs | ruvector-core | 1,605 | 90% | DEEP | Real AVX-512/AVX2/NEON runtime detection. PQ incomplete (RESOLVED by product_quantization.rs) | C |
| agenticdb.rs | ruvector-core | 1,447 | 70% | DEEP | Metadata filtering integration. Hash embeddings CRITICAL | C |
| lockfree.rs | ruvector-core | 591 | 85% | DEEP | Real lock-free structures via crossbeam | C |
| hnsw.rs | hnsw_rs vendored | 1,873 | 98-100% | DEEP | NOT a patch — vendored upstream v0.3.3. Zero modifications. Complete Malkov & Yashunin | R52 |
| hnswio.rs | hnsw_rs vendored | 1,704 | 95-98% | DEEP | Dual-file persistence, 4 format versions, hybrid mmap. No postgres/AgentDB connection | R52 |
| libext.rs | hnsw_rs fork | 1,241 | 75-85% | DEEP | Julia FFI. CRIT: no bounds checking, std::mem::forget | R36 |
| datamap.rs | hnsw_rs fork | 458 | 85-90% | DEEP | Zero-copy mmap. CRIT: use-after-free risk | R36 |
| product_quantization.rs | ruvector-core | 551 | 88-92% | DEEP | Real k-means++ + Lloyd's + ADC with LUT. RESOLVES H1 | R90 |
| conformal_prediction.rs | ruvector-core | 505 | 88-93% | DEEP | Valid split-conformal, Vovk et al. quantile, 3 nonconformity measures. 7 tests | R90 |
| hypergraph.rs | ruvector-core | 551 | 85-90% | DEEP | Genuine bipartite incidence, k-hop BFS, causal memory utility fn. Cites HyperGraphRAG (NeurIPS 2025) | R90 |
| tda.rs | ruvector-core | 497 | 60-70% | DEEP | MISLABELED — graph metrics only, no persistent homology. 11th mislabeled file | R90 |

### Attention & Neural

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| ruvector-attention (66 files) | ruvector-attention | ~9,200 | 80% | DEEP | 18+ real implementations. SIMD/Rayon no-ops | B |
| ruvector-gnn (~40 files) | ruvector-gnn | ~6,000 | 80% | DEEP | Custom hybrid GAT+GRU+edge, full EWC | C |
| micro-hnsw-wasm | ruvector | 1,263 | 60-70% | DEEP | Novel `#![no_std]` HNSW. 6 neuromorphic features UNTESTED | R36 |
| mmap.rs | ruvector-gnn (or ruvector) | 918 | 88-92% | DEEP | Real memmap2 file-backed mmap, AtomicBitmap lock-free, Linux madvise(MADV_WILLNEED), 17 tests. GNN training only — no HNSW integration. Pin count unused (no eviction) | R91 |
| scheduler.rs | ruvector-gnn | 532 | 82-88% | DEEP | 5 correct LR algorithms (StepDecay, Exponential, CosineAnnealing SGDR, WarmupLinear, ReduceOnPlateau). Zero GNN integration — no imports from crate. Falls genuine side of bimodal | R94 |
| replay.rs | ruvector-gnn | 503 | 88-92% | DEEP | Correct Vitter/Knuth Algorithm R reservoir sampling, Welford online stats, partial Fisher-Yates. No prioritized replay (uniform only). False KL divergence claim (actually Cohen's d). 12 tests | R94 |
| error.rs | ruvector-gnn | 112 | 95%+ | DEEP | Standard thiserror flat enum (GnnError, 11 variants). #[from] for io::Error and ruvector_core::error::RuvectorError. Mmap variant #[cfg(not(target_arch = "wasm32"))] gated — correct WASM exclusion. Training variant vestigial (GNN inference-only per R99). All variants have named constructors | R101 |
| mmap_fixed.rs | ruvector-gnn | 83 | 90-93% | DEEP | AtomicBitmap GENUINE (lock-free, Acquire/Release ordering, efficient bit iteration via trailing_zeros). MmapManager + MmapGradientAccumulator ABSENT despite module doc advertisement (H86). memmap2/UnsafeCell/RwLock/File all imported but orphaned. "fixed" = planned bugfix never completed | R101 |

### SQL Attention (ruvector-postgres) (R96+R97)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| attention/operators.rs | ruvector-postgres | 426 | 88-92% | DEEP | GENUINE pgx extension — 5 #[pg_extern] functions. CORRECTS R91 AttentionService.ts skepticism. Only 3/10 attention types SQL-dispatchable; 7 fall to ScaledDot default. JsonB matrix boundary. 6 genuine pg_test tests | R96 |
| attention/flash.rs | ruvector-postgres | 388 | 72-78% | DEEP | GENUINE algorithm (KV-tiling + correct online softmax). NOT a pgx function — zero #[pg_extern]. False O(sqrt(N)) space claim (actual O(N)). block_size_q dead_code (query tiling abandoned) | R96 |
| attention/multi_head.rs | ruvector-postgres | 368 | 88-92% | DEEP | Genuine Rayon parallel MHA, delegates to ScaledDotAttention. No SQL generation, no W_Q/W_K/W_V projections (retrieval-focused) | R97 |
| attention/scaled_dot.rs | ruvector-postgres | 308 | 90-93% | DEEP | Correct QK^T/sqrt(d_k), numerically stable softmax, real simsimd SIMD with fallback. Dead dropout field | R97 |
| attention/mod.rs | ruvector-postgres | 291 | 82-87% | DEEP | Orchestration hub with real pgrx PostgresEnum. Inflated "39 mechanisms" claim — only 10 enum variants defined | R97 |
| hyperbolic/poincare.rs | ruvector-postgres | 268 | 88-92% | DEEP | Real pgx Poincare ball math: distance (acosh), Mobius addition, exp_map, log_map. Pure Rust library; SQL via operators.rs. Correct curvature scaling, 13 unit tests. simsimd imported but unused | R98 |
| hyperbolic/lorentz.rs | ruvector-postgres | 258 | 87-92% | DEEP | Correct Lorentz/Minkowski model: inner product, hyperboloid constraint, acosh distance. Bidirectional Poincare↔Lorentz transforms. CRIT: no manifold validation, no pgx annotations (pure Rust). 13 tests with cross-model validation | R98 |
| hyperbolic/mod.rs | ruvector-postgres | 31 | ~92% | DEEP | Clean module root: lorentz, poincare, operators (internal, not pub use-d). DEFAULT_CURVATURE=-1.0, EPSILON=1e-8, MAX_NORM=1.0-1e-5 — correct constants. Completes SQL hyperbolic arc (R98). operators.rs is internal shared math only | R100 |
| hyperbolic/operators.rs | ruvector-postgres | 395 | 92-95% | DEEP | 8 pg_extern SQL functions: poincare_distance, lorentz_distance, mobius_add, exp_map, log_map, poincare_to_lorentz, lorentz_to_poincare, minkowski_dot. All immutable+parallel_safe. 11 pg_tests verifying symmetry, identity, exp/log roundtrip, coordinate conversion. No manifold validation (confirms R98 H70). COMPLETES postgres hyperbolic/ (4/4 DEEP) | R101 |

### ruvector LLM Extensions

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| compress.rs | ruvector (graph compression module) | 679 | 55-65% | DEEP | **12th MISLABELED FILE** — named "graph compression" implements embedding/tensor quantization. 5-tier access-frequency tiers. Fake IEEE f16 (fixed-point ×1000). Trivial PQ codebook (linear interpolation). Binary quantization correct. Zero GNN graph types | R91 |
| speculative.rs | ruvector | 788 | 88-92% | DEEP | EAGLE-style tree speculative decoding. Textbook rejection sampling. Novel lambda-guided confidence from mincut signal. Correct tree attention mask. Sequential path verification — not true parallel tree forward. Logit-processing only, no model objects | R91 |
| rope.rs | ruvector | 777 | 88-92% | DEEP | Correct RoPE (Su et al. 2021). NTK-aware scaling (CodeLlama/Qwen formula). Partial YaRN (base+bands, missing attention scale factor). Q15 quantized path. 11 substantive tests. No false SIMD claims. Independent from ruvllm/kernels/rope.rs | R91 |
| kv_cache/legacy.rs | ruvector | 773 | 82-88% | DEEP | RotateKV (IJCAI 2025) with Fast Walsh-Hadamard Transform. 2-bit/4-bit quantization with correct bit-packing. Per-head scale stomping bug (overwrites min/max on each new token). No eviction policy. 15 genuine tests | R91 |

### Temporal Tensor (Production-Grade)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| store.rs | temporal-tensor | ~2,500 | 92-95% | DEEP | BEST FILE. 74.7KB. CRC32, SVD reconstruction, 4-tier quant | R22 |
| store_ffi.rs | temporal-tensor | 889 | 90-92% | DEEP | 11 extern "C" FFI functions for WASM/C | R37 |
| agentdb.rs | temporal-tensor | 843 | 88-92% | DEEP | Pattern-aware tiering, 4-dim PatternVector. 36 tests | R37 |
| quantizer.rs | temporal-tensor | 1,430 | 93-95% | DEEP | K-means PQ with asymmetric distance | R37 |
| compressor.rs | temporal-tensor | 1,568 | 95-98% | DEEP | Delta+run-length+Huffman pipeline, CRC32 | R37 |
| tiering.rs | temporal-tensor | 1,613 | 93-95% | DEEP | 4-tier Hot→Warm→Cold→Archive with LRU | R37 |

### ruvllm LLM Inference

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| bitnet/backend.rs | ruvllm | 4,559 | 92-95% | DEEP | Complete BitNet 1-bit LLM. MLA 17.8x memory reduction | R22 |
| kernels/attention.rs | ruvllm | 2,215 | 88-92% | DEEP | Flash Attention 2, NEON dot product, paged KV cache | R22 |
| kernels/matmul.rs | ruvllm | 2,050 | 85-90% | DEEP | 12x4 GEMM micro-kernel, Apple Accelerate, Metal GPU | R22 |
| memory_pool.rs | ruvllm | 1,704 | 95% | DEEP | Lock-free bump allocator, RAII buffer pool, 12 tests | R34 |
| autodetect.rs | ruvllm | 1,945 | 92% | DEEP | Hardware detection (Metal, CPU features). 27 tests | R34 |
| kv_cache.rs | ruvllm | 1,528 | 90% | DEEP | Two-tier KV cache, NEON SIMD quantize/dequantize | R34 |
| norm.rs | ruvllm/kernels | 652 | 95% | DEEP | BEST quality — 4x unrolled FMA, correct variance | R35 |
| rope.rs | ruvllm/kernels | 660 | 95% | DEEP | Real RoPE, NEON interleaved ops, NTK-aware scaling | R35 |
| quantized.rs | ruvllm/kernels | 1,219 | 92% | DEEP | Real NEON int8/int4/q4k kernels, llama.cpp-compatible | R35 |
| activations.rs | ruvllm/kernels | 1,041 | 92% | DEEP | Vectorized exp/sigmoid/tanh, polynomial approx | R35 |
| ane_ops.rs | ruvllm/kernels | 1,758 | 70% | DEEP | MISLEADING: gelu_ane/silu_ane are SCALAR FALLBACKS | R35 |
| scheduler.rs | ruvllm/serving | 840 | 90-92% | DEEP | vLLM-style continuous batching, preemption, chunked prefill | R35 |
| engine.rs | ruvllm/serving | 1,302 | 80-85% | DEEP | Real continuous batching. Fallback: hash%32000 when no model | R35 |
| speculative.rs | ruvllm/bitnet | 1,392 | 55-60% | DEEP | CRITICAL: 2K forward passes for K tokens = SLOWER | R35 |
| reasoning_bank.rs | ruvllm/claude_flow | 1,520 | 92-95% | DEEP | Fourth ReasoningBank. Real K-means, EWC++. 16 tests | R37 |
| hnsw_router.rs | ruvllm/claude_flow | 1,288 | 90-93% | DEEP | BEST ruvector-core integration. Hybrid semantic+keyword | R37 |
| model_router.rs | ruvllm/claude_flow | 1,292 | 88-92% | DEEP | 7-factor complexity, feedback tracking 1000 predictions | R37 |
| pretrain_pipeline.rs | ruvllm/claude_flow | 1,394 | 85-88% | DEEP | Multi-phase pretraining. CRIT: hash-based embeddings | R37 |
| claude_integration.rs | ruvllm/claude_flow | 1,344 | 70-75% | DEEP | CRIT: execute_workflow SIMULATION, hardcoded 500 tokens | R37 |
| micro_lora.rs | ruvllm/training | 1,261 | 92-95% | DEEP | BEST learning code. NEON 8x unroll, EWC++. <1ms forward | R37 |
| grpo.rs | ruvllm/training | 898 | 90-92% | DEEP | Textbook GRPO: GAE, PPO clipping, adaptive KL. 16 tests | R37 |
| real_trainer.rs | ruvllm/training | 1,000 | 70-75% | DEEP | Triplet loss + InfoNCE. CRIT: hash-based embeddings | R37 |
| tool_dataset.rs | ruvllm/training | 2,147 | 88-92% | DEEP | 140+ MCP tool-call templates, 19 categories | R37 |

### Postgres Extension

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| distance/simd.rs | ruvector-postgres | 2,129 | 95-98% | DEEP | BEST SIMD IN ECOSYSTEM. AVX-512/AVX2/NEON. 23 tests | R22 |
| index/hnsw_am.rs | ruvector-postgres | 1,997 | 75-80% | DEEP | CRIT: connect_node_to_neighbors() EMPTY | R22 |
| index/ivfflat_am.rs | ruvector-postgres | 2,165 | 80-85% | DEEP | Real k-means++, Lloyd. STUBS: insert, delete, retrain | R22 |
| sparql/parser.rs | ruvector-postgres | 2,496 | 93-95% | DEEP | PRODUCTION W3C SPARQL 1.1 parser. All 4 query forms, property paths, 33+ functions | R52 |
| sparql_executor.rs | ruvector-postgres | 1,885 | 92% | DEEP | COMPLETE SPARQL 1.1 query engine. BGP, property paths, 7 aggs | R34 |
| index_bench.rs | ruvector-postgres | 1,395 | 42% | DEEP | THEATRICAL: HNSW search is brute-force O(n). Zero postgres integration despite location | R52 |
| operators.rs | ruvector-postgres | ~1,200 | 85% | DEEP | 54 verified SQL functions | Initial |
| healing/learning.rs | ruvector-postgres | 670 | 92-95% | DEEP | Genuine adaptive weight formula, confidence scoring | R36 |
| healing/detector.rs | ruvector-postgres | 826 | 85-90% | DEEP | 8 problem types. All 8 metric collection methods EMPTY | R36 |
| healing/engine.rs | ruvector-postgres | 789 | 75-80% | DEEP | Cooldown/rate-limiting real. CRIT: no timeout enforcement | R36 |
| healing/strategies.rs | ruvector-postgres | 1,166 | 60-65% | DEEP | StrategyRegistry 95%. ALL 5 executions log-only stubs | R36 |
| healing/functions.rs | ruvector-postgres | 468 | 88-92% | DEEP | 17 pg_extern SQL functions across 5 groups (health status/history/triggers/config/strategy). dry_run genuine. read/write lock inconsistency in set_thresholds (engine.read() for mutation). Empty test placeholder. COMPLETES healing/ (7/7 DEEP) | R101 |
| healing/mod.rs | ruvector-postgres | 234 | 90-93% | DEEP | Module root + orchestration hub. OnceLock global singleton HEALING_ENGINE limits per-connection isolation. 4-stage pipeline (detect→diagnose→repair→learn). OutcomeTracker cloned into RemediationEngine AND kept as self.tracker — diverging state risk. HealingWorkerState initialized but worker not spawned here. 3 tests | R101 |

### ruQu Quantum Error Correction

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| decoder.rs | ruQu | 2,400 | 95-98% | DEEP | BEST FILE. Union-Find O(α(n)) + MWPM. Top 15% quality | R37 |
| syndrome.rs | ruQu | 1,640 | 90-92% | DEEP | Real AVX2 SIMD vpshufb lookup popcount | R37 |
| surface_code.rs | ruQu | 1,820 | 88-92% | DEEP | Complete surface code, weight-2 stabilizers | R37 |
| qec_scheduler.rs | ruQu | 1,505 | 88-92% | DEEP | Critical path learning. All remote providers stub | R37 |
| noise_model.rs | ruQu | 1,330 | 82-85% | DEEP | 7 noise channels, Kraus operator validation | R37 |
| tile.rs | ruQu | 2,125 | 92% | DEEP | Coherence gate, Union-Find, Ed25519 signatures. 27 tests | R39 |
| planner.rs | ruQu | 1,478 | 88% | DEEP | 4 backend cost models, entanglement estimation. 33 tests | R39 |
| filters.rs | ruQu | 1,357 | 82-86% | DEEP | MISNAMED — coherence quality gate, NOT quantum filtering. Three-filter pipeline (structural/shift/evidence). 14 tests | R54 |
| fabric.rs | ruQu | 1,280 | 93-96% | DEEP | Production 256-tile WASM fabric orchestrator. Blake3 audit trails, surface code generator. 23 tests | R54 |

### ruqu-core Extended

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| mitigation.rs | ruqu-core | 1,276 | 95-98% | DEEP | Three NISQ-era strategies (ZNE, Measurement Error, CDR). Richardson extrapolation, tensor-product calibration. 40+ tests | R54 |
| transpiler.rs | ruqu-core | 1,211 | 95-98% | DEEP | Complete 3-phase circuit transpiler. 3 hardware backends (IBM/IonQ/Rigetti). 44 tests. BEST-IN-CLASS | R54 |
| subpoly_decoder.rs | ruqu-core | 1,208 | 35-40% | DEEP | **FALSE SUBPOLYNOMIAL** (3rd instance). O(n²) greedy under "provable O(d^{2-ε})" claims. Zero citations | R54 |
| noise.rs | ruqu-core | 1,175 | 96-98% | DEEP | Production Kraus operator formalism. 4 noise channels + hardware calibration pipeline. 498 test lines. BEST-IN-CLASS | R54 |

### Prime-Radiant Hyperbolic (R97)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| hyperbolic/mod.rs | prime-radiant | 363 | 88-92% | DEEP | Genuine Poincare geometry (correct formulas), brute-force O(n) search (no real HNSW back-end), disconnected from sheaf substrate, 17 tests | R97 |
| hyperbolic/energy.rs | prime-radiant | 352 | 82-87% | DEEP | Pure data container — all Riemannian math in adapter.rs. merge() has silent curvature compatibility bug. curvature stored but unused in aggregation | R97 |
| hyperbolic/adapter.rs | prime-radiant | 333 | 78-83% | DEEP | Poincare-only (no Lorentz conversion despite name), HNSW is documented brute-force stub, exp_map lacks Mobius addition (approximate geodesic stepping) | R97 |
| hyperbolic/depth.rs | prime-radiant | 215 | 78-82% | DEEP | Correct Poincare depth formula (2*arctanh(|x|)/sqrt(-c)). Curvature stored but NEVER modulates calculations (hardcoded -1.0). Hardcoded level thresholds. Dead weight_multiplier(). 6 tests with curvature=-1.0 only | R98 |
| hyperbolic/config.rs | prime-radiant | 170 | 75-82% | DEEP | Well-structured serde config with curvature, dimension, Frechet, HNSW params. HNSW params are DEAD CONFIG (never used, R97). Euclidean defaults (M=16, ef_construction=200) uncorrected for Poincare geometry. Weak validation | R98 |

### Prime-Radiant Coherence

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| restriction.rs | prime-radiant | 1,489 | 90-92% | DEEP | BEST sparse matrix — CSR 6 formats, 4x SIMD unroll | R37 |
| memory_layer.rs | prime-radiant | 1,260 | 92-95% | DEEP | Triple memory, real cosine similarity. 19 tests | R37 |
| witness_log.rs | prime-radiant | 1,130 | 88-92% | DEEP | blake3 hash chains with tamper evidence. 16 tests | R37 |
| coherence.rs | prime-radiant | 1,500 | 88-90% | DEEP | Sheaf Laplacian, spectral gap computation | R37 |
| knowledge_graph.rs | prime-radiant | 1,190 | 85-88% | DEEP | DashMap concurrent graph, topological sort | R37 |

### Cognitum Gate & Other Specialized

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| lib.rs | cognitum-gate-kernel | 713 | 95% | DEEP | Custom bump allocator, no_std WASM. 6 FFI exports | R36 |
| report.rs | cognitum-gate-kernel | 491 | 98% | DEEP | 64-byte cache-line aligned, compile-time assertions | R36 |
| delta.rs | cognitum-gate-kernel | 465 | 98% | DEEP | Tagged union 7 operation types, fixed-size FFI-safe | R36 |
| shard.rs | cognitum-gate-kernel | 983 | 92% | DEEP | Optimal union-find, iterative path compression | R36 |
| evidence.rs | cognitum-gate-kernel | 852 | 88% | DEEP | Fixed-point log-space, anytime-valid e-process | R36 |
| sparse.rs | sublinear-solver | 964 | 95% | DEEP | 4 sparse formats (CSR/CSC/COO/Graph), no_std | R28 |
| wrapper/mod.rs | ruvector-mincut | 1,505 | 90% | DEEP | Bounded-range decomposition from arXiv:2512.13105. 22 tests | R34 |
| hierarchy.rs | ruvector-mincut | 1,489 | 88% | DEEP | 3-level hierarchy. check_and_split_expander incomplete | R34 |
| subpolynomial/mod.rs | ruvector-mincut | 1,385 | 45-50% | DEEP | FALSE subpolynomial complexity. Invalid arXiv citation. Same R39 pattern | R52 |
| graph Cypher parser | ruvector-graph | 1,296 | 95% | DEEP | Production parser. CRIT: NO EXECUTOR | C |

### ruvector-graph Distributed

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| distributed/shard.rs | ruvector-graph | 596 | 70-80% | DEEP | BEST distributed file. EdgeCutMinimizer multilevel KL, real xxh3/blake3. In-memory only | R90 |
| distributed/gossip.rs | ruvector-graph | 624 | 45-55% | DEEP | Correct SWIM state machine + failure detector, no network transport (log-only) | R90 |
| distributed/federation.rs | ruvector-graph | 583 | 40-50% | DEEP | Real merge/dedup logic + FederationStrategy dispatch, execute_on_cluster always returns empty Vec | R90 |
| distributed/coordinator.rs | ruvector-graph | 536 | 30-35% | DEEP | 2PC types defined, state machine frozen at Active, no network layer, naive string-based query planner | R90 |
| distributed/rpc.rs | ruvector-graph | 516 | 15-20% | DEEP | All 4 RPC methods stubs. gRPC (tonic) feature-gated out of default build | R90 |

### Edge-Net P2P Transport

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| simd.rs | edge-net (ruvector) | 1,418 | 92-95% | DEEP | COMPLETE independent SIMD for NN inference. Real AVX2/WASM/SSE4.1, Q4/Q8 quantization, numerically stable | R52 |
| lora.rs | edge-net (ruvector) | 1,355 | 90-95% | DEEP | Complete edge LoRA. Dual SIMD (AVX2+WASM128), Q4/Q8 quantization, LRU adapter pool, WASM bindings. 15 tests. Independent from micro_lora.rs | R54 |
| federated.rs | edge-net (ruvector) | 1,218 | 95-98% | DEEP | BEST federated learning in project. Byzantine-robust (MAD+median), differential privacy (Gaussian), TopK compression with error feedback, reputation-weighted FedAvg. 13 tests | R54 |
| p2p.rs | edge-net (ruvector) | 845 | 92-95% | DEEP | **REVERSES R42**: Real libp2p (Gossipsub/Kademlia/RequestResponse/Identify), NOISE+yamux+TCP, direct RAC integration via broadcast_rac_event(), 6 gossipsub topics, production P2P | R44 |
| advanced.rs | edge (ruvector) | 2041 | 72% | DEEP | MISNOMER — zero networking. ML primitives: Raft 85%, SNN 95% (STDP), HDC 93%, HNSW reimpl 88%, hash embeddings (8th occurrence), quantization 92% | R44 |
| swarm.rs | edge (ruvector) | 612 | 72% | DEEP | Production crypto protocol (Ed25519+AES-256-GCM 88%, identity registry 85%, task claiming 80%) but 0% GUN network transport — all publish = stubs | R44 |

### Hyperbolic HNSW & Attention (R92)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| hnsw.rs | ruvector-hyperbolic-hnsw | 651 | 88-93% | DEEP | NATIVE HNSW (not hnsw_rs). Poincaré ball distance, tangent-space pruning, DualSpaceIndex RRF | R92 |
| poincare.rs | ruvector-hyperbolic-hnsw | 628 | 90-95% | DEEP | ALL formulas correct (Ganea 2018). Möbius ops, exp/log maps, Fréchet mean. Comprehensive stability | R92 |
| lorentz_cascade.rs | ruvector-attention | 580 | 90-93% | DEEP | Genuine Lorentz model attention. Minkowski metric, Busemann functions, multi-curvature cascade, Einstein midpoint | R92 |
| lib.rs | ruvector-hyperbolic-hnsw-wasm | 633 | 88-92% | DEEP | **17th GENUINE WASM**. Production wasm_bindgen, 7 real math ops, ShardedIndex | R92 |
| shard.rs | ruvector-hyperbolic-hnsw | 576 | 82-85% | DEEP | Hyperbolic-aware radius partitioning. Canary deployment. Transport-absent (confirms R90) | R92 |
| tangent.rs | ruvector-hyperbolic-hnsw | 349 | 88-92% | DEEP | Two-phase Poincare pruning: Euclidean filter in tangent space → exact Poincare re-rank. TangentCache Fréchet centroid. O(N) linear scan by design (receives HNSW candidates from caller). Dead import: norm_squared. 2 genuine tests. Completes crate | R99 |

### GNN Bindings (napi-rs & WASM)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| lib.rs | ruvector-gnn-node | 428 | 88-92% | DEEP | Real napi-rs #[napi] macros. Inference-only FFI: forward/compress/decompress/differentiable_search/hierarchical_forward. Solid error handling (Status::InvalidArg, .map_err). hierarchical_forward() deserializes layer configs from JSON strings per-call (serde overhead). 5-tier compression enum | R99 |
| lib.rs | ruvector-gnn-wasm | 415 | 90-94% | DEEP | **18th GENUINE WASM**. All exports delegate to real ruvector_gnn core via serde_wasm_bindgen. Inline cosineSimilarity() correct (95%). console_error_panic_hook initialized. Same inference-only surface as gnn-node | R99 |

### Attention — Hyperbolic (R100)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| hyperbolic_attention.rs | ruvector-attention | 172 | 88-92% | DEEP | Fully working Poincare-ball attention, Fréchet mean aggregation. HIGH: adaptive_curvature is dead field — fixed at construction, never updated despite config flag. Unconditional project_to_ball on every compute() wastes cycles | R100 |
| mixed_curvature.rs | ruvector-attention | 241 | 90-94% | DEEP | MOST ARCHITECTURALLY NOVEL — splits embeddings into [euclidean \|\| hyperbolic] halves, independent attention, blend. HIGH: linear blend of two softmax distributions is ad-hoc (not proper mixture model); renormalization rescues correctness | R100 |
| poincare.rs | ruvector-attention | 181 | 93-96% | DEEP | Shared mathematical foundation for entire hyperbolic cluster. All Gyrovector operations correct: poincare_distance, mobius_add, mobius_scalar_mult, exp_map, log_map. EPS clamping, acosh clamping, projection after every update. MEDIUM: frechet_mean fixed learning rate (0.1) may oscillate at high curvature | R100 |
| mod.rs (attention hyperbolic) | ruvector-attention | 26 | 100% | DEEP | Pure wiring — re-exports hyperbolic_attention, lorentz_cascade, mixed_curvature, poincare. Flat pub re-export confirms lorentz_cascade (R92) is first-class alongside Poincare | R100 |

### Hyperbolic HNSW — Library Entry Point (R100)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| lib.rs | ruvector-hyperbolic-hnsw | 211 | 90-93% | DEEP | Crate entry point exposing 5 submodules: error, hnsw, poincare, shard, tangent. Tangent-space pruning: log_c(x) at shard centroid, cheap Euclidean prune before exact Poincare. Per-shard curvature (ShardedHyperbolicHnsw). DualSpaceIndex: synchronized Euclidean fallback for ranking fusion near ball boundary. HIGH: no bounds check on shard index in tangent pruning; DualSpaceIndex sync overhead not benchmarked | R100 |
| error.rs | ruvector-hyperbolic-hnsw | 43 | 95%+ | DEEP | 8-variant HyperbolicError enum via thiserror. OutsideBall carries norm + curvature for programmatic recovery. HyperbolicResult type alias exported. Clone-derived for retry/backoff. Clean: no std::io::Error wrapping | R100 |

### Postgres GNN (ruvector-postgres) (R101)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| gnn/operators.rs | ruvector-postgres | 426 | 82-87% | DEEP | 4 genuine SQL operators + 1 FACADE (ruvector_message_pass returns format string, H90). Self-contained — imports from super::gcn, super::graphsage, super::aggregators, NOT ruvector-gnn crate (H91). Deterministic weights. 8 pg_test cases | R101 |
| gnn/gcn.rs | ruvector-postgres | 224 | 82-88% | DEEP | Independent Kipf & Welling GCN — Xavier init, 1/sqrt(degree) normalization, ReLU. Missing self-loops: uses A not A+I (pure neighbor aggregation, not canonical GCN, H93). Zero SQL — purely in-memory Vec<Vec<f32>> (H92). Rayon par_iter. Deterministic pseudo-random weights (H94). 6 tests with hand-computed expected values | R101 |
| gnn/message_passing.rs | ruvector-postgres | 234 | 88-92% | DEEP | Core MessagePassing trait (message/aggregate/update). Genuine build_adjacency_list (inbound adjacency), propagate() with rayon par_iter, propagate_weighted() with per-edge f32 weights. Zero Postgres-specific code — pure Rust graph algorithm. Only SUM aggregation. 3 tests | R101 |

### AIDefence Security (R92)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| AIDefenceGuard.ts | ruvbot (npm) | 763 | 82-88% | DEEP | PARTIALLY REAL — 28 genuine regex patterns, zero ML, behavioral stub, policy ghost | R92 |
| aidefence-guard.test.ts | ruvbot (npm) | 280 | — | DEEP | Tests MOCKED — exercise regex only, behavioral analysis untested, PII false positives unverified | R92 |
| aidefence-integration.ts | agentdb | 166 | — | DEEP | SIMULATION-ONLY — hardcoded threat data, correct AgentDB pattern, no real AIDefence | R92 |

### CUDA-WASM Flash Attention (R92)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| flash_attention.rs | cuda-wasm (ruv-FANN) | 528 | 88-92% | DEEP | MISPLACED — CPU Flash Attention v2 reference, zero CUDA/GPU code. Textbook online softmax + causal masking. 7 tests | R92 |

### MinCut-Gated-Transformer Core (R93)

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| attention/spike_driven.rs | ruvector-mincut-gated-transformer | 585 | 88-93% | DEEP | GENUINE neuromorphic LIF + STDP coincidence attention. saturating_mul violates "multiplication-free" claim. Zero energy_gate.rs integration | R93 |
| sparse_attention.rs | ruvector-mincut-gated-transformer | 571 | 72-80% | DEEP | Novel mincut-aware design thesis, but partition boundaries are PLACEHOLDER (uniform stride). _target_density and _gate dead params. BTreeSet dedup correct | R93 |
| q15.rs | ruvector-mincut-gated-transformer | 634 | 88-92% | DEEP | Correct Q15 fixed-point (u16). CRITICAL: u16/i16 type mismatch with rope.rs. Newtype exported but unused within crate. 11 tests | R93 |
| kernel/qgemm.rs | ruvector-mincut-gated-transformer | 621 | 88-93% | DEEP | GENUINE quantized GEMM. Real AVX2+NEON widening multiply-accumulate. Correct asymmetric quantization. Compile-time-only SIMD dispatch (no runtime detection) | R93 |
| kv_cache/manager.rs | ruvector-mincut-gated-transformer | 596 | 72-78% | DEEP | 3-tier AdaptiveKVCache (Hot/Warm/Archive). Pure FIFO eviction, no H2O/StreamingLLM. SQuat/KVQuant quantizers dead code. Rematerialization wired but never triggered | R93 |
| ffn.rs | ruvector-mincut-gated-transformer | 628 | 88-92% | DEEP | Genuine INT8 FFN with real SIMD GELU (AVX2 Padé tanh + NEON Newton-Raphson). Vanilla GELU, not SwiGLU/GeGLU. Allocation-free forward. No MoE | R93 |
| kv_cache/kvquant.rs | ruvector-mincut-gated-transformer | 565 | 78-85% | DEEP | Genuine pre-RoPE KVQuant (Hooper et al. 2024). 3-bit packing silently broken (falls back to 4-bit). O(n²) outlier scan. calibrate() per-layer API lies (flattens to single pair) | R93 |
| energy_gate.rs | ruvector-mincut-gated-transformer | 560 | 88-93% | DEEP | **R34 "MOST NOVEL" CONFIRMED.** Genuine 3-component EBT: sigmoid lambda + boundary penalty + partition entropy. Central-difference gradient. System-2 gradient descent refinement. Hybrid GatePolicy fallback | R93 |
| early_exit.rs | ruvector-mincut-gated-transformer | 661 | 88-92% | DEEP | Novel lambda-stability-driven early exit (LayerSkip alternative). 40/40/20 confidence weighting. abs() overflow risk. Unnormalized logit verification. Adaptive/config threshold inconsistency | R93 |
| arena.rs | ruvector-mincut-gated-transformer | 472 | 92% | DEEP | GENUINE bump allocator with 64B cache-line alignment. QKV+FFN weight layout in calculate_arena_size. Aliasing UB hazard (multiple &mut slices from same Vec). calculate_arena_size ignores _heads. 10 tests | R96 |
| kv_cache/squat.rs | ruvector-mincut-gated-transformer | 467 | 78-84% | DEEP | GENUINE SQuAt 2024 paper. Hadamard basis + Gram-Schmidt correct. 2/4-bit packing complete. CRIT: calibrate() STUB — ignores calibration_data, no data-driven basis learning | R96 |
| kv_cache/kivi.rs | ruvector-mincut-gated-transformer | 458 | 72-78% | DEEP | GENUINE FWHT (butterfly recursion). Per-channel quantization claim FALSE — global min/max regardless of QuantScheme. PerGroup stub. SIMD dequantize TODO. Low-level primitive for quantized_store.rs | R96 |
| kv_cache/policy.rs | ruvector-mincut-gated-transformer | 440 | 82-87% | DEEP | Age-based tier graduation only. NO H2O/attention-sink/LRU. Three-struct architecture genuine. Evaluate/tracker pressure divergence. Cost model inversion. Zero hot_buffer.rs integration | R96 |
| kv_cache/hot_buffer.rs | ruvector-mincut-gated-transformer | 419 | 82-87% | DEEP | FIFO ring buffer for "FP16" (actually f32 — test acknowledges). pop_oldest() BUG (no read cursor for sequential pops). Dual push API trap (push vs push_head+advance). Zero policy.rs integration | R96 |
| trace.rs | ruvector-mincut-gated-transformer | 413 | 88-92% | DEEP | Gate-decision diagnostics with stack-allocated [T;64] circular buffer. Records GateDecision/lambda/tier from packets.rs. Feature gate claim documentation-only — compiles unconditionally. 6 genuine tests | R96 |
| packets.rs | ruvector-mincut-gated-transformer | 492 | 90-95% | DEEP | Pure type-definition coherence interface. GatePacket/GateDecision/SpikePacket/InferInput/Witness. Novel QuarantineUpdates isolation mode. Consistent Q15. repr(C) throughout | R94 |
| state.rs | ruvector-mincut-gated-transformer | 500 | 88-92% | DEEP | Zero-alloc buffer layout (64-byte aligned). Inference-only — no checkpoint/safetensors. KV ring buffer correct. Unchecked layer index UB risk. No version tagging | R94 |
| quantized_store.rs | ruvector-mincut-gated-transformer | 523 | 88-92% | DEEP | Two-tier KIVI (4-bit warm, 2-bit archive). Correct byte packing. Per-channel keys / per-token values (KIVI paper). Silent warm overflow. Orthogonal to kvquant.rs | R94 |
| mod_routing.rs | ruvector-mincut-gated-transformer | 537 | 72-78% | DEEP | MoD (Mixture-of-Depths) NOT MoE. Deterministic lambda-delta routing. route_unstable/stable are byte-identical duplicates. Boundary detection = stride heuristic, not actual mincut edges | R94 |
| window.rs | ruvector-mincut-gated-transformer | 481 | 82-87% | DEEP | Pure causal sliding window (NOT Longformer despite docstring). Correct numerical-stable softmax. Scalar attention kernels. Feature-gated SparseMask bridge. 5 tests | R94 |
| metrics.rs | ruvector-mincut-gated-transformer | 495 | 78-82% | DEEP | PPL/accuracy quality tracker, NOT cache hit-rates despite docstring. tier_metrics() mostly hardcoded. Dead code paths (should_adapt, boundary_adjustment_factor unused by manager.rs). Rolling window correct | R94 |
| quant4.rs | ruvector-mincut-gated-transformer | 506 | 72-78% | DEEP | RTN (round-to-nearest) NOT GPTQ/AWQ. Fully scalar, no SIMD. BlockInt4Weights write-only (no dequantize). Wastes 12.5% INT4 range. 9 tests including exhaustive round-trip | R94 |
| spike.rs | ruvector-mincut-gated-transformer | 366 | 88-92% | DEEP | Scheduling layer (NOT neuron dynamics), Q15 rate-tiering, FNV-1a novelty hashing, full coherence with packets.rs (R94). no_std compatible | R97 |
| config.rs | ruvector-mincut-gated-transformer | 369 | 88-92% | DEEP | Well-structured serde config, covers only ~40% subsystem surface (missing energy gate, spike, quantization, KV cache tier, and MoD routing configs) | R97 |
| kv_cache/tier.rs | ruvector-mincut-gated-transformer | 305 | 90-93% | DEEP | Clean 3-tier definitions (Hot/Warm/Archive = FP16/4-bit/2-bit), age-based not access-frequency tier assignment. Aligns with R94 KIVI findings. 9 unit tests | R97 |
| lib.rs | ruvector-mincut-gated-transformer | 261 | 90-95% | DEEP | EXCELLENT crate root: 23 public modules, 60+ exported items, 10 feature flags, 43-item prelude, dual KV cache re-export. Rustdoc with academic citations + working example. All DEEP modules accessible | R98 |
| norm.rs | ruvector-mincut-gated-transformer | 213 | 55-65% | DEEP | WEAKEST MinCut kernel. LayerNorm + RMSNorm math correct but ALL pure scalar — no SIMD despite SIMD siblings (qgemm.rs, ffn.rs). RMSNorm feature-flag body duplication is dead code. 6 tests | R98 |
| kv_cache/mod.rs | ruvector-mincut-gated-transformer | 98 | ~90% | DEEP | ADR-004 Three-Tier Adaptive KV Cache module root: FP16 hot → 4-bit KIVI warm → 2-bit archive. 9 submodules publicly re-exported. Legacy backward-compat preserved. Pure module root — all logic in children | R100 |
| error.rs | ruvector-mincut-gated-transformer | 95 | ~92% | DEEP | 5-variant Error: BadConfig, BadWeights, BadInput, OutputTooSmall, UnsupportedMode. Hard no-panic contract documented. is_recoverable() + is_config_error() for caller retry logic. 3 tests, no_std compatible. HIGH QUALITY | R100 |
| attention/linear.rs | ruvector-mincut-gated-transformer | 67 | 15-20% | DEEP | **LINEAR ATTENTION IS A PLACEHOLDER** — struct exists with config, NO forward pass, NO ELU+1 approximation, NO kernel math. Docstring cites Katharopoulos 2020 but implementation EMPTY. Hidden behind `linear_attention` feature gate (not in default build). Deflates MinCut O(n) attention novelty claims | R100 |
| attention/mod.rs | ruvector-mincut-gated-transformer | 35 | ~88% | DEEP | Feature-gated attention architecture: linear/spike/sparse all opt-in, default = SlidingWindowAttention only. MinCut sparse path (apply_mincut_sparse_mask) real but non-default. 4 academic citations: MInference, Spike-driven Transformer, Spectral Attention | R100 |
| kernel/mod.rs | ruvector-mincut-gated-transformer | 24 | ~90% | DEEP | Pure re-export root for bench_utils, norm, qgemm, quant4. Dual-precision: INT8 GEMM + INT4 pipeline. Benchmark utilities built into kernel module | R100 |
| kernel/bench_utils.rs | ruvector-mincut-gated-transformer | 442 | 80-85% | DEEP | PURE MEASUREMENT SCAFFOLDING — zero MinCut SIMD kernel invocations. Timer/BenchStats/BenchConfig harness library. Three timer bugs: non-serialized RDTSC (can produce negative cycles), hardcoded 3 GHz divisor, no black_box (optimizer may eliminate benchmarked code). Wrong SSE2 gate on RDTSC (should be unconditional on x86_64). Genuine no_std with AArch64 CNTVCT_EL0 path. 5 unit tests | R101 |

### SONA & Learning

| File | Package | LOC | Real% | Depth | Key Verdict | Session |
|------|---------|-----|-------|-------|-------------|---------|
| sona (27 files) | ruvector | ~4,500 | 85% | DEEP | MicroLoRA, EWC++, federated, SafeTensors. Production-ready | R13 |

## 3. Findings Registry

### 3a. CRITICAL Findings

| ID | Description | File(s) | Session | Status |
|----|-------------|---------|---------|--------|
| C1 | **Hash-based embeddings systemic** — 7+ files (Rust + JS) default to character-sum, FNV-1a, or position hash. All semantic search using defaults is character-frequency matching | ruvector-core agenticdb.rs, ruvector-cli hooks.rs, pretrain_pipeline.rs, real_trainer.rs, rlm_embedder.rs, learning-service.mjs, enhanced-embeddings.ts | C, R22, R37 | Open |
| C2 | **HNSW deletions broken** — hnsw_rs does not support vector deletion. Delete method silently fails or panics | ruvector-core | C | Open |
| C3 | **Postgres HNSW neighbor connections empty** — `connect_node_to_neighbors()` is COMPLETELY EMPTY. Graph never actually built during insertion | index/hnsw_am.rs | R22 | Open |
| C4 | **Postgres IVFFlat mutations stubbed** — `aminsert()`, `ambulkdelete()`, `retrain()` all stubs. Index can be built/searched but not updated | index/ivfflat_am.rs | R22 | Open |
| C5 | **ruvector-graph has NO query executor** — Cypher parser generates full AST but nothing executes queries. "Working Cypher" claim is false | ruvector-graph | C | Open |
| C6 | **execute_workflow SIMULATION** — claude_integration.rs hardcodes 500 tokens, no real Claude API calls despite complete type system | claude_integration.rs | R37 | Open |
| C7 | **Speculative decoding SLOWER than vanilla** — 2K sequential forward passes for K tokens vs K vanilla passes. Current implementation is anti-optimization | speculative.rs | R35 | Open |
| C8 | **Fourth ReasoningBank with zero code sharing** — reasoning_bank.rs joins claude-flow, agentic-flow, agentdb implementations. All implement same algorithms independently | reasoning_bank.rs | R37 | Open |
| C9 | **HNSW patches FFI unsafe** — libext.rs has no bounds checking on C pointer dereferences, std::mem::forget on vectors returned to C (memory leak risk) | libext.rs | R36 | Open |
| C10 | **HNSW patches no integrity validation** — hnswio.rs has no checksum/hash on serialized dumps. Corrupted data silently loads | hnswio.rs | R36 | Open |
| C11 | **Postgres healing strategies are stubs** — All 5 healing strategies (reindex, promote, evict, block queries, repair edges) are log-only | healing/strategies.rs | R36 | Open |
| C12 | **Postgres healing metrics empty** — All 8 metric collection methods in detector.rs return empty/zero. Self-healing system cannot detect problems | healing/detector.rs | R36 | Open |
| C13 | **micro-hnsw-wasm neuromorphic unvalidated** — 6 novel features (spike encoding, homeostatic plasticity, 40Hz resonance, WTA, dendritic computation, temporal patterns) have ZERO tests | micro-hnsw-wasm | R36 | Open |
| C14 | **ruQu remote quantum providers ALL stub** — IBM/IonQ/Rigetti/Braket all return AuthenticationFailed. Only LocalSimulator works | qec_scheduler.rs | R37 | Open |
| C15 | **HNSW "patches" are vendored upstream** — hnsw.rs is unmodified hnsw_rs v0.3.3 from crates.io. Directory name `scripts/patches/` is misleading — zero ruvector-specific modifications | hnsw.rs | R52 | Open |
| C16 | **index_bench.rs HNSW is brute-force O(n)** — search_with_ef() iterates ALL nodes linearly. Benchmarks measure brute-force kNN, NOT real HNSW graph traversal | index_bench.rs | R52 | Open |
| C17 | **index_bench.rs zero postgres** — Located in ruvector-postgres/benches/ but contains zero PostgreSQL code (no pgrx, sqlx, tokio_postgres). Complete mislabeling | index_bench.rs | R52 | Open |
| C18 | **subpolynomial/mod.rs invalid paper citation** — Claims arXiv:2512.13105 (Dec 2024) but arXiv format 2512.NNNNN means Dec 2025, not 2024. Theoretical foundation suspect | subpolynomial/mod.rs | R52 | Open |
| C19 | **subpolynomial/mod.rs FALSE complexity claims** — Claims O(n^{o(1)}) subpolynomial but implements O(log n) levels × O(recourse). Same R39 false sublinearity pattern | subpolynomial/mod.rs | R52 | Open |
| C20 | **subpolynomial/mod.rs deterministic contradiction** — Claims "deterministic" subpolynomial mincut, which is an OPEN PROBLEM in graph algorithms. No randomization in code either | subpolynomial/mod.rs | R52 | Open |
| C21 | **subpoly_decoder.rs FALSE subpolynomial** — 3rd instance of false complexity. Claims "provable O(d^{2-ε} polylog d)" but ALL 3 decoders use O(n²) greedy_pair_and_correct. Zero citations, zero empirical validation | subpoly_decoder.rs | R54 | Open |
| C22 | **filters.rs COMPLETE domain mislabeling** — Named "filters" in quantum crate but implements classical coherence quality gate. Zero connection to decoder.rs. Zero quantum error filtering | filters.rs | R54 | Open |
| C23 | **ruQu contains two unrelated systems** — QEC (decoder/syndrome/surface_code) and coherence gate (filters/fabric/tile) share a crate name but have ZERO cross-references. "Qu" may mean "Quality" not "Quantum" | ruQu crate | R54 | Open |
| C24 | **"Transport-absent distributed protocol" — new pattern class.** All 5 files in ruvector-graph/src/distributed/ share the same defect: algorithm logic and state machines are correctly designed, but every network send is replaced with a debug log comment "In production, send actual network message". Zero socket I/O anywhere in the module. The distributed graph system is a design doc rendered as code | distributed/ (5 files) | R90 | Open |
| C25 | **tda.rs MISLABELED — 11th mislabeled file.** Named "Topological Data Analysis" but implements zero canonical TDA. No Vietoris-Rips complex, no boundary operators, no Betti numbers, no persistence diagrams. Implements graph metrics (connected components, clustering coefficient, diagonal-covariance degeneracy, multi-scale component counting). Misleading to any consumer expecting homology | tda.rs | R90 | Open |
| C26 | **coordinator.rs 2PC never transitions from Active.** TransactionState enum has Active/Preparing/Committed/Aborted. commit_transaction() removes the entry and logs — no prepare phase, no participant coordination, no WAL, no rollback. 2PC is type-system scaffolding only; the state machine is frozen at creation | coordinator.rs | R90 | Open |
| C27 | **rpc.rs all 4 RPC methods are stubs, gRPC feature-gated out.** RpcClient.execute_query() returns empty QueryResult with all-zero stats. RpcServer.start() logs a string. GraphRpcService and tonic::async_trait are gated behind cfg(feature="federation") absent from default Cargo.toml. In the standard build, zero gRPC code compiles | rpc.rs | R90 | Open |
| C28 | **compress.rs 12th MISLABELED FILE — "graph compression" is actually embedding/tensor quantization.** Zero GNN graph types (GraphEdge, NodeFeature, etc.) are imported or referenced. The file implements 5-tier access-frequency compression (hot/warm/cool/cold/archive) for embedding vectors and tensors. No compression of graph topology, adjacency, or edge weights whatsoever. Consumers expecting graph compression will find tensor quantization routines | compress.rs | R91 | Open |
| C29 | **compress.rs fake IEEE f16 — fixed-point *1000.0 substituted for real half-precision float.** WarmCompressor::compress_to_f16() multiplies by 1000.0 and stores as i16. This is lossy fixed-point with range ±32.767 (values ≥32.768 overflow silently). Real IEEE 754 binary16 has 5-bit exponent and full float semantics. Any consumer expecting f16 precision, exponent range, or NaN/Inf handling will get incorrect results | compress.rs | R91 | Open |
| C30 | **cuda-wasm/flash_attention.rs MISPLACED — pure CPU code in CUDA-WASM crate.** Zero cuBLAS, device memory, GPU kernels, or WASM bindgen. All computation on heap Vec<f32>. "HBM access" in docstring is conceptual, not actual. Multi-head loop is serial CPU, not GPU-parallel. Architecture mismatch between crate name and implementation | flash_attention.rs (cuda-wasm) | R92 | Open |
| C31 | **AIDefenceGuard.ts `aidefence` npm dependency declared but never imported.** package.json declares `aidefence@^2.1.1` dependency. Zero imports of this package appear in the 763-LOC file. The entire implementation is self-contained regex + heuristics. Misleading header implies external AI defense integration | AIDefenceGuard.ts | R92 | Open |

### 3b. HIGH Findings

| ID | Description | File(s) | Session | Status |
|----|-------------|---------|---------|--------|
| H1 | **PQ incomplete** — codebook training partial, missing PQ distance computation | ruvector-core simd_intrinsics.rs | C | Open |
| H2 | **ID translation overhead** — u64 internal ↔ string external mapping adds indirection at every operation | ruvector-core | C | Open |
| H3 | **Attention SIMD/Rayon no-ops** — Features declared but zero actual usage across 66 files | ruvector-attention (66 files) | B | Open |
| H4 | **Core HNSW wraps hnsw_rs** — Not novel implementation, but adds real value (SIMD, storage, concurrency) | ruvector-core | C | Open |
| H5 | **FP16 path not SIMD** — matmul.rs FP16 GEMV uses scalar `half` crate, NOT NEON FP16 intrinsics despite comments claiming it | kernels/matmul.rs | R22 | Open |
| H6 | **ANE naming misleading** — Functions named `*_ane` are SCALAR FALLBACKS. BNNS API limitations acknowledged in comments | ane_ops.rs | R35 | Open |
| H7 | **Architecture-complete, persistence-incomplete** — Model backends have correct math but incomplete weight loading. Only Candle backend can load models | Gemma2, Phi3, CoreML, Mistral backends | R35 | Open |
| H8 | **TL1 kernel LUT generation wrong** — Lookup table generation has incorrect mapping but NEVER CALLED in practice | tl1_kernel.rs | R35 | Open |
| H9 | **rlm_embedder HashEmbedder FAKE** — FNV-1a hash, not semantic. NO BitNet integration | rlm_embedder.rs | R35 | Open |
| H10 | **datamap.rs use-after-free risk** — unsafe slice::from_raw_parts with mmap lifetime issues | datamap.rs | R36 | Open |
| H11 | **Postgres healing timeout not enforced** — execute_with_safeguards() does NOT enforce timeout despite comment | healing/engine.rs | R36 | Open |
| H12 | **Postgres healing bgworker registration commented out** — worker.rs register_healing_worker() has bgworker registration COMMENTED OUT | healing/worker.rs | R36 | Open |
| H13 | **prime-radiant SIMD not enabled by default** — wide::f32x8 cfg-gated behind `simd` feature which is not in default features | prime-radiant | R37 | Open |
| H14 | **Training data augmentation simplistic** — tool_dataset and claude_dataset have weak paraphrasing (5 word pairs, literal replacement) | tool_dataset.rs, claude_dataset.rs | R37 | Open |
| H15 | **tile.rs witness hash only processes 6/255 worker reports** — Significant undersampling | tile.rs | R39 | Open |
| H16 | **planner.rs CliffordT overflow** — Silently becomes u64::MAX at t_count>40 | planner.rs | R39 | Open |
| H17 | **Two independent SIMD codebases** — ruvector-core (distance metrics) and edge-net (NN inference). Zero code reuse despite overlapping math (dot product in both) | simd_intrinsics.rs, simd.rs | R52 | Open |
| H18 | **subpolynomial expander splitting incomplete** — check_and_split_expander() has TODO "A full split would require more complex logic". Just marks invalid | subpolynomial/mod.rs | R52 | Open |
| H19 | **index_bench.rs code duplication** — Reimplements HNSW (280 LOC) and IVFFlat (200 LOC) internally instead of importing ruvector-core production code | index_bench.rs | R52 | Open |
| H20 | **subpolynomial fragmentation + witness imports unused** — Two supporting modules imported but never called. Incomplete integration | subpolynomial/mod.rs | R52 | Open |
| H21 | **transpiler.rs "noise-aware" claim FALSE** — Module comment claims noise-aware transpilation but ZERO noise modeling. No error rates, gate fidelities, or noise-adaptive compilation | transpiler.rs | R54 | Open |
| H22 | **TWO independent LoRA implementations** — micro_lora.rs (training, EWC++, sona) and lora.rs (inference, quantization, WASM, edge-net). Zero code sharing despite same algorithm | micro_lora.rs, lora.rs | R54 | Open |
| H23 | **THREE independent noise/noise_model files** — ruQu noise_model.rs (R37), ruqu-core noise.rs (R54), plus surface_code.rs noise handling. No single noise source of truth | noise.rs, noise_model.rs | R54 | Open |
| H24 | **subpoly_decoder.rs zero citations** — Unlike mod.rs (invalid arXiv), subpoly_decoder.rs has NO references, DOIs, or paper citations. Theoretical claims unsupported | subpoly_decoder.rs | R54 | Open |
| H25 | **federation.rs execute_on_cluster always empty.** The scatter-gather framework (FederationStrategy: Parallel/Sequential/Fallback) dispatches tokio::spawn correctly but every work unit returns an empty stub QueryResult. No real cross-cluster data flows | federation.rs | R90 | Open |
| H26 | **gossip.rs SWIM failure detector has no network transport.** join(), send_ping(), handle_ping(), handle_ack() correctly model SWIM with incarnation numbers and suspicion timeouts, but no socket write is ever made. Failure detection cannot fire across processes; protocol only works for in-process state tracking | gossip.rs | R90 | Open |
| H27 | **H1 RESOLVED — PQ now complete in product_quantization.rs.** H1 ("PQ incomplete — codebook training partial, missing PQ distance computation") was recorded against simd_intrinsics.rs. R90 confirms product_quantization.rs (88-92%) has complete k-means++ codebook training, encode(), and ADC LookupTable. The capability exists; it is in a different module than H1 assumed | product_quantization.rs | R90 | Resolved |
| H28 | **kv_cache/legacy.rs per-head scale stomping bug.** In the 4-bit quantization path, the per-head min and max scale values are recomputed and overwritten on every new-token append. Prior scale values are discarded, making it impossible to correctly dequantize previously quantized tokens after even one update. Dequantization of the full KV history is silently corrupted | kv_cache/legacy.rs | R91 | Open |
| H29 | **kv_cache/legacy.rs no eviction policy.** The KV cache grows unboundedly with no LRU, sliding window, or capacity limit. For long-context inference this will exhaust memory without graceful degradation. Contrast with ruvllm/kv_cache.rs which has a hot/cold tiering strategy | kv_cache/legacy.rs | R91 | Open |
| H30 | **mmap.rs pin count allocated but unused — no page eviction mechanism.** PinnedPage and pin_count fields are defined but pin_count is never incremented, checked, or used in any eviction guard. The mmap region is always available for unmapping regardless of whether callers have outstanding references | mmap.rs | R91 | Open |
| H31 | **speculative.rs sequential path verification negates tree parallelism benefit.** EAGLE tree decoding produces a draft tree of candidate continuations intended for parallel verification. This implementation verifies accepted tokens sequentially along the accepted path only. No batch forward pass over tree branches occurs. The novelty is in tree construction (lambda-guided confidence) but the parallel verification that makes EAGLE fast is not implemented | speculative.rs | R91 | Open |
| H32 | **compress.rs PQ codebook uses trivial linear interpolation, not k-means.** ColdCompressor::encode_pq() selects centroids by linear interpolation between min and max values (linspace). This produces uniform quantization, NOT vector quantization optimized to data distribution. Product quantization requires k-means or k-means++ training on representative data. The "PQ" labeling is misleading — this is uniform scalar quantization applied per subvector dimension | compress.rs | R91 | Open |
| H33 | **AIDefenceGuard.ts behavioral baseline in-memory only.** analyzeBehavior() computes 4 statistical features (length, punct, caps, digits) with hardcoded 2.0 deviation threshold. Baseline stored in ephemeral Map, lost on process restart. No persistence to AgentDB, DB, or any storage. Trivially gameable — not actual ML anomaly detection | AIDefenceGuard.ts | R92 | Open |
| H34 | **AIDefenceGuard.ts enablePolicyVerification ghost feature.** Valid config flag but zero policy verification logic exists in the file. Flag checked nowhere in analyze(). Config-level ghost — consumers enabling this option get zero additional protection | AIDefenceGuard.ts | R92 | Open |
| H35 | **lorentz_cascade.rs inference-only — no learning mechanism.** Curvatures and focal directions are fixed at initialization, never updated. head.weight is uniform. For a claimed "novel architecture," absence of backpropagation through hyperbolic operations limits to inference use only | lorentz_cascade.rs | R92 | Open |
| H36 | **hyperbolic-hnsw shard.rs ID mapping O(n²) search.** ShardedHyperbolicHnsw.search() uses linear scan over id_to_shard for every result per shard — O(|global_ids| × |shards| × k). Inefficient for large datasets; needs bidirectional map | shard.rs (hyperbolic-hnsw) | R92 | Open |
| H37 | **spike_driven.rs saturating_mul contradicts "multiplication-free" claim.** spike_value_contribution() uses saturating_mul — a multiplication — directly contradicting the neuromorphic energy advantage (Yao et al. 2023) | spike_driven.rs | R93 | Open |
| H38 | **spike_driven.rs energy_ratio hardcoded 30% average spike rate.** Reported 87.2x energy savings is a constant, not measured from actual spike density | spike_driven.rs | R93 | Open |
| H39 | **sparse_attention.rs partition boundaries are PLACEHOLDER.** estimate_partition_boundaries() uses uniform stride with comment "in practice would come from actual mincut partition info." The core value proposition (mincut-aware sparsity) does not hold at runtime | sparse_attention.rs | R93 | Open |
| H40 | **sparse_attention.rs _target_density and _gate dead params.** Adaptive density scheduling (3 strategies: Linear/Threshold/Adaptive) computes density but build_sparse_positions() ignores it. Window hardcoded at 4 | sparse_attention.rs | R93 | Open |
| H41 | **q15.rs u16/i16 type incompatibility with rope.rs.** q15.rs wraps u16 (unsigned), rope.rs uses i16 (signed for rotation). Same Q15 scale but divergent types — rope.rs cannot use Q15::from_f32() without sign loss | q15.rs, rope.rs | R93 | Open |
| H42 | **q15.rs newtype exported but UNUSED within crate.** rope.rs duplicates the scale constant as a raw const. Type-safe Q15 exists but nobody imports it | q15.rs | R93 | Open |
| H43 | **qgemm.rs compile-time-only SIMD dispatch.** No is_x86_feature_detected!() runtime guard. Generic cargo build without -C target-feature=+avx2 silently uses scalar on AVX2 machines | qgemm.rs | R93 | Open |
| H44 | **manager.rs no selective eviction.** Pure FIFO graduation (oldest token always moves to next tier). No H2O, attention-sink, StreamingLLM, or any attention-weighted eviction. Semantically critical mid-sequence tokens evicted identically to noise | manager.rs | R93 | Open |
| H45 | **manager.rs SQuat and KVQuant quantizers dead code.** Enum variants documented and used in memory estimates but squat_quantizer/kvquant_quantizer fields always None. Archive tier hardcoded to KIVI 2-bit regardless of config | manager.rs | R93 | Open |
| H46 | **manager.rs rematerialization wired but never triggered.** policy.rs RematerializationPolicy is complete and correct, but append()/attention() never call should_rematerialize() | manager.rs | R93 | Open |
| H47 | **ffn.rs single-scale approximation.** forward() uses w1_scales[0] for entire activation scale. Per-row scales from qgemm_i8 are ignored — incorrect for rows with significantly different scale values | ffn.rs | R93 | Open |
| H48 | **kvquant.rs 3-bit packing silently broken.** bits=3 branch in all three quantization paths falls back to 4-bit packing. Paper's "5.3x compression vs FP16" requires genuine 3-bit. Test checks formula, not actual bit density | kvquant.rs | R93 | Open |
| H49 | **early_exit.rs abs() overflow risk.** lambda_delta_abs computed as gate.lambda_delta().abs() as u32. i32::MIN.abs() panics in debug or wraps in release. Pattern duplicated in evaluate_exit_conditions() | early_exit.rs | R93 | Open |
| H50 | **early_exit.rs unnormalized logit verification.** verify_speculation() compares argmax of raw i32 logits without softmax. Draft token can pass even when full model's distribution is radically different | early_exit.rs | R93 | Open |
| H51 | **early_exit.rs adaptive/config threshold inconsistency.** calculate_adaptive_exit_layer() uses hardcoded 30000/25000 thresholds that differ from config.min_lambda_stability_q15 (28000 default). Can select earlier layer then refuse to exit there | early_exit.rs | R93 | Open |
| H52 | **mod_routing.rs route_unstable_tokens / route_stable_tokens byte-identical.** Both functions iterate left-to-right assigning Compute until target_count reached. Semantic distinction is managed by upstream capacity calc only. Divergence risk if one is modified | mod_routing.rs | R94 | Open |
| H53 | **mod_routing.rs boundary detection is stride heuristic, not actual mincut edges.** mark_boundary_tokens() uses regular stride intervals. Comment acknowledges "in practice would use actual boundary edge IDs from mincut" but GatePacket only carries aggregate stats, not per-token boundary membership | mod_routing.rs | R94 | Open |
| H54 | **quantized_store.rs per-layer counter vs per-head data.** warm_len[layer] is single counter shared across all heads. graduate_to_archive() assumes all heads at identical fill levels. Uneven head pushing corrupts per-head data by shifting misaligned byte ranges | quantized_store.rs | R94 | Open |
| H55 | **quant4.rs fully scalar, no SIMD in kernel/ directory.** In same directory as qgemm.rs (real AVX2+NEON), this file has zero SIMD intrinsics. Conspicuous quality gap | quant4.rs | R94 | Open |
| H56 | **quant4.rs BlockInt4Weights write-only.** No dequantize_row, no GEMV/GEMM method. Cannot be used for inference, only construction | quant4.rs | R94 | Open |
| H57 | **state.rs unchecked layer index — UB risk.** k_cache()/v_cache() use unbounded [start..] slice with no layer bounds check. Caller passing OOB layer_idx triggers undefined behavior | state.rs | R94 | Open |
| H58 | **state.rs no serialization support.** Zero Serialize/Deserialize derives, no safetensors, no save/load. State exists only for single inference session lifetime. No version tagging or migration path | state.rs | R94 | Open |
| H59 | **metrics.rs docstring claims "cache hit rates per tier" but file has zero hit-rate tracking.** QualityTracker tracks PPL/accuracy quality scores only. The "metrics" name and module doc are misleading | metrics.rs | R94 | Open |
| H60 | **replay.rs false KL divergence claim.** detect_distribution_shift() documents "Kullback-Leibler divergence approximation" but computes per-dimension standardized mean difference (Cohen's d analogue). Ignores variance of recent window entirely | replay.rs | R94 | Open |
| H60a | **arena.rs aliasing UB hazard.** alloc_i8/alloc_f32/alloc_i32 return &mut [T] slices derived from the SAME backing Vec<u8>. Rust borrow checker cannot track these across call sites — callers holding two live mutable slices simultaneously trigger undefined behavior. No lifetime coupling prevents concurrent mutable borrows of overlapping regions | arena.rs | R96 | Open |
| H60b | **squat.rs calibrate() STUB — ignores calibration_data entirely.** Real SQuAt paper uses PCA/SVD on actual KV distributions to learn the orthogonal basis. This implementation initializes a Hadamard basis then runs Gram-Schmidt (no-op on already-orthogonal Hadamard) — calibration_data accepted but discarded | kv_cache/squat.rs | R96 | Open |
| H60c | **kivi.rs per-channel quantization claim is FALSE.** quantize() dispatches on PerChannel/PerToken/PerGroup but computes global min/max across all dimensions regardless of QuantScheme. True per-channel quantization never implemented. PerChannel and PerToken treated identically | kv_cache/kivi.rs | R96 | Open |
| H60d | **kivi.rs SIMD dequantization explicit TODO placeholder.** dequantize_batch_avx2 contains a TODO comment and falls back to scalar loop. AVX2 intrinsics mentioned in comments but never implemented. File claims SIMD acceleration it does not provide | kv_cache/kivi.rs | R96 | Open |
| H60e | **hot_buffer.rs pop_oldest() BUG — no read cursor for sequential pops.** pop_oldest() assumes oldest_pos=0. After one pop, the next pop returns index 0 again (not the new oldest), producing stale data. No read_pos cursor maintained — repeated sequential pops are semantically broken | kv_cache/hot_buffer.rs | R96 | Open |
| H60f | **operators.rs only 3/10 attention types SQL-dispatchable.** ruvector_attention_score() collapses all non-ScaledDot/FlashV2 types to ScaledDot fallback. Linear, GAT, Sparse, MoE, Cross, Sliding, Poincare registered in ruvector_attention_types() but cannot be dispatched from SQL | attention/operators.rs | R96 | Open |
| H60g | **flash.rs is not a PostgreSQL function despite living in pgrx crate.** Zero #[pg_extern] macros in main code path. All computation on in-memory &[f32] slices, not SQL result sets or PostgreSQL data types. Unreachable from SQL | attention/flash.rs | R96 | Open |
| H60h | **flash.rs false O(sqrt(N)) space complexity claim.** Module docstring claims O(sqrt(N)) space. Actual: block_outputs Vec collects all blocks before combining — peak memory is O(N). Only KV-dimension tiling; query tiling abandoned (block_size_q dead_code) | attention/flash.rs | R96 | Open |
| H60i | **trace.rs feature gate claim is documentation-only — compiles unconditionally.** Module doc says "available only with trace feature flag" but no #[cfg(feature="trace")] attribute exists anywhere. TraceState and all diagnostics compile in every build | trace.rs | R96 | Open |
| H60j | **policy.rs age-based eviction only — NO H2O, attention-sink, StreamingLLM, or LRU.** should_evict() is solely positional age (total_tokens - position). No attention-score weighting. Semantically critical mid-sequence tokens evicted identically to noise tokens | kv_cache/policy.rs | R96 | Open |
| H60k | **policy.rs RematerializationPolicy evaluate/tracker pressure divergence.** evaluate() uses raw local pressure ratio rather than memory_tracker.pressure(). Tracker state and evaluate() can diverge if update_memory() not called before evaluate() | kv_cache/policy.rs | R96 | Open |
| H61 | **hyperbolic/adapter.rs Poincare-only despite "adapter" name — no Lorentz/hyperboloid conversion.** The file name implies interoperability between hyperbolic models (Poincare ball ↔ hyperboloid). No x₀=(1+‖x‖²)/(1-‖x‖²) or xᵢ=2xᵢ/(1-‖x‖²) maps exist. prime-radiant hyperbolic system is locked to one model | hyperbolic/adapter.rs | R97 | Open |
| H62 | **Prime-Radiant hyperbolic HNSW is O(n) brute-force stub.** adapter.rs search() and mod.rs similarity search both use linear scans over all vectors. index_built flag is written but never read — no actual HNSW index construction. Comment: "In production, this would use ShardedHyperbolicHnsw." Contrast with ruvector-hyperbolic-hnsw (genuine native HNSW, R92) | hyperbolic/adapter.rs, hyperbolic/mod.rs | R97 | Open |
| H63 | **HyperbolicEnergy::merge() silently corrupts cross-curvature aggregation.** Lines 179-187 merge two HyperbolicEnergy objects without checking that curvatures match. Energies computed under different curvatures (e.g., -1.0 and -2.0) are added directly — hyperbolic distances scale differently under different curvatures, so the merged total_energy is mathematically meaningless. No assertion or warning emitted | hyperbolic/energy.rs | R97 | Open |
| H64 | **exp_map uses Euclidean addition instead of Mobius addition — approximate geodesic stepping only.** adapter.rs exp_map (line 173) adds the tangent result directly to the base point using standard addition rather than Mobius addition ⊕_c. Standard Poincare exponential map requires x ⊕_c (tanh(√c·‖v‖/2)·v/‖v‖) to produce an exact geodesic step. The current implementation is an approximation that accumulates error across iterative Frechet mean steps | hyperbolic/adapter.rs | R97 | Open |
| H65 | **Prime-Radiant hyperbolic module disconnected from sheaf substrate.** hyperbolic/mod.rs has ZERO imports from prime-radiant's restriction.rs, coherence.rs, or knowledge_graph.rs. The hyperbolic geometry layer is an isolated add-on with no integration into the CSR sparse sheaf Laplacian or witness_log hash chains that define prime-radiant's core identity | hyperbolic/mod.rs | R97 | Open |
| H66 | **attention/mod.rs "39 mechanisms" claim is 4x inflation — only 10 enum variants defined.** AttentionType enum has 10 variants (ScaledDot, MultiHead, Flash, Linear, LocalGlobal, Hyperbolic, LorentzCascade, EdgeFeatured, MoE, Graph). Module doc claims 39. The 18+ "real implementations" count (R37) referred to impl files; the exported type system supports only 10 named types | attention/mod.rs | R97 | Open |
| H67 | **MinCut-Gated-Transformer TransformerConfig covers only ~40% of subsystem surface.** config.rs defines sequence, hidden, KV layer counts and GatePolicy, but has NO fields for: energy gate hyperparameters (alpha/beta/gamma EBT weights), spike-driven LIF/STDP neuron parameters (tau_m, tau_s, membrane threshold), quantization bit-width selection, KV cache tier boundaries (Hot/Warm/Archive capacity), or MoD routing thresholds (lambda_delta). Five out of nine DEEP-read subsystems are unconfigurable via config.rs | config.rs | R97 | Open |
| H68 | **kv_cache/tier.rs uses age-based (positional) tier assignment, not access-frequency.** TierBoundary.tier_for_position() computes age as (current_len - position - 1) and delegates to tier_for_age(). The Hot/Warm/Archive names suggest LRU or access-frequency semantics, but recent-position tokens are always "hot" regardless of how often they are accessed. Frequently accessed mid-sequence tokens still age into cold tiers | kv_cache/tier.rs | R97 | Open |
| H69 | **spike.rs is a scheduling/dispatch layer, not a spiking neuron implementation.** spike.rs contains SpikeScheduler for Q15-gated token dispatch — it reads SpikePacket (LIF output from spike_driven.rs) but has ZERO neuron dynamics: no membrane_potential, no spike threshold, no STDP weight updates. The file consumes spike signals produced elsewhere; it does not generate them | spike.rs | R97 | Open |
| H70 | **lorentz.rs (ruvector-postgres) has no manifold validation — distance() accepts off-hyperboloid points silently.** is_on_hyperboloid() is defined but never called internally. Callers passing arbitrary x₀²-‖x‖²≠1 points get numerically incorrect distances with no error or panic. Contrast with poincare.rs which clamps ‖x‖ to MAX_NORM=0.99999 | hyperbolic/lorentz.rs | R98 | Open |
| H71 | **lorentz.rs has no pgx annotations — no SQL exposure despite living in ruvector-postgres.** Zero #[pg_extern] macros. All functions are pure Rust and only accessible via operators.rs re-export. A SQL caller wanting a Lorentz-model distance function cannot reach it directly | hyperbolic/lorentz.rs | R98 | Open |
| H72 | **norm.rs is the only MinCut kernel with zero SIMD.** LayerNorm and RMSNorm are pure scalar loops while all sibling kernel files (qgemm.rs AVX2+NEON, ffn.rs Padé+Newton-Raphson) have real SIMD intrinsics. The missing optimization is conspicuous given that LayerNorm is on the critical path for every transformer forward pass | norm.rs | R98 | Open |
| H73 | **norm.rs RMSNorm feature-flag body duplication is dead code.** `#[cfg(feature = "rmsnorm")]` block and the unconditional block contain byte-identical implementations. Any change to one must be manually mirrored to the other — a maintainability trap. The feature flag provides no behavioral variation | norm.rs | R98 | Open |
| H74 | **depth.rs curvature parameter stored but never modulates calculations — effectively hardcoded -1.0.** HyperbolicDepth.curvature field exists and is passed through constructors, but sqrt(-c) in the depth formula always evaluates to sqrt(1.0)=1.0 when c=-1.0 (the only tested value). All 6 tests use curvature=-1.0; multi-curvature behavior is untested and likely incorrect | hyperbolic/depth.rs | R98 | Open |
| H75 | **depth.rs level thresholds hardcoded with no mathematical justification.** Thresholds [0.5, 1.0, 2.0, 3.0] determining Root/High/Mid/Deep/VeryDeep hierarchy levels are magic numbers. No derivation from curvature, dimensionality, or empirical calibration. Changing curvature without changing thresholds produces meaningless level assignments | hyperbolic/depth.rs | R98 | Open |
| H76 | **config.rs HNSW parameters are dead configuration — the HNSW back-end is never used.** R97 confirmed prime-radiant hyperbolic search is O(n) brute-force with "In production would use ShardedHyperbolicHnsw" comment. HyperbolicConfig.hnsw_m (16) and hnsw_ef_construction (200) are Euclidean-HNSW defaults, not Poincare-tuned, and are never read by any search or index-build code | hyperbolic/config.rs | R98 | Open |
| H77 | **tangent.rs TangentPruner.search() is O(N) linear scan by design — no HNSW graph traversal.** search() iterates over the entire vector store to build the candidate pool before pruning. The pruner is designed to receive candidates from caller-level HNSW traversal, but no caller wires HNSW output into it. Standalone use is O(N) brute-force despite the file being part of an HNSW crate | tangent.rs | R99 | Open |
| H78 | **tangent.rs dead import: norm_squared imported from poincare.rs but never called.** The symbol appears in the use statement at the top of the file and is never referenced in any function body. Minor dead-code signal | tangent.rs | R99 | Open |
| H79 | **gnn-node/lib.rs hierarchical_forward() deserializes GNN layer configs from JSON strings on every call.** Each invocation of hierarchical_forward() on the NAPI binding parses a JSON string (GnnLayerConfig) to reconstruct the layer configuration. For high-frequency inference loops this adds serde deserialization overhead that could be avoided by caching the parsed config or accepting a typed NAPI object | crates/ruvector-gnn-node/src/lib.rs | R99 | Open |
| H80 | **GNN binding layers (node + wasm) export inference-only APIs — training APIs absent from FFI surface.** Neither gnn-node/lib.rs nor gnn-wasm/lib.rs exposes train(), backprop(), update_weights(), or any gradient computation method. The ruvector-gnn core crate contains training code (confirmed R94: scheduler.rs, replay.rs) but it is unreachable from Node.js or WASM consumers | crates/ruvector-gnn-node/src/lib.rs, crates/ruvector-gnn-wasm/src/lib.rs | R99 | Open |
| H81 | **hyperbolic_attention.rs adaptive_curvature is a dead field.** The `adaptive_curvature` field is set at construction from a config flag but never updated during compute(). Curvature is effectively fixed at construction time despite the "adaptive" naming, negating the ability to dynamically modulate the hyperbolic geometry | hyperbolic_attention.rs (ruvector-attention) | R100 | Open |
| H82 | **mixed_curvature.rs linear blend of two softmax distributions is mathematically imprecise.** Blending two independent softmax-normalized score distributions with scalar weights does not produce a proper mixture model — the result is only approximately normalized and loses the geometric meaning of each manifold's attention distribution. Renormalization rescues numerical correctness but not semantic rigor | mixed_curvature.rs (ruvector-attention) | R100 | Open |
| H83 | **attention/linear.rs is a PLACEHOLDER with no O(n) kernel math — deflates MinCut linear attention novelty.** LinearAttention struct exists with config fields but has NO forward() method and NO ELU+1 feature map approximation (the key contribution of Katharopoulos 2020). The docstring cites the paper and claims O(n) complexity but the implementation is entirely empty. Hidden behind the `linear_attention` feature gate so it does not appear in default builds. Any claim that MinCut-Gated-Transformer supports linear attention is false | attention/linear.rs (ruvector-mincut-gated-transformer) | R100 | Open |
| H84 | **lib.rs (ruvector-hyperbolic-hnsw) has no bounds check on shard index in tangent pruning.** The tangent-space pruning path computes log_c(x) at a shard centroid keyed by hierarchy depth without validating that the depth value corresponds to an existing shard. An out-of-range depth would index into an invalid shard, producing a panic or incorrect centroid. No defensive bounds check or fallback is present | lib.rs (ruvector-hyperbolic-hnsw) | R100 | Open |
| H85 | **Postgres hyperbolic/ has ZERO manifold validation at any layer** — operators.rs passes points to PoincareBall.distance() without checking |x|<1. Combined with R98 H70 (lorentz.rs accepts off-hyperboloid points), the entire postgres hyperbolic SQL API has no point-on-manifold enforcement. Invalid inputs produce silently wrong distances | hyperbolic/operators.rs, hyperbolic/lorentz.rs | R101 | Open |
| H86 | **mmap_fixed.rs advertises MmapManager + MmapGradientAccumulator but both are ABSENT** — module doc describes two structs, imports memmap2/UnsafeCell/RwLock/File but only AtomicBitmap exists. "Fixed" mmap management was planned but never implemented | mmap_fixed.rs (ruvector-gnn) | R101 | Open |
| H87 | **Healing engine global OnceLock singleton limits per-connection isolation** — HEALING_ENGINE is process-wide OnceLock<Arc<RwLock<HealingEngine>>>. All PostgreSQL connections share one healing state. Per-schema or per-connection healing policies impossible | healing/mod.rs | R101 | Open |
| H88 | **OutcomeTracker cloned into RemediationEngine AND kept as self.tracker — diverging state risk** — If OutcomeTracker is not Arc-backed, the two instances diverge after first mutation. Healing outcomes may be recorded in one copy but not the other | healing/mod.rs | R101 | Open |
| H89 | **ruvector_healing_set_thresholds takes read lock for mutation** — functions.rs uses engine.read() (shared lock) then calls detector.update_thresholds(). ruvector_healing_configure correctly uses engine.write(). Potential soundness issue if update_thresholds requires &mut self | healing/functions.rs | R101 | Open |
| H90 | **ruvector_message_pass is FACADE — returns human-readable format string** — pg_extern function accepting node_table, edge_table, embedding_col, hops, layer_type returns "Multi-hop gcn message passing over N hops from table X..." with zero SQL execution | gnn/operators.rs | R101 | Open |
| H91 | **Postgres GNN is self-contained reimplementation, NOT ruvector-gnn composition** — gnn/operators.rs imports from super::gcn, super::graphsage, super::aggregators (local postgres siblings). Zero imports from the ruvector-gnn crate (11+ DEEP files from R91/R94/R99). Two independent GNN ecosystems coexist | gnn/operators.rs, gnn/gcn.rs | R101 | Open |
| H92 | **gcn.rs has no SQL integration despite living in ruvector-postgres** — Purely in-memory Vec<Vec<f32>> operations. Graph topology passed as &[(usize, usize)] edge lists. The SQL bridge presumably lives in workers/gnn.rs (not yet read) | gnn/gcn.rs | R101 | Open |
| H93 | **gcn.rs missing self-loops — pure neighbor aggregation, not canonical GCN** — update() ignores _node_features parameter. Full Kipf & Welling requires A+I (adjacency + identity) before normalization to include node's own features. Current implementation is neighbor-only aggregation | gnn/gcn.rs | R101 | Open |
| H94 | **GCN/GraphSAGE use deterministic pseudo-random weights, no trained weight loading** — GCNLayer::new() uses val = ((i * out_features + j) * 0.01) % 1.0. Comment labels it "Simple deterministic initialization for testing." No weight loading path exists — all postgres GNN inference produces structurally consistent but semantically meaningless results | gnn/operators.rs, gnn/gcn.rs | R101 | Open |

### 3c. MEDIUM Findings

| ID | Description | File(s) | Session | Status |
|----|-------------|---------|---------|--------|
| M1 | **tangent.rs test coverage minimal — only 2 tests for full two-phase pruning pipeline.** Tests verify construction and a single round-trip through the tangent-space filter + Poincare re-rank, but do not cover edge cases: empty candidate pools, single-vector stores, curvature changes mid-run, or TangentCache invalidation after insertions | tangent.rs | R99 | Open |
| M2 | **gnn-wasm/lib.rs browser-targeted tests are construction-only — no round-trip coverage.** Tests in the #[cfg(test)] block verify that WasmGnnGraph::new() constructs without panic and that WasmCompressor::new() initializes, but no test exercises forward(), compress()+decompress() round-trip, or differentiable_search() output correctness. Browser-targeted wasm_bindgen tests cannot run under standard cargo test | crates/ruvector-gnn-wasm/src/lib.rs | R99 | Open |
| M3 | **gnn-node/lib.rs WasmGnnGraph and WasmCompressor compression tiers use 5-tier enum with no validation of tier ordinal.** CompressionTier::from_u8() uses a match on hardcoded u8 values but does not reject unknown tier values — out-of-range input defaults silently. NAPI callers passing arbitrary integers receive the default None tier rather than an error | crates/ruvector-gnn-node/src/lib.rs | R99 | Open |
| M4 | **poincare.rs (ruvector-attention) frechet_mean uses a fixed learning rate of 0.1 with no adaptive scheduling.** At high curvature values the Riemannian gradient magnitudes grow, and a fixed 0.1 step can overshoot the Poincare ball boundary, requiring repeated projection. No damping, decay, or line search is applied. This may cause oscillation in the Fréchet mean computation for embeddings near the ball boundary | poincare.rs (ruvector-attention hyperbolic) | R100 | Open |
| M5 | **mixed_curvature.rs [euclidean \|\| hyperbolic] packing is an implicit unenforced API contract.** The module assumes that input embeddings are packed with the first half being Euclidean coordinates and the second half being Poincare-ball coordinates. This split is not validated, not type-enforced, and not documented in the function signatures. Callers passing unsplit embeddings silently receive nonsensical mixed-geometry attention scores | mixed_curvature.rs (ruvector-attention) | R100 | Open |

## 4. Positives Registry

| Description | File(s) | Session |
|-------------|---------|---------|
| **temporal-tensor production-ready** — 95% complete, 125+ tests, 4-tier quantization, CRC32 integrity, SVD reconstruction. BEST crate in repo | temporal-tensor (6 files) | Initial, R22, R37 |
| **ruvector-postgres SIMD** — BEST SIMD in ecosystem. Real AVX-512/AVX2/NEON intrinsics with 4x unrolling, simsimd integration, 23 tests | distance/simd.rs | R22 |
| **ruvllm BitNet backend** — Complete 1-bit LLM inference. MLA 17.8x memory reduction (genuine innovation) | bitnet/backend.rs | R22 |
| **ruvllm kernels** — Production BLAS-level GEMM micro-kernels, textbook Flash Attention 2, Apple Accelerate + Metal GPU integration | kernels/attention.rs, matmul.rs | R22 |
| **ruQu QEC genuine** — Union-Find O(α(n)) decoder, real AVX2 SIMD, complete surface code. Top 15% quality in ecosystem | ruQu (7 files) | R37, R39 |
| **cognitum-gate-kernel exceptional** — 93%, rivals neural-network-implementation. Anytime-valid e-process, custom bump allocator, 64-byte cache-line aligned | cognitum-gate-kernel (5 files) | R36 |
| **prime-radiant coherence substrate** — Sheaf-theoretic memory, BEST sparse matrix (CSR 6 formats), blake3 hash chains | prime-radiant (5 files) | R37 |
| **ruvector-attention real** — 18+ implementations (Flash, Hyperbolic, MoE, Graph, Sheaf, OT, PDE) across 66 files | ruvector-attention | B |
| **sona production-ready** — 85% complete. MicroLoRA + EWC++ + federated learning + SafeTensors export | sona | R13 |
| **ruvector-gnn custom hybrid** — GAT+GRU+edge, full EWC, not a wrapper. ~6,000 LOC | ruvector-gnn | C |
| **postgres extension substantial** — 290+ SQL functions, 3 vector types, rivals pgvector in feature scope | ruvector-postgres | Initial |
| **Raft complete** — Pre-vote, snapshots, dynamic membership, linearizable reads | ruvector-raft | Initial |
| **graph Cypher parser production-quality** — 1,296-line recursive descent, correct lexer | ruvector-graph | C |
| **micro-hnsw-wasm novel** — Genuine `#![no_std]` HNSW in <12KB with neuromorphic extensions | micro-hnsw-wasm | Initial, R36 |
| **hyperbolic HNSW** — Tangent space pruning for Poincare ball geometry | hyperbolic-hnsw | Initial |
| **NAPI bindings well-structured** — Proper async, multi-platform CI, 7+ targets | ruvector-node, gnn-node, sona | Initial |
| **HNSW patches correct algorithm** — Malkov & Yashunin with Rayon parallel insertion, 4 I/O format versions | hnsw_rs fork | R36 |
| **ruvllm memory_pool** — BEST systems code. Lock-free bump allocator, RAII buffer pool, per-thread scratch. 95% real | memory_pool.rs | R34 |
| **postgres SPARQL executor** — COMPLETE SPARQL 1.1 query engine. Property paths, all 7 aggregates, 30+ expression types | sparql_executor.rs | R34 |
| **mincut wrapper** — Genuine bounded-range decomposition from arXiv:2512.13105 (Dec 2024). Among best algorithmic code | wrapper/mod.rs | R34 |
| **edge-net P2P production libp2p** — REVERSES R42's "NO P2P transport". Real Gossipsub/Kademlia/RequestResponse, NOISE encryption, 6 topics, direct RAC integration via broadcast_rac_event() | p2p.rs (edge-net) | R44 |
| **advanced.rs SNN excellence** — 95% real LIF neuron model with STDP Hebbian learning, refractory periods. Highest quality neural component in file | advanced.rs (edge) | R44 |
| **swarm.rs cryptographic envelope** — 88% real Ed25519+AES-256-GCM, nonce replay protection, counter ordering, registry-based identity (never trusts envelope keys) | swarm.rs (edge) | R44 |
| **ruvllm autodetect** — Real hardware feature detection with platform-specific probes. 92% real with 27 tests | autodetect.rs | R34 |
| **ruvllm scheduler** — vLLM-style continuous batching, preemption (recompute+swap), chunked prefill. BEST serving code | scheduler.rs | R35 |
| **ruvllm kernel extensions exceptional** — norm.rs, rope.rs, quantized.rs, activations.rs all 92-95% with real NEON | 4 files | R35 |
| **hnsw_router.rs BEST ruvector-core integration** — Real HnswIndex, HybridRouter blends semantic+keyword | hnsw_router.rs | R37 |
| **micro_lora.rs BEST learning code** — NEON 8x unroll, EWC++ Fisher penalty, <1ms forward. 92-95% real | micro_lora.rs | R37 |
| **sparse.rs** — 95%, 4 sparse formats (CSR/CSC/COO/Graph), no_std compatible. BEST matrix code | sparse.rs | R28 |
| **SPARQL parser production W3C** — 93-95%, 2,496 LOC recursive-descent. All 4 query forms, property paths, aggregates, 33+ built-in functions, UPDATE operations. ruvector has BOTH parser AND executor for SPARQL | sparql/parser.rs | R52 |
| **edge-net SIMD production-quality** — 92-95%, real AVX2/WASM/SSE4.1 intrinsics. Numerically stable softmax (log-sum-exp), Welford layer norm, Q4/Q8 quantization, 19 tests. Independent from ruvector-core SIMD | simd.rs (edge-net) | R52 |
| **Hyperbolic HNSW native implementation** — 88-93%, NOT wrapping hnsw_rs. Genuine Poincaré ball distance in all graph traversals, tangent-space two-phase pruning (cheap Euclidean filter → exact geodesic top-N), DualSpaceIndex with Reciprocal Rank Fusion, runtime-configurable curvature | hnsw.rs (hyperbolic-hnsw) | R92 |
| **poincare.rs mathematically correct Möbius operations** — 90-95%, every formula verified against Ganea et al. 2018. Möbius addition, scalar multiplication, exp/log maps, parallel transport, Fréchet mean (Riemannian GD), fused norms (3x optimization), comprehensive EPS guards, fast_acosh 3-regime approximation, 7 tests covering algebraic invariants | poincare.rs | R92 |
| **lorentz_cascade.rs genuine Lorentz model attention** — 90-93%, correct Minkowski inner product ⟨x,y⟩_L = -x₀y₀ + Σxᵢyᵢ, Busemann function scoring (O(d) and encodes hierarchy), multi-curvature cascade (log-spaced c_min to c_max), Einstein midpoint centroid, correct log/exp maps on hyperboloid. One of most mathematically rigorous files in ecosystem | lorentz_cascade.rs | R92 |
| **hyperbolic-hnsw-wasm 17th GENUINE WASM** — 88-92%, production wasm_bindgen wrapper exposing 7 real math operations. HyperbolicIndex with insert/search/batch, ShardedIndex with per-shard curvature + canary deployment, WasmTangentCache. Zero stubs, zero console.log facades | lib.rs (hyperbolic-hnsw-wasm) | R92 |
| **AIDefenceGuard.ts genuine regex-based security** — 82-88%, 28 hand-crafted injection patterns, 8 jailbreak patterns, 6 PII categories, Unicode homoglyph normalization (Cyrillic defense), control character sanitization, response-side injection detection. Clean middleware factory pattern | AIDefenceGuard.ts | R92 |
| **cuda-wasm flash_attention.rs textbook Flash Attention v2** — 88-92% algorithm quality, correct tiled forward pass with online softmax (Milakov & Gimelshein 2018), causal masking, SRAM budget formula matching paper Section 3.1, 7 comprehensive tests including naive reference cross-validation | flash_attention.rs (cuda-wasm) | R92 |
| **hnswio.rs BEST-IN-CLASS persistence** — 95-98%, dual-file format (graph+data), hybrid mmap strategy, 4 backward-compatible versions, zero-copy serialization, concurrent safety | hnswio.rs | R52 |
| **hnsw.rs vendored upstream quality** — 98-100%, complete Malkov & Yashunin 2018. Rayon parallel insert, FilterT filtered search, LayerGenerator exponential sampling. Battle-tested upstream code | hnsw.rs | R52 |
| **energy_gate.rs GENUINE Energy-Based Transformer gate — R34 "MOST NOVEL" CONFIRMED** — 88-93%, 3-component weighted energy function (sigmoid lambda + boundary penalty + partition entropy), genuine central-difference gradient with sign-correct test assertions, System-2 iterative refinement via gradient descent on lambda. Novel connection between graph-cut coherence and gate policy. Cites Gladstone 2025 | energy_gate.rs | R93 |
| **spike_driven.rs genuine neuromorphic attention** — 88-93%, authentic LIF neuron rate coding with membrane accumulate-and-reset, correct spike-timing-dependent plasticity (STDP) coincidence detection, refractory period enforcement. 12 tests. Not renamed softmax — binary spike trains with temporal coincidence | spike_driven.rs | R93 |
| **qgemm.rs genuine quantized GEMM** — 88-93%, real AVX2 widening i8→i16→i32 multiply-accumulate (correct madd_epi16, not maddubs), real NEON dual-accumulator pipeline, overflow-safe i64 scalar path, correct asymmetric per-row quantization matching llama.cpp convention. 4 non-trivial tests | qgemm.rs | R93 |
| **ffn.rs SIMD GELU + allocation-free guarantee** — 88-92%, genuine Padé tanh approximation (27+x²)/(27+9x²) in AVX2, Newton-Raphson reciprocal in NEON. Forward pass takes all scratch buffers as parameters, zero Vec allocations. Correct INT8 quantized inference pipeline | ffn.rs | R93 |
| **early_exit.rs novel lambda-stability early exit** — 88-92%, genuine LayerSkip alternative using precomputed mincut lambda signals. Novel 40/40/20 confidence weighting (lambda-strength + stability + boundary dispersion). Cites Elhoushi et al. 2024. 10 tests covering all ExitReason variants | early_exit.rs | R93 |
| **q15.rs correct fixed-point arithmetic** — 88-92%, textbook Q15 with transparent u16 newtype. saturating_mul correctly widens to u32 before >>15. Direction-aware LERP. 11 tests over 101 values — strongest test coverage in crate | q15.rs | R93 |
| **kvquant.rs genuine pre-RoPE insight** — 78-85%, only KVQuant pre-RoPE implementation in ruvnet ecosystem. Correct deferred RoPE application at attention time. Outlier detection via percentile sort. Cites Hooper et al. 2024 (arXiv:2401.18079) | kvquant.rs | R93 |
| **ruqu-core noise.rs BEST-IN-CLASS** — 96-98%, production Kraus operator formalism. 4 noise channels (depolarizing, amplitude damping, phase damping, thermal relaxation). Hardware calibration pipeline (T1/T2→γ/λ). 498 test lines. Comparable to Qiskit Aer | noise.rs | R54 |
| **ruqu-core mitigation.rs publication-quality** — 95-98%, three NISQ-era strategies (ZNE, Measurement Error, CDR). Richardson exact extrapolation, tensor-product calibration optimization, Clifford data regression. 40+ tests at 1e-12 precision | mitigation.rs | R54 |
| **ruqu-core transpiler.rs BEST-IN-CLASS** — 95-98%, complete 3-phase quantum circuit transpiler. 3 hardware backends (IBM Eagle, IonQ Aria, Rigetti Aspen), BFS qubit routing, 2-level optimization, 44 tests. Mid-tier Qiskit equivalent | transpiler.rs | R54 |
| **fabric.rs production orchestration** — 93-96%, 256-tile WASM fabric. Blake3 cryptographic audit trails, surface code topology generator, 2μs latency target, 23 tests | fabric.rs | R54 |
| **edge-net federated.rs BEST federated learning** — 95-98%, Byzantine-robust MAD+median, (ε,δ)-DP Gaussian mechanism, TopK compression with error feedback (arXiv:1712.01887), reputation-weighted FedAvg with superlinear weighting, WASM cross-platform. Exceeds sona (85%) | federated.rs | R54 |
| **edge-net lora.rs production edge LoRA** — 90-95%, complete Low-Rank Adaptation with dual SIMD (AVX2+WASM128), Q4/Q8 quantization (4-8× memory reduction), LRU adapter pool with task routing, online gradient accumulation, P2P serialization, 15 tests | lora.rs | R54 |
| **product_quantization.rs genuine PQ** — 88-92%, real k-means++ initialization (D² weighting), Lloyd's algorithm (assignment + centroid update), asymmetric distance computation (ADC) with LookupTable per subspace. RESOLVES H1 | product_quantization.rs | R90 |
| **conformal_prediction.rs valid statistical guarantees** — 88-93%, textbook split-conformal procedure (Vovk et al. 2005). Correct calibration/inference separation, Bonferroni-corrected quantile, 3 nonconformity measures (distance threshold, inverse rank, normalized distance), 7 tests | conformal_prediction.rs | R90 |
| **hypergraph.rs genuine bipartite hypergraph** — 85-90%, correct incidence representation (entity_to_hyperedges + hyperedge_to_entities HashMaps), real k-hop BFS over hyperedge-mediated paths, causal utility function with log1p uplift, temporal expiry. Cites HyperGraphRAG (NeurIPS 2025) | hypergraph.rs | R90 |
| **shard.rs EdgeCutMinimizer multilevel KL** — 70-80%, genuine 3-phase multilevel k-way partitioning: heavy-edge coarsening, greedy initial partition, Kernighan-Lin local search (10 iterations). Real xxh3_64 + blake3 hashing, RangePartitioner with binary search | shard.rs (distributed) | R90 |
| **mmap.rs production memmap2 with lock-free AtomicBitmap** — 88-92%, real memmap2 file-backed memory mapping, Linux madvise(MADV_WILLNEED) prefetch, AtomicBitmap with CAS-based lock-free bit set/clear, RwLock granularity for MmapGradientAccumulator. 17 genuine tests. Clean `#![cfg(not(wasm32))]` gating | mmap.rs | R91 |
| **speculative.rs novel lambda-guided confidence from mincut signal** — 88-92%, EAGLE-style tree speculative decoding with textbook rejection sampling (min(1, target/draft)), adaptive tree width controlled by confidence threshold, correct tree attention mask generation. Unique integration of mincut boundary signal as lambda-guided confidence weight not seen in reference EAGLE implementations | speculative.rs | R91 |
| **rope.rs correct NTK-aware scaling with Q15 quantized path** — 88-92%, faithful implementation of Su et al. 2021 RoPE with CodeLlama/Qwen NTK-aware scaling formula, Q15 fixed-point quantized inference path for edge deployment, 11 substantive tests. No inflated SIMD claims | rope.rs | R91 |
| **kv_cache/legacy.rs RotateKV with Fast Walsh-Hadamard Transform** — 82-88%, genuine RotateKV rotation (IJCAI 2025 paper), correct 2-bit and 4-bit quantization with proper bit-packing, FWHT rotation for key diversity. 15 genuine tests | kv_cache/legacy.rs | R91 |
| **packets.rs coherence interface — architectural glue for MinCut** — 90-95%, pure type-definition layer connecting energy gate, spike scheduler, and transformer kernel. GatePacket carries lambda/boundary/partition signals. 5-level GateDecision hierarchy with novel QuarantineUpdates (speculative inference discarding all state changes). SpikePacket bridges LIF+STDP output to attention. repr(C) throughout with correct Q15 fixed-point convention. 8-entry boundary_edge_ids side-channel | packets.rs | R94 |
| **quantized_store.rs two-tier KIVI quantized KV cache** — 88-92%, correct warm (4-bit) → archive (2-bit) graduation pipeline. Asymmetric quantization (PerChannel keys, PerToken values) per KIVI paper. Correct byte packing formula (head_dim * bits + 7) / 8. O(n) copy_within for warm-shift | quantized_store.rs | R94 |
| **state.rs zero-allocation buffer layout** — 88-92%, single contiguous Vec with computed offsets (Q/K/V/attn_scores/FFN/residual/norm/K_cache/V_cache), 64-byte alignment for cache lines. KV ring buffer with correct modular write indices. Genuine inference infrastructure | state.rs | R94 |
| **replay.rs correct Vitter/Knuth reservoir sampling** — 88-92%, Algorithm R with decreasing replacement probability (capacity/total_seen). Partial Fisher-Yates O(batch_size) sampling. Welford online mean/variance with Bessel correction. 12 substantive tests including reservoir distribution validation | replay.rs | R94 |
| **scheduler.rs five correct LR scheduling algorithms** — 82-88%, CosineAnnealing with warm restarts (SGDR, Loshchilov & Hutter 2017), WarmupLinear two-phase, ReduceOnPlateau with epsilon threshold, StepDecay, Exponential. 9 tests with exact float verification | scheduler.rs | R94 |
| **arena.rs genuine bump allocator with cache-line alignment** — 92%, single Vec<u8> backing store with manual offset tracking and 64-byte alignment via bit-mask arithmetic. WeightRef stores u32 offset+size for serialization-safe weight indexing. QKV+FFN weight layout encoded in calculate_arena_size. SAFETY comments on all unsafe blocks are accurate and correct | arena.rs | R96 |
| **squat.rs SQuAt 2024 paper Hadamard math genuine** — 78-84%, correct Kronecker product recursion for Walsh-Hadamard matrix, orthogonality verified by test (self-dot=1.0, cross-dot=0.0), correct gram_schmidt() with 1e-8 norm guard. 2-bit/4-bit packing with per-subspace asymmetric min-max quantization complete | kv_cache/squat.rs | R96 |
| **kivi.rs FWHT implementation genuine and self-inverse** — 72-78%, Fast Walsh-Hadamard Transform with correct butterfly operations and 1/sqrt(n) normalization. Self-inverse property correctly noted. Power-of-2 dimension enforced by assertion. Strongest algorithmic contribution in the file | kv_cache/kivi.rs | R96 |
| **operators.rs genuine pgx extension — CORRECTS R91 skepticism** — 88-92%, all 5 public functions carry #[pg_extern(immutable, parallel_safe)] — real PostgreSQL extension entry points. Dispatches to ScaledDotAttention, FlashAttention, MultiHeadAttention Rust structs via Box<dyn Attention> trait. 6 genuine pg_test tests running inside live PostgreSQL instance | attention/operators.rs | R96 |
| **flash.rs textbook online softmax algorithm** — 72-78%, global_max tracked across KV blocks with correction factor exp(block_max - global_max) applied per block. Core online softmax trick from Flash Attention paper correctly implemented. 7 tests including naive reference cross-validation | attention/flash.rs | R96 |
| **trace.rs stack-allocated gate diagnostics** — 88-92%, TraceState uses fixed-size [T;64] stack arrays as circular rolling buffer with zero heap allocation in hot path. Tier 0-3 classification consistent with packets.rs GateDecision semantics. intervention_rate() correctly subtracts skipped from denominator. 6 genuine unit tests | trace.rs | R96 |
| **Prime-Radiant hyperbolic Poincare math correct** — hyperbolic/mod.rs correct d(x,y) formula, correct log map with conformal factor lambda_base, Frechet mean via Riemannian GD, 17 tests. poincare_distance() guards denominators with .max(EPS). hyperbolic/energy.rs correct depth formula 2*arctanh(‖x‖)/sqrt(-c) and weighted energy formula (R97 confirms R92) | hyperbolic/mod.rs, hyperbolic/energy.rs | R97 |
| **attention/scaled_dot.rs real simsimd SIMD with numerically stable softmax** — 90-93%, f32::dot() via simsimd with Option<f64> fallback, correct QK^T/sqrt(d_k) scale stored at construction, max-subtraction softmax (fold NEG_INFINITY), 9 unit tests + 2 pg_tests. Comprehensive test coverage including forward pass shapes and edge cases | attention/scaled_dot.rs | R97 |
| **attention/multi_head.rs genuine Rayon parallel MHA** — 88-92%, real into_par_iter() for head-parallel forward pass, correct score averaging across heads, pgrx PostgresEnum for AttentionType (genuine pgrx integration). Building-block pattern with Vec<ScaledDotAttention> per head | attention/multi_head.rs | R97 |
| **kv_cache/tier.rs clean 3-tier KV cache definitions** — 90-93%, Hot=FP16/16-bit, Warm=4-bit KIVI, Archive=2-bit KIVI/SQuat/KVQuant. Correct memory_bytes() formulas (Hot=2B/elem, Warm=0.5B+4B scale, Archive=0.25B+4B scale). 9 unit tests covering tier_bits, compression ratios, boundary defaults, edge cases. Aligns perfectly with quantized_store.rs two-tier KIVI (R94) | kv_cache/tier.rs | R97 |
| **spike.rs coherent scheduling with packets.rs** — 88-92%, SpikeScheduler correctly reads SpikePacket from packets.rs (DEEP R94), Q15 rate-tiering (three thresholds: skip/medium/high), FNV-1a novelty hashing for input signature. no_std compatible (extern crate alloc). 7 tests covering all scheduling branches | spike.rs | R97 |
| **poincare.rs (ruvector-postgres) genuine hyperbolic geometry in Postgres** — 88-92%, real Poincare ball distance (acosh formula), Mobius addition, exp_map, log_map, all with correct curvature scaling and EPSILON=1e-8/MAX_NORM=0.99999 numerical guards. 13 unit tests validate symmetry, inverse properties, curvature effects. Clean clamping prevents NaN divergence | hyperbolic/poincare.rs (ruvector-postgres) | R98 |
| **lorentz.rs correct Lorentz/Minkowski model with bidirectional coordinate transforms** — 87-92%, correct inner product (-x0y0+x1y1+...), hyperboloid acosh distance, bidirectional Poincare-Lorentz conversions with dimension augmentation, simsimd SIMD dot product. 13 tests including cross-model distance equivalence validation | hyperbolic/lorentz.rs (ruvector-postgres) | R98 |
| **lib.rs EXCELLENT MinCut crate root — all DEEP modules accessible** — 90-95%, 23 public modules (18 unconditional + 5 feature-gated), 60+ exported items via prelude, 10 feature flags with semantic groupings (default/full/novel), dual KV cache re-export for backward compat, academic citations in rustdoc, working code example. Confirms no hidden silos in the crate | lib.rs (ruvector-mincut-gated-transformer) | R98 |
| **depth.rs correct Poincare depth formula implementation** — 78-82%, textbook formula depth = 2*arctanh(|x|)/sqrt(-c) correctly implemented, silent clamping for out-of-ball points prevents panic, HierarchyLevel enum with 5 levels. 6 passing tests | hyperbolic/depth.rs (prime-radiant) | R98 |
| **tangent.rs genuine two-phase Poincare pruning — COMPLETES hyperbolic-hnsw crate** — 88-92%, TangentPruner implements correct Fréchet mean centroid (iterative Riemannian GD via poincare.rs log_map/exp_map) as TangentCache origin, cheap Euclidean distance filter in tangent space to reduce arcosh calls, exact Poincare re-rank for final top-N selection. Two-phase design is mathematically coherent. All poincare.rs imports consistent — crate is purely Poincare ball throughout. Crate now internally consistent across all DEEP files | tangent.rs (ruvector-hyperbolic-hnsw) | R99 |
| **gnn-node/lib.rs real napi-rs FFI with solid error handling** — 88-92%, proper #[napi], #[napi(constructor)], #[napi(factory)], #[napi(object)] macros throughout. Status::InvalidArg for range validation (e.g. k parameter bounds), .map_err() with Status::GenericFailure for Rust errors — no unwrap()/panic!(). 5-tier compression enum with correct defaults. Imports 6 real symbols from ruvector-gnn core crate | crates/ruvector-gnn-node/src/lib.rs | R99 |
| **gnn-wasm/lib.rs 18th GENUINE WASM — real delegation to ruvector_gnn core** — 90-94%, all exported functions delegate to genuine ruvector_gnn algorithms via serde_wasm_bindgen deserialization. Inline cosineSimilarity() uses correct dot product / (‖a‖·‖b‖) with epsilon guard (95% correct). console_error_panic_hook::init() properly initialized on first use. Zero stubs — every export exercises real core crate code | crates/ruvector-gnn-wasm/src/lib.rs | R99 |
| **poincare.rs (ruvector-attention) 93-96% — most rigorous Gyrovector implementation in the attention crate** — all operations correct: poincare_distance with acosh denominator guard, Mobius addition closed-form, Mobius scalar multiplication with repeated exp/log maps, exp_map and log_map with conformal factor lambda_base. EPS clamping prevents NaN on boundary, acosh argument clamped to ≥1.0, projection after every exp_map update. Best-quality shared math foundation in the ruvector-attention hyperbolic cluster | poincare.rs (ruvector-attention hyperbolic) | R100 |
| **error.rs (ruvector-mincut-gated-transformer) hard no-panic contract with recovery classification** — ~92%, 5-variant Error enum covering all failure modes: BadConfig, BadWeights, BadInput, OutputTooSmall, UnsupportedMode. is_recoverable() allows callers to implement retry/fallback logic; is_config_error() distinguishes configuration bugs from runtime failures. 3 tests, no_std compatible. Clean thiserror derive with no unnecessary wrapping of std::io::Error | error.rs (ruvector-mincut-gated-transformer) | R100 |
| **error.rs (ruvector-hyperbolic-hnsw) clean thiserror enum with programmatic recovery fields** — 95%+, 8-variant HyperbolicError covers all geometry failure modes. OutsideBall variant carries both norm and curvature fields enabling callers to compute projection distance or select an alternative curvature without re-querying. Clone-derived for retry/backoff patterns. HyperbolicResult type alias exported for ergonomic use throughout the crate | error.rs (ruvector-hyperbolic-hnsw) | R100 |
| **mixed_curvature.rs MOST ARCHITECTURALLY NOVEL attention in ruvector-attention** — 90-94%, genuinely novel product-space attention (Euclidean × Hyperbolic): splits embedding dimension in half, runs independent softmax-scaled dot-product attention in each geometry, blends output with learned alpha. The design empirically approximates mixed-curvature product manifolds (Gu et al. 2019) without requiring Riemannian optimization of alpha. Renormalization after blend rescues numerical correctness. No equivalent exists elsewhere in the crate | mixed_curvature.rs (ruvector-attention) | R100 |
| **Postgres hyperbolic SQL API complete and production-grade** — 8 immutable+parallel_safe pg_extern functions cover full hyperbolic geometry surface (distance, Mobius addition, exp/log map, coordinate conversion in both directions, Minkowski inner product). 11 pg_tests verify mathematical properties including symmetry, identity, exp/log roundtrip, and coordinate conversion correctness | hyperbolic/operators.rs (ruvector-postgres) | R101 |
| **Healing module has comprehensive SQL boundary** — 17 pg_extern functions across 5 functional groups (health status, history, triggers, configuration, strategy/threshold introspection) with consistent JsonB returns, genuine dry_run path, and echo-back pattern for config changes. Clean separation between SQL boundary (functions.rs) and orchestration (mod.rs) | healing/functions.rs, healing/mod.rs | R101 |
| **message_passing.rs MessagePassing trait is clean open protocol** — enables custom aggregation strategies beyond SUM. Pure Rust graph algorithm with rayon parallelism — fully portable beyond Postgres context. propagate_weighted() provides per-edge f32 weights with safe scalar fallback for zero-weight edges | gnn/message_passing.rs (ruvector-postgres) | R101 |
| **bench_utils.rs genuine no_std/embedded design** — CPU-native cycle counters with AArch64 CNTVCT_EL0 path (PMCCNTR_EL0 system register) alongside x86_64 RDTSC. Correct for WASM/bare-metal contexts where POSIX time APIs are unavailable. BenchStats accumulates min/max/sum/count for distribution analysis | kernel/bench_utils.rs (ruvector-mincut-gated-transformer) | R101 |

## 5. Subsystem Sections

### 5a. HNSW Implementations

Three distinct HNSW implementations exist, each serving different use cases (confirmed Phases B+C):

**ruvector-core (Primary)** wraps the third-party `hnsw_rs` crate, NOT a from-scratch implementation. Adds real value: SIMD intrinsics (AVX-512/AVX2/NEON with runtime CPU detection, 1,605 LOC), REDB persistent storage, lock-free concurrency (parking_lot, DashMap, crossbeam). **CRITICAL issues**: placeholder embeddings (sums character bytes, not semantic), HNSW deletions broken (hnsw_rs limitation), ID translation overhead (u64↔string). **PQ NOTE (H1 RESOLVED by R90)**: simd_intrinsics.rs had partial PQ; product_quantization.rs (advanced_features/) has complete k-means++ codebook training + ADC. PQ capability exists; it is in the advanced_features module, not simd_intrinsics.

**micro-hnsw-wasm** is genuinely novel, from-scratch HNSW for ultra-constrained WASM: `#![no_std]`, fixed capacity (32 vectors/core, 16 dims max, 6 neighbors/node), static memory (all in `static mut` arrays, no heap), 256-core sharding (8K total vectors), Quake III fast inverse sqrt, SNN integration (LIF neurons, STDP learning), target <12KB binary. R36 deep-read revealed 6 novel neuromorphic features (spike encoding, homeostatic plasticity, 40Hz resonance, WTA, dendritic computation, temporal patterns) ALL UNVALIDATED with ZERO tests (CRITICAL).

**hyperbolic-hnsw (88-93% confirmed R92)** adapts HNSW for Poincaré ball geometry. **Native implementation** — does NOT wrap hnsw_rs (unlike ruvector-core). All data structures from scratch: HnswNode with Vec<Vec<usize>> connection layers, BinaryHeap<Reverse<SearchResult>> candidate queues. Uses `poincare_distance_from_norms()` in every graph traversal. **Key innovation: tangent-space two-phase search** — TangentCache precomputes log_map() coordinates at Fréchet mean centroid, filters `prune_factor × k` candidates cheaply via Euclidean distance in tangent space, then scores only those with exact Poincaré distance (reduces arcosh calls from O(N) to O(k)). DualSpaceIndex maintains both Poincaré + Euclidean indices and fuses via Reciprocal Rank Fusion. Curvature is a runtime parameter with reprojects-on-change. Level multiplier 1/ln(M) per Malkov & Yashunin 2018. Mathematical foundation in poincare.rs (90-95%) verified against Ganea et al. 2018. WASM wrapper (88-92%) = 17th GENUINE WASM.

**HNSW "patches" (R36, CORRECTED R52)**: R52 line-by-line DEEP read reveals these are **NOT patches** — `scripts/patches/hnsw_rs/` contains a **vendored copy** of upstream hnsw_rs v0.3.3 with ZERO ruvector-specific modifications. Directory naming is misleading. hnsw.rs (98-100%) is complete Malkov & Yashunin with Rayon parallel insert, FilterT search, LayerGenerator exponential sampling — all upstream features. hnswio.rs (95-98%) is BEST-IN-CLASS HNSW persistence: dual-file format (graph+data), hybrid mmap (upper layers in memory, lower mmapped), 4 backward-compatible format versions, zero-copy raw serialization, concurrent safety via unique basename generation. **No postgres or AgentDB integration** — file-based persistence only. **CRITICAL**: libext.rs (75-85%) Julia FFI has no bounds checking on C pointers, std::mem::forget leaks. datamap.rs (85-90%) use-after-free risk with mmap lifetimes. All distance calculations delegated to external `anndists` crate — SIMD is NOT in hnsw.rs itself.

### 5b. Hash-Based Embeddings (Systemic Weakness)

The most pervasive architectural weakness across the entire ruvnet ecosystem. Confirmed in 7+ files across 5 packages in both Rust and JavaScript (C, R13, R22, R37):

| File | Package | Mechanism |
|------|---------|-----------|
| agenticdb.rs | ruvector-core | Sums character bytes of input text (R13, C) |
| hooks.rs | ruvector-cli | Position-based hash (R22) |
| pretrain_pipeline.rs | ruvllm/claude_flow | character sum % dim (R37) |
| real_trainer.rs | ruvllm/training | text_to_embedding_batch deterministic hash (R37) |
| rlm_embedder.rs | ruvllm/bitnet | FNV-1a hash (R35) |
| learning-service.mjs | claude-flow | Math.sin(seed) mock (ecosystem) |
| enhanced-embeddings.ts | agentdb | Math.sin(seed) fallback (ecosystem) |

In practice, all "semantic search" using defaults is character-frequency matching. HNSW indices are structurally valid but search results are meaningless without plugging in a real embedding provider.

### 5c. Attention Mechanisms (Corrected R37)

**Initial assessment was WRONG.** Phase B deep read of Rust source (66 files, ~9,200 LOC across 19 modules) reveals **18+ real implementations** with algorithmic substance. Earlier analysis examined npm-packaged `.js` files only — actual implementations live in `crates/ruvector-attention/src/`.

**Real implementations**: Scaled Dot-Product (standard softmax), Multi-Head, FlashAttention (tiled + online softmax), LinearAttention (FAVOR+), LocalGlobal (Longformer-style), HyperbolicAttention (Poincare + Frechet mean), LorentzCascade (novel), MixedCurvature (E^e × H^h × S^s), EdgeFeatured (GATv2), DualSpace (Euclidean+Hyperbolic), GraphRoPE, MoEAttention (Top-K routing), SlicedWasserstein, CentroidOT, TopologyGated, SheafAttention, DiffusionAttention, NaturalGradient.

**Concerns**: SIMD feature flag is no-op (zero `#[target_feature]` usage in 66 files), Rayon parallelism unused (zero `par_iter()` — multi-head processes heads serially), zero unsafe code (positive for safety but means no hand-tuned SIMD), novel algorithms unvalidated (LorentzCascade, SheafAttention, TopologyGated — no benchmarks vs baselines).

**NAPI bindings** (5 files, ~2,548 LOC): 24 classes, 7 async functions, 9 utilities, 3 enums. Zero unsafe blocks, all errors via `Error::from_reason()`. Async uses `tokio::task::spawn_blocking()` with proper thread safety. Rust native is 10-40x faster than JS fallbacks.

**SQL Attention (ruvector-postgres, R97)**: Three files (967 LOC) implement multi-head attention as a Postgres extension building block. attention/mod.rs (82-87%) is the orchestration hub — exposes AttentionType via real pgrx PostgresEnum (genuine pgrx integration, maps enum to PostgreSQL type directly), declares Attention trait (Send+Sync) with attention_scores, apply_attention (numerically-stable scalar default), and softmax/softmax_inplace (both with max-subtraction trick). **Inflation**: module doc claims "39 mechanisms" but AttentionType defines only 10 variants (H66). mod.rs has NO pg_module_magic! or #[pg_extern] — it is a pure Rust library layer, not the Postgres entry point. attention/multi_head.rs (88-92%) implements genuine parallel MHA via Rayon into_par_iter() with Vec<ScaledDotAttention> (one per head). Score aggregation averages across heads. **No W_Q/W_K/W_V projections** — Q/K/V split is mechanical slice partitioning, making this retrieval-focused (not learned attention). No SQL generation or pgrx macros. attention/scaled_dot.rs (90-93%) has correct QK^T/sqrt(d_k) scale stored at construction. Real simsimd SIMD via f32::dot() returning Option<f64> with scalar fallback. Numerically stable softmax. Dead dropout field (#[allow(dead_code)]). 9 unit tests + 2 pg_tests. Comprehensive test coverage.

### 5d. Postgres Extension

Substantial PostgreSQL extension rivaling pgvector in feature scope. 290+ SQL functions claimed, 54 verified in operators.rs (R22), 20 module directories. Three vector types (ruvector, halfvec, sparsevec).

**SIMD (95-98%, BEST IN ECOSYSTEM, R22)**: distance/simd.rs has real AVX-512 (16 floats/iter), AVX2 (8 floats/iter with 4x unrolling = 32/iter), ARM NEON (4 floats/iter), simsimd 5.9 integration, runtime feature detection, zero-copy PostgreSQL pointer APIs, 23 test functions. Dimension-specialized dispatch (384/768/1536/3072).

**Index implementations**: hnsw_am.rs (75-80%) has real beam search + greedy descent, insertion logic. **CRITICAL**: `connect_node_to_neighbors()` COMPLETELY EMPTY — graph never actually linked. ivfflat_am.rs (80-85%) has real k-means++ initialization (D² weighting, ChaCha8Rng), Lloyd clustering, adaptive probes. **CRITICAL**: insert, delete, retrain all STUBS.

**SPARQL system (93% weighted, R34+R52)**: COMPLETE SPARQL 1.1 with both parser AND executor. parser.rs (93-95%, 2,496 LOC, R52) is a production W3C SPARQL 1.1 recursive-descent parser: all 4 query forms (SELECT/CONSTRUCT/ASK/DESCRIBE), full UPDATE operations (INSERT/DELETE/LOAD/CLEAR/CREATE/DROP), property paths (sequence/alternative/inverse/transitive), graph patterns (OPTIONAL/UNION/MINUS/GRAPH/FILTER/BIND/VALUES/SERVICE/subqueries), aggregates (COUNT/SUM/AVG/MIN/MAX/GROUP_CONCAT/SAMPLE), 33+ built-in functions (string/numeric/datetime/hash/UUID), proper AST representation (907 LOC ast.rs). Total SPARQL module: 7,421 LOC across 7 files. executor.rs (92%, 1,884 LOC, R34) full algebra execution, property paths (BFS), all 7 aggregates. DELETE is no-op. In-memory TripleStore. **Two SPARQL implementations exist**: ruvector-postgres (this) and rvlite (embedded).

**Benchmark system (42%, R52)**: index_bench.rs is **theatrical benchmarking** — uses production-quality criterion framework (proper warmup, statistical analysis, recall@10, p50/p95/p99 latency percentiles) but measures **wrong implementations**. HNSW search_with_ef() is brute-force O(n) linear scan, NOT real HNSW graph traversal. IVFFlat K-means is genuine. Reimplements HNSW (280 LOC) and IVFFlat (200 LOC) internally instead of importing ruvector-core. Located in ruvector-postgres/benches/ but contains ZERO postgres code. Different category of facade than R43's rustc_benchmarks (15%): not asymptotic deception, but algorithmic mislabeling.

**Healing subsystem (now 7/7 DEEP, weighted ~81%, R36+R101)**: Real learning but stub execution. learning.rs (92-95%) BEST — genuine adaptive weight formula, confidence scoring, human feedback. detector.rs (85-90%) 8 problem types but all 8 metric collection methods return empty (CRITICAL). engine.rs (75-80%) cooldown/rate-limiting real but timeout enforcement missing (CRITICAL). strategies.rs (60-65%) StrategyRegistry 95% real but ALL 5 execution methods (reindex, promote, evict, block, repair) are log-only stubs (CRITICAL). worker.rs (70-75%) health check loop works but bgworker registration COMMENTED OUT. **R101 completions**: functions.rs (88-92%) — 17 pg_extern SQL functions covering all 5 healing operation groups with genuine dry_run path; read/write lock inconsistency in set_thresholds (H89). mod.rs (90-93%) — OnceLock global singleton HEALING_ENGINE (H87), 4-stage pipeline wired (detect→diagnose→repair→learn), OutcomeTracker diverging-state risk (H88).

**Verdict**: EXCELLENT read-path foundations but incomplete write paths. SIMD production-ready. Index builds functional. Index searches real. Index mutations 40-60% incomplete. Healing system can learn which strategies work but cannot execute any of them or detect problems. SQL boundary (functions.rs) is complete and genuine; execution layer (strategies.rs) remains log-only stubs.

### 5d-ii. SQL Attention Extension (R96+R97)

**Five files, ~1,791 LOC, ruvector-postgres attention module.** These extend the postgres extension with attention mechanism operators.

**operators.rs (88-92%, R96):** GENUINE pgx extension layer — the ACTUAL Rust implementation that CORRECTS R91's skepticism about AttentionService.ts. Five public functions all carry `#[pg_extern(immutable, parallel_safe)]` making them real PostgreSQL callable functions. Dispatches to ScaledDotAttention, FlashAttention, MultiHeadAttention via `Box<dyn Attention>` trait. **CRITICAL GAP**: only 3/10 declared attention types are SQL-dispatchable; Linear, GAT, Sparse, MoE, Cross, Sliding, Poincare all fall through to ScaledDot default (H60f). Matrix inputs pass through PostgreSQL JsonB serialization boundary (correct but slow). 6 genuine pg_test tests running inside live PostgreSQL instance.

**flash.rs (72-78%, R96):** Flash Attention algorithm in pgrx crate but NOT a PostgreSQL function — zero `#[pg_extern]` in main code path. Operates on in-memory `&[f32]` tensors only. Only KV-dimension tiling; query tiling abandoned (`block_size_q` is `#[allow(dead_code)]`). Online softmax is CORRECTLY implemented (core Flash Attention insight). **FALSE complexity claim**: module docstring claims O(sqrt(N)) space but `block_outputs` Vec collects all outputs before combining — actual peak is O(N) (H60h). Tests gated by `pg_test` feature — not run by default `cargo test`. Role: likely algorithmic ground truth for GPU version in ruvector-mincut-gated-transformer.

**multi_head.rs (88-92%, R97):** Genuine Rayon parallel MHA — delegates per-head computation to ScaledDotAttention via `par_iter()`. No W_Q/W_K/W_V projections (retrieval not generation). Returns (seq_len, d_model)-shaped output by concatenating heads then projecting back — standard MHA architecture.

**scaled_dot.rs (90-93%, R97):** Correct QK^T/sqrt(d_k) attention with numerically stable softmax. Real simsimd SIMD with scalar fallback. Dead dropout field (stored but never applied). Best-quality file in the attention module.

**mod.rs (82-87%, R97):** Orchestration hub defining AttentionType PostgresEnum (real pgrx integration). "39 mechanisms" inflation claim — AttentionType enum defines only 10 variants (H66).

**SQL Attention verdict**: operators.rs is the real bridge between SQL queries and Rust attention algorithms (resolves R91 AttentionService.ts concern). flash.rs is an in-memory reference implementation not yet wired to SQL. The module is functional for 3 common attention types from SQL; 7 declared types are accessible only from Rust.

**SQL Hyperbolic Functions (R98):** Two additional files in ruvector-postgres/hyperbolic/ complete the hyperbolic geometry layer for SQL consumption.

**poincare.rs (88-92%, 268 LOC, R98):** Pure Rust Poincare ball operations: poincare_distance() uses acosh formula with denominator guards, mobius_addition() correct M_c formula, exp_map/log_map with conformal factor lambda_base = 2/(1-‖x‖²·c), project_to_ball() clamps to MAX_NORM=0.99999. Correct curvature scaling throughout. EPSILON=1e-8 prevents NaN on boundary points. simsimd imported but unused (minor dead import). 13 unit tests validate symmetry, inverse consistency, curvature effects. SQL exposure is INDIRECT — function implementations are pure Rust, accessed via operators.rs `#[pg_extern]` re-exports. CONFIRMS R97 "Poincare-only" verdict: this file IS the Poincare implementation; lorentz.rs is separate.

**lorentz.rs (87-92%, 258 LOC, R98):** Correct Lorentz (hyperboloid) model: inner product ⟨x,y⟩_L = -x₀y₀ + Σxᵢyᵢ (correct indefinite signature), hyperboloid constraint (x₀² - ‖x̃‖² = 1/|c|), acosh Lorentz distance. Bidirectional coordinate transforms: poincare_to_lorentz() uses x₀=(1+|c|‖p‖²)/(1-|c|‖p‖²), xᵢ=2pᵢ/(1-|c|‖p‖²) (correct formula). lorentz_to_poincare() inverse. simsimd::SpatialSimilarity for SIMD dot product. 13 tests including cross-model distance equivalence. **CRITICAL H70**: distance() accepts off-hyperboloid points silently — is_on_hyperboloid() defined but never called internally. **H71**: zero #[pg_extern] annotations — no direct SQL exposure, only via operators.rs.

**SQL Hyperbolic verdict**: Both poincare.rs and lorentz.rs implement genuine hyperbolic geometry primitives. The math is correct (with H70 caveat for Lorentz manifold validation). SQL exposure is via operators.rs acting as the single pgx entry point — a clean separation of math from SQL binding. The ruvector-postgres hyperbolic module is now fully characterized (5 files DEEP: operators.rs, flash.rs, multi_head.rs, scaled_dot.rs, mod.rs from SQL attention + poincare.rs + lorentz.rs from hyperbolic geometry).

**R101 — postgres hyperbolic/ ARC COMPLETE (4/4 DEEP):** operators.rs (92-95%, 395 LOC) adds 8 pg_extern SQL functions exposing the full hyperbolic geometry surface: ruvector_poincare_distance, ruvector_lorentz_distance, ruvector_mobius_add, ruvector_exp_map, ruvector_log_map, ruvector_poincare_to_lorentz, ruvector_lorentz_to_poincare, ruvector_minkowski_dot. All carry #[pg_extern(immutable, parallel_safe)] — correct for pure math. 11 pg_test functions verify mathematical properties (symmetry, identity, exp/log roundtrip, coordinate conversion roundtrip). **SYSTEMIC GAP (H85)**: operators.rs passes points directly to PoincareBall.distance() and Lorentz model functions without any |x|<1 or hyperboloid constraint check. Combined with R98's H70 (lorentz.rs is_on_hyperboloid() defined but never called), the complete postgres hyperbolic SQL API has ZERO point-on-manifold enforcement across all 4 files. The hyperbolic module is mathematically rigorous in internal operations but silently accepts geometrically invalid SQL inputs.

### 5d-iii. Postgres GNN Subsystem (R101)

**Verdict: SELF-CONTAINED REIMPLEMENTATION, NOT ruvector-gnn COMPOSITION (82-92% range, 3 files DEEP).** The ruvector-postgres/gnn/ module is a fully independent GNN implementation that does NOT import from the ruvector-gnn crate (11+ DEEP files from R91/R94/R99). This creates TWO parallel GNN ecosystems:

**Ecosystem 1 — ruvector-gnn (native Rust crate):** 11+ DEEP files including custom hybrid GAT+GRU+edge-weighted GNN, full EWC, genuine LR scheduling, reservoir replay. Exposed to users via napi-rs (gnn-node, R99) and WASM (gnn-wasm, 18th GENUINE WASM, R99). Inference-only FFI surface.

**Ecosystem 2 — ruvector-postgres/gnn/ (SQL-side reimplementation, R101):** Three files implement GNN directly within the postgres extension crate. Imports exclusively from local siblings (super::gcn, super::graphsage, super::aggregators) — no cross-crate dependency on ruvector-gnn. Zero code reuse despite implementing the same algorithms.

**gnn/operators.rs (82-87%, 426 LOC, R101):** 4 genuine SQL operators: ruvector_gcn_forward, ruvector_gnn_aggregate, ruvector_graphsage_forward, ruvector_gnn_batch_forward. 1 FACADE: ruvector_message_pass returns "Multi-hop gcn message passing over N hops from table X..." — a human-readable string with zero SQL execution (H90). Deterministic weight initialization: val = ((i * out + j) * 0.01) % 1.0 in loop — "testing" pattern with no trained weight loading (H94). 8 pg_test cases. SQL exposure confirmed via #[pg_extern].

**gnn/gcn.rs (82-88%, 224 LOC, R101):** Fully independent GCN (Kipf & Welling 2016). Genuine math: Xavier init, 1/sqrt(degree) message normalization, message-aggregate-update pipeline, ReLU activation. Rayon par_iter for activation and message passing. **Algorithmic gap (H93)**: uses adjacency matrix A, not A+I — update() ignores _node_features parameter, making this pure neighbor aggregation rather than canonical GCN which requires self-loops. **Integration gap (H92)**: entirely in-memory (Vec<Vec<f32>>, edge lists as &[(usize, usize)]) — no SQL data types, no postgres row access, no pgx macros. 6 unit tests with hand-computed expected values.

**gnn/message_passing.rs (88-92%, 234 LOC, R101):** Core algorithmic foundation. MessagePassing trait defines 3-phase protocol: message() (source→target feature transform), aggregate() (sum collection), update() (final node state). build_adjacency_list() constructs inbound adjacency (target→[sources]). propagate() uses rayon par_iter for parallel message computation per node. propagate_weighted() accepts per-edge f32 weights with safe fallback (weight 0.0 → skip). Zero Postgres-specific code — pure portable Rust. Only SUM aggregation provided. 3 tests covering basic propagation and weighted edges.

**Parallel GNN ecosystems verdict**: Two independent implementations of the same algorithms with zero code sharing. The separation between ruvector-gnn (Rust crate, FFI-exposed) and ruvector-postgres/gnn (SQL-integrated) may be intentional (separate concerns: training+inference vs SQL operations), but the absence of any bridge (ruvector-gnn crate not imported even for shared types) means bugs and improvements must be fixed twice. The SQL bridge between in-memory gcn.rs algorithms and actual postgres tables is presumably in workers/gnn.rs (not yet read).

### 5e. ruvllm LLM Inference

Complete BitNet 1-bit LLM inference backend optimized for Apple Silicon M4 Pro. Three-tier architecture: Metal GPU → Apple Accelerate → NEON SIMD → Scalar fallback. Three deep-read sessions (R22, R34, R35, R37) covering 39 files, ~58K LOC, weighted avg 86% real.

**bitnet/backend.rs (4,559 LOC, 92-95%, R22)**: TL1 ternary lookup tables (2-bit decode), GQA attention (4-wide unrolling), MLA Multi-Head Latent (17.8x memory reduction — genuine innovation, stores latents only), expert predictor (Laplace-smoothed transitions), GGUF model loading, ScratchPool (zero-allocation), AVX2 SIMD dispatch.

**kernels (90.3% weighted, R22+R35)**: attention.rs (88-92%) Flash Attention 2 matching Tri Dao paper, NEON dot product (8x unroll, dual accumulators), PagedKvCache (zero-alloc), GQA parallel (rayon), paged attention, softmax NEON (60% vectorized — exp falls back to scalar). matmul.rs (85-90%) 12x4 GEMM micro-kernel (production BLAS-level), Accelerate integration, Metal GPU offload, 8-accumulator dot for ILP, FP16 path uses scalar `half` crate NOT NEON FP16 (40% real). norm.rs (95%) BEST quality — 4x unrolled FMA, correct variance. rope.rs (95%) real RoPE, NEON interleaved ops, NTK-aware scaling. quantized.rs (92%) real NEON int8/int4/q4k kernels, llama.cpp-compatible. activations.rs (92%) vectorized exp/sigmoid/tanh with polynomial approx. ane_ops.rs (70%) MISLEADING — gelu_ane/silu_ane are SCALAR FALLBACKS, not real ANE ops.

**Infrastructure (92% weighted, R34)**: memory_pool.rs (95%) BEST systems code — lock-free bump allocator (atomic CAS), RAII buffer pool (5 size classes), per-thread scratch with WASM variant, 12 tests. autodetect.rs (92%) real hardware detection (platform, CPU features NEON/AVX, Metal probe), 27 tests, CUDA/WebGPU stub. kv_cache.rs (90%) two-tier KV cache (hot FP16 + cold quantized), real NEON SIMD quantize/dequantize, f32 storage gap (simulated compression), potential deadlock in lock ordering.

**Backends (82% weighted, R35)**: Architecture-complete, persistence-incomplete (SYSTEMIC). All backends have correct math but incomplete weight loading. CoreML (88-92%) real objc2-core-ml bindings, ANE detection, expects pre-converted .mlmodel. Candle (80-85%) ONLY FUNCTIONAL BACKEND — real GGUF + safetensors loading. Mistral (70-75%) real via mistral-rs, X-LoRA manager 90% (learned MLP routing). Gemma2 (88-92%) real soft-capping, alternating local/global attention, from_gguf stub. Phi3 (85-90%) real SuRoPE (128K context), sliding window, from_gguf stub. HybridPipeline (70-75%) generate/stream ALL return NotImplemented.

**Serving (86% weighted, R35)**: scheduler.rs (90-92%) BEST scheduler in ecosystem — vLLM-style continuous batching, preemption (recompute+swap), chunked prefill, priority queues. engine.rs (80-85%) real continuous batching + speculative integration, fallback hash%32000 when no model. paged_attention.rs (75-80%) real page table + block allocator, kernel simplified.

**BitNet extensions (75% weighted, R35)**: expert_cache.rs (88-92%) real LRU/LFU/Adaptive eviction, batch scheduling. tl1_kernel.rs (80-85%) real NEON GEMV (i8→i16→i32 widening), LUT generation wrong but never called. rlm_embedder.rs (75-80%) real recursive refinement, NO BitNet integration, HashEmbedder FAKE (FNV-1a). speculative.rs (55-60%) **CRITICAL perf bug**: 2K sequential forward passes for K tokens vs K vanilla passes — SLOWER than vanilla.

**Training + LoRA (83% weighted, R37)**: micro_lora.rs (92-95%) BEST learning code — NEON SIMD 8x unrolling, EWC++ Fisher penalty, <1ms forward, 18 tests. grpo.rs (90-92%) textbook GRPO (GAE, PPO clipping, adaptive KL), 16 tests. real_trainer.rs (70-75%) triplet loss + InfoNCE, hash-based embeddings (CRITICAL). tool_dataset.rs (88-92%) 140+ templates, 19 categories. claude_dataset.rs (75-80%) 60+ templates, weak augmentation.

**Claude Flow bridge (87% weighted, R37)**: reasoning_bank.rs (92-95%) FOURTH ReasoningBank — real K-means (10 iterations), EWC++ consolidation, 16 tests. hnsw_router.rs (90-93%) BEST ruvector-core integration — HybridRouter blends HNSW semantic + keyword. model_router.rs (88-92%) 7-factor complexity, feedback tracking 1000 predictions. pretrain_pipeline.rs (85-88%) multi-phase pretraining, hash-based embeddings (CRITICAL). claude_integration.rs (70-75%) execute_workflow SIMULATION — hardcoded 500 tokens, no real API (CRITICAL).

**ruvllm coverage after R37**: Total .rs files 338, DEEP 39, LOC read ~58K, weighted avg 86%, files remaining 309 (197K LOC).

### 5f. Temporal Tensor (Production-Ready)

**HIGHEST QUALITY CRATE** — 93% weighted avg, 213 tests total, production-ready. All files ≥88%. Deep-read across R22 and R37.

store.rs (~2,500 LOC, 92-95%, R22) BEST FILE — 74.7KB. Real 4-tier quantization (3-8 bit), CRC32 integrity, SVD frame reconstruction. store_ffi.rs (889 LOC, 90-92%, R37) 11 extern "C" FFI functions for WASM/C, real quantization via crate::quantizer. agentdb.rs (843 LOC, 88-92%, R37) pattern-aware tiering with 4-dim PatternVector, cosine similarity, weighted neighbor voting, 36 tests. quantizer.rs (1,430 LOC, 93-95%, R37) K-means PQ with configurable subvectors, asymmetric distance computation. compressor.rs (1,568 LOC, 95-98%, R37) Delta + run-length + Huffman pipeline, CRC32 integrity. tiering.rs (1,613 LOC, 93-95%, R37) 4-tier storage (Hot→Warm→Cold→Archive) with LRU tracking, promotion/demotion with hysteresis.

### 5g. ruQu + ruqu-core Quantum Computing

**GENUINE QEC + COMPLETE QC PIPELINE** — not a facade. Now 15 files across ruQu + ruqu-core, ~18,500 LOC. Revised weighted avg ~89% (subpoly_decoder drags from 91.3%). Deep-read R37, R39, R54.

**R54 CRITICAL DISCOVERY**: ruQu contains TWO unrelated systems under one crate:
- **QEC system**: decoder.rs, syndrome.rs, surface_code.rs, noise_model.rs, qec_scheduler.rs — genuine quantum error correction
- **Coherence gate system**: filters.rs, fabric.rs, tile.rs, planner.rs — classical statistical decision pipeline for gate quality

These systems have ZERO cross-references. "Qu" may mean "Quality" not "Quantum" for the coherence gate subsystem.

**ruQu QEC (unchanged from R37/R39)**: decoder.rs (2,400 LOC, 95-98%) BEST FILE — Union-Find O(α(n)) + MWPM. syndrome.rs (1,640 LOC, 90-92%) real AVX2 SIMD. surface_code.rs (1,820 LOC, 88-92%) complete surface code. qec_scheduler.rs (1,505 LOC, 88-92%) critical path learning, remote providers stub. noise_model.rs (1,330 LOC, 82-85%) 7 noise channels.

**ruQu Coherence Gate (R39+R54)**: tile.rs (2,125 LOC, 92%) coherence gate architecture, Union-Find, Ed25519, 27 tests. planner.rs (1,478 LOC, 88%) 4 backend cost models, 33 tests. filters.rs (1,357 LOC, 82-86%, R54) MISNAMED — three-filter coherence pipeline (structural min-cut + shift drift + evidence e-value), production statistical methods, 14 tests, ZERO quantum filtering. fabric.rs (1,280 LOC, 93-96%, R54) production 256-tile WASM orchestrator, Blake3 audit trails, surface code topology generator, 23 tests.

**ruqu-core Foundation (R54)**: mitigation.rs (1,276 LOC, 95-98%) THREE NISQ-era strategies — ZNE (Richardson exact extrapolation, polynomial least-squares), Measurement Error (tensor-product calibration, O(n·2^n) scalable inversion), CDR (Clifford data regression). 40+ tests at 1e-12 precision. transpiler.rs (1,211 LOC, 95-98%) BEST-IN-CLASS — complete 3-phase transpiler (decompose/route/optimize), 3 real hardware backends (IBM Eagle, IonQ Aria, Rigetti Aspen), BFS qubit routing with SWAP insertion, 2-level optimization (inverse cancellation + Rz merging), 44 tests. noise.rs (1,175 LOC, 96-98%) BEST-IN-CLASS — production Kraus operator formalism, 4 channels (depolarizing, amplitude damping, phase damping, thermal relaxation), hardware calibration pipeline (T1/T2→γ/λ derivation), confusion matrix inversion for readout, 498 test lines. Comparable to Qiskit Aer. subpoly_decoder.rs (1,208 LOC, 35-40%) **FALSE SUBPOLYNOMIAL** — 3rd instance of false complexity pattern (R39, R52, R54). Claims "provable O(d^{2-ε} polylog d)" but ALL 3 decoders (Hierarchical, Renormalization, SlidingWindow) use O(n²) greedy_pair_and_correct. Zero citations. Implementation is CORRECT but conventional — use decoder.rs's Union-Find instead.

**Combined ruQu+ruqu-core pipeline**: noise.rs (noise models) → mitigation.rs (error mitigation) → transpiler.rs (circuit compilation) → surface_code.rs (QEC layout) → decoder.rs (error correction). This is a **near-complete quantum computing stack** from noise characterization to error-corrected execution.

### 5h. Prime-Radiant & Cognitum-Gate

**prime-radiant (89% weighted, R37)**: Sheaf-theoretic knowledge substrate for AI memory governance. restriction.rs (1,489 LOC, 90-92%) BEST sparse matrix in ecosystem — complete CSR with 6 formats, 4x SIMD unrolling, zero-alloc hot paths. memory_layer.rs (1,260 LOC, 92-95%) triple memory (Agentic/Working/Episodic) with real cosine similarity, genuine temporal/semantic/hierarchical edge creation, 19 tests. witness_log.rs (1,130 LOC, 88-92%) blake3 hash chains with tamper evidence, chain verification (genesis, content hashes, linkage), 16 tests. coherence.rs (1,500 LOC, 88-90%) global/local coherence via sheaf Laplacian, real spectral gap computation. knowledge_graph.rs (1,190 LOC, 85-88%) DashMap concurrent graph, blake3 hashing, topological sort. **Issue**: SIMD not enabled by default — wide::f32x8 cfg-gated behind `simd` feature (HIGH).

**cognitum-gate-kernel (93% weighted, 5 files, 3,504 LOC, R36)**: EXCEPTIONAL CODE — rivals neural-network-implementation as best in ecosystem. 256-tile distributed coherence verification via anytime-valid sequential testing (e-values). lib.rs (713 LOC, 95%) custom bump allocator for no_std WASM, complete tick loop, 6 WASM FFI exports. report.rs (491 LOC, 98%) TileReport exactly 64 bytes with cache-line alignment, compile-time size assertions, correct aggregation. delta.rs (465 LOC, 98%) tagged union 7 operation types, fixed-size FFI-safe layout. shard.rs (983 LOC, 92%) optimal union-find with iterative path compression and union by rank, cache-line alignment for hot fields. evidence.rs (852 LOC, 88%) fixed-point log-space arithmetic with pre-computed thresholds (eliminates libm), genuine sequential testing via e-process.

### 5h2. Prime-Radiant Hyperbolic Geometry (R97+R98)

**Verdict: GENUINE MATH, STUB SEARCH, MODULE COMPLETE (75-92% range across 5 files, ~81% weighted avg).** Five files (~1,433 LOC) implement hyperbolic geometry within prime-radiant. Module is now FULLY CHARACTERIZED. Distinct from the dedicated ruvector-hyperbolic-hnsw crate (R92, 88-95%) which has a genuine HNSW back-end.

**hyperbolic/mod.rs (88-92%, 363 LOC):** Orchestration layer. Poincare distance formula correct: d(x,y) = acosh(1 + 2‖x-y‖² / ((1-‖x‖²)(1-‖y‖²))) / sqrt(-c). Log map with correct conformal factor lambda_base = 2/(1-‖base‖²). Frechet mean via iterative Riemannian GD with exp/log maps. HierarchyLevel enum (Root/High/Mid/Deep/VeryDeep) with hardcoded depth thresholds. 17 tests. **Critical gap**: similarity_search() uses O(n) linear scan over all vectors — "In production would use HNSW." Zero imports from prime-radiant sheaf substrate (restriction.rs, coherence.rs, knowledge_graph.rs). The hyperbolic module is an isolated add-on.

**hyperbolic/energy.rs (82-87%, 352 LOC):** Pure data container — WeightedResidual, HyperbolicEnergy, DepthBucketEnergy. All Riemannian math (poincare_distance, log_map, exp_map) delegated to adapter.rs. Correct depth formula: 2*arctanh(‖x‖)/sqrt(-c). Weighted energy formula: base_weight × residual_norm_sq × depth_weight. **Bug**: HyperbolicEnergy::merge() (lines 179-187) merges two energy objects without checking curvature compatibility — energies computed under different curvatures are directly summed (mathematically meaningless, H63). curvature field stored but never used in aggregation methods (avg_energy, energy_by_depth_buckets). 3 tests cover basic coherence, weighted energy, hierarchy levels.

**hyperbolic/adapter.rs (78-83%, 333 LOC):** Implements Poincare-ball operations but named "adapter" implying model interoperability that does not exist (H61). poincare_distance() correct with EPS guard. log_map uses lambda_base.sqrt() scale factor (slightly non-standard variant). **exp_map bug (H64)**: adds tangent result via Euclidean addition rather than Mobius addition — approximate geodesic stepping. **HNSW stub**: index_built flag written on insert/update but never read; search() is O(n) brute-force with comment "In production, would use ShardedHyperbolicHnsw" (H62). frechet_mean() uses gradient descent on Poincare manifold — correct but calls exp_map with the approximate formula. 4 unit tests covering projection, distance correctness, self-distance=0.

**R98 additions — depth.rs and config.rs (prime-radiant hyperbolic module COMPLETE):**

**hyperbolic/depth.rs (78-82%, 215 LOC, R98):** HyperbolicDepth computes hierarchical depth from a vector's position in the Poincare ball. Core formula: depth = 2 * arctanh(‖x‖) / sqrt(-c). This is mathematically correct (arctanh(‖x‖) → ∞ as ‖x‖→1, matching Poincare ball boundary = "infinity"). classify_depth() maps scalar depth values to HierarchyLevel::Root/High/Mid/Deep/VeryDeep via hardcoded thresholds [0.5, 1.0, 2.0, 3.0] with no mathematical derivation (H75). **CRITICAL H74**: curvature stored but never modulates calculations — sqrt(-c) always evaluates to 1.0 (only curvature=-1.0 tested). All 6 tests use curvature=-1.0 only; multi-curvature depth is effectively untested. Silent clamping: points with ‖x‖≥1 are clamped to 1-EPSILON before arctanh (prevents NaN). weight_multiplier() is dead code — returns 1.0/sqrt(-c) but never called by any sibling file. Module integrates logically into prime-radiant's HierarchyLevel taxonomy used in energy.rs and adapter.rs.

**hyperbolic/config.rs (75-82%, 170 LOC, R98):** HyperbolicConfig is a serde-compatible configuration struct covering curvature (default=-1.0), dimension (default=64), frechet_learning_rate (default=0.01), frechet_max_iterations (default=100), hnsw_m (default=16), hnsw_ef_construction (default=200), depth_weight (default=1.0). Well-structured: implements Default, provides validate() method, has small/large presets. **H76 CONFIRMED**: hnsw_m and hnsw_ef_construction are Euclidean-HNSW defaults (M=16, ef=200 are pgvector/hnswlib defaults for Euclidean ANN). The R97 confirmation that HNSW is never used means these fields are entirely dead configuration. validate() checks curvature<0.0 and dimension>0 but does NOT check that frechet/hnsw params are positive — weak boundary conditions. Preset scaling (small/large) keeps dimension fixed at 64 in both (same default). No from_file() or from_toml() constructor — only serde_json::from_str deserialization.

**Comparison to ruvector-hyperbolic-hnsw (R92):** The dedicated crate (88-93%) has a NATIVE HNSW implementation with tangent-space two-phase pruning and DualSpaceIndex RRF. Prime-radiant's hyperbolic module is a lower-quality subset (~81% avg across 5 files) with no HNSW back-end, an exp_map approximation bug (H64), a merge() curvature bug (H63), and curvature parameters that don't modulate calculations (H74). Both use correct core Poincare math, but the dedicated crate is 7-11 points better on average.

**Prime-Radiant hyperbolic module quality summary (5 files complete):**
```
hyperbolic/mod.rs     88-92%  — Poincare geometry orchestration, brute-force search
hyperbolic/energy.rs  82-87%  — pure data container, merge() curvature bug
hyperbolic/adapter.rs 78-83%  — Poincare-only, exp_map approximation, HNSW stub
hyperbolic/depth.rs   78-82%  — correct depth formula, curvature hardcoded -1.0
hyperbolic/config.rs  75-82%  — dead HNSW config, Euclidean defaults
Weighted avg:         ~81%
```

### 5i. Graph Database

**ruvector-graph (30-35% complete, C)**: Cypher parser is production-quality (1,296-line recursive descent, correct lexer). **CRITICAL**: NO query executor — AST generated but never executed. MVCC incomplete (no conflict detection/GC), ALL optimizations 0% stubs, hybrid features type-defs only. Hyperedge support unique but partial. "Working Cypher queries" claim is FALSE.

**Distributed module (R90, ~2,855 LOC, 5 files) — "transport-absent distributed protocol" (new pattern class):** Every file in ruvector-graph/src/distributed/ shares the same defect: algorithm logic and state machines are correctly designed, but actual network sends are replaced with debug log comments ("In production, send actual network message"). No socket I/O exists anywhere in the module. Quality gradient spans from 15-80%:

- **shard.rs (70-80%, BEST):** Three genuine partitioners — HashPartitioner (xxh3_64 + blake3 dual hashing), RangePartitioner (binary search with dynamic repartitioning), EdgeCutMinimizer (3-phase multilevel k-way: heavy-edge coarsening, greedy initial partition, Kernighan-Lin local search over 10 iterations). GraphShard data container is in-memory DashMap only; no persistence, replication, or split/merge.
- **gossip.rs (45-55%):** Complete SWIM state machine (GossipMessage, MembershipEvent, NodeHealth, incarnation numbers, suspicion timeout). join(), send_ping(), handle_ping(), handle_ack() model SWIM correctly. All send operations are debug logs only. emit_event() never calls registered listeners. Failure detection cannot fire across processes.
- **federation.rs (40-50%):** FederationStrategy dispatch (Parallel/Sequential/Fallback via tokio::spawn) is real. merge_results() performs real node/edge deduplication and stats aggregation. execute_on_cluster() always returns empty QueryResult stub. health_check() hardcodes Healthy. discover_clusters() always returns empty Vec (DNS-SD, Consul, etcd all TODO).
- **coordinator.rs (30-35%):** DashMap concurrency and UUID generation solid. ShardCoordinator fan-out is in-process only (Arc<GraphShard>, no inter-node routing). Query planner is naive string search (contains "match"/"count"/"limit"), not AST-based. 2PC state machine frozen at Active — commit_transaction() removes HashMap entry and logs; no prepare phase, no WAL, no rollback (CRITICAL C26). execute_query() is O(steps × shards) sequential, defeating sharding.
- **rpc.rs (15-20%):** All 4 RPC methods (execute_query, broadcast, health_check, get_shard_info) return hardcoded stubs. RpcServer.start() logs a debug message. GraphRpcService (tonic) feature-gated behind cfg(feature="federation") absent from Cargo.toml defaults — zero gRPC compiles in standard builds (CRITICAL C27). RpcConnectionPool infrastructure (DashMap, get_client()) is correct but connects only to stub clients.

**Contrast**: ruvector-postgres has COMPLETE SPARQL 1.1 system (R34+R52) — BOTH production parser (93-95%, 2,496 LOC) AND executor (92%, 1,884 LOC). Total 7,421 LOC across 7 files. Property paths, all 7 aggregates, full algebra execution, 33+ built-in functions. Cypher has parser only, SPARQL has parser AND executor. ruvector is a **multi-model database** supporting both property graph (Cypher) and RDF triple store (SPARQL) paradigms, though only SPARQL has end-to-end query capability.

### 5j. SONA & Learning

**sona (85%, ~4,500 LOC, R13)**: Production-ready. Complete MicroLoRA (2,211 ops/sec) + EWC++ (online Fisher, adaptive lambda) + ReasoningBank (K-means++) + federated learning + SafeTensors export. Lock-free trajectory recording. ~21 MB memory.

**ruvector-gnn (80%, ~6,000 LOC, C)**: Custom hybrid GNN (GAT+GRU+edge-weighted), not a wrapper. EWC fully implemented. Reads HNSW structure, refines embeddings. 3 unsafe blocks (mmap, properly audited).

### 5k. Subpolynomial Algorithms (R52)

**subpolynomial/mod.rs (45-50%, 1,385 LOC, R52)**: Theatrical claims with partial implementation. Claims to implement a "December 2024 breakthrough" from arXiv:2512.13105, but the arXiv ID format is invalid (2512 = Dec 2025, not 2024). The theoretical foundation is suspect.

**FALSE complexity**: Claims O(n^{o(1)}) subpolynomial update time but implements O(log n) levels × O(recourse). Same pattern as R39's false sublinearity in sublinear-time-solver. Claims "Deterministic" subpolynomial mincut, which is an OPEN PROBLEM in graph algorithms — neither randomization nor subpolynomial bounds are achieved.

**Partial implementation**: Multi-level hierarchy and incremental API (insert_edge/delete_edge) are real. Core algorithmic primitive (expander splitting) has TODO "A full split would require more complex logic". Falls back to full recomputation on deletions (NOT truly incremental). Two supporting modules (fragmentation, witness) imported but NEVER CALLED. 12 tests cover API behavior but not complexity bounds.

**Comparison to R42 dynamic_mincut**: More ambitious claims but less complete implementation. dynamic_mincut EXCEEDS R34 with working algorithms; subpolynomial/mod.rs has grander documentation but incomplete primitives.

### 5l. Edge-Net AI Layer (R52+R54)

**PRODUCTION-GRADE edge computing stack** — SIMD compute (R52) + LoRA inference (R54) + Federated learning (R54). Three files, combined 93-96% weighted avg.

**simd.rs (92-95%, R52)**: Complete SIMD for NN inference (see 5l-simd below).

**lora.rs (90-95%, 1,355 LOC, R54)**: Complete edge LoRA implementation. True low-rank adaptation W' = W + (A·B) * (alpha/rank) with Kaiming init for A, zero-init for B. Dual SIMD targets (AVX2 + WASM128) with automatic detection — same architectural pattern as simd.rs. Q4/Q8 quantization for edge devices (4-8× memory reduction). LRU adapter pool with configurable slots, task-based cosine similarity routing, usage tracking. Online gradient accumulation (SGD update on B matrix). P2P serialization via bincode. 9 WASM exports. 15 tests. **Independent from micro_lora.rs** — this is inference-focused (quantized, WASM, adapter pool) while micro_lora.rs is training-focused (EWC++, federated, sona). ComputeOps trait defined but UNUSED (over-engineering). Task embeddings hardcoded, not learned.

**federated.rs (95-98%, 1,218 LOC, R54)**: **BEST federated learning in entire project** — exceeds sona (85%) by 10-13 points. Five major components: (1) TopK Sparsifier with stateful error feedback (Deep Gradient Compression, arXiv:1712.01887, 90% compression ratio), (2) Byzantine detection via coordinate-wise median + MAD with Z-score threshold (1.4826 scaling), (3) Differential privacy with (ε,δ)-DP Gaussian mechanism (Box-Muller transform, WASM-compatible PRNG), (4) Reputation-weighted FedAvg with superlinear weighting (rep^1.5), (5) Gossipsub gradient sharing protocol with multi-stage validation (model hash, staleness, reputation, magnitude). SGD with momentum for model updates. Cross-platform WASM support. 13 tests. **Missing**: No actual libp2p networking code (architectural separation — networking in p2p.rs per R44). No signature implementation (field exists, unused).

**Edge-net stack verdict**: The complete edge-net AI pipeline (SIMD→LoRA→Federated) is **production-grade** at 93-96% weighted. Combined with p2p.rs (92-95%, R44), this is a real distributed edge AI system — inference, adaptation, and collaborative learning all functional.

### 5l-simd. Edge-Net SIMD Compute (R52)

**simd.rs (92-95%, 1,418 LOC, R52)**: Complete, independent SIMD compute library for neural network inference. **CRITICAL finding: completely independent from ruvector-core**. The ruvector ecosystem has TWO independent SIMD codebases with zero code sharing:
- **ruvector-core**: Distance metrics (L2, cosine, dot product) for HNSW indexing
- **edge-net**: NN layer operations (matmul, activations, normalization, quantization) for inference

Real SIMD intrinsics: AVX2 (`#[target_feature(enable = "avx2")]`), WASM simd128, SSE4.1, with runtime dispatch via `is_x86_feature_detected!()`. Numerically stable: softmax uses log-sum-exp trick (tested with [1000, 1001, 1002]), layer norm uses Welford's algorithm with f64 accumulation. Production Q4/Q8 quantization: block-wise with per-block scales, on-the-fly dequantization in matvec, <15% Q4 error, <2% Q8 error.

Activation functions: GELU (fast tanh via Padé approximation), ReLU (SIMD), SiLU (scalar only — missed optimization). Tiled matrix multiplication with TILE_SIZE=64 but suboptimal B column gathering (strided access, per-iteration Vec allocation). 19 tests validate correctness. CPU-only (no GPU backend integration despite R38 CUDA-WASM findings).

### 5m. Development Methodology

RuVector is **explicitly AI co-authored**. Commits credit "Claude Opus 4.5/4.6". Velocity: 834 commits in 81 days (10.3/day), ~600 LOC/commit, 76 crates (~0.94/day), v0.1.0 "Production Ready" 1 day after repo creation. This is 6-20x faster than sustainable human-only development.

Scope (GNN, quantum, FPGA, distributed consensus, graph DB, 39 attention types, PostgreSQL extension) would typically require 2-3 years for an experienced team. Real achievement is demonstrating human-AI collaboration at scale, not creating battle-tested production system.

**Bulk feature pattern**: Feb 8, 2026 — temporal tensor store ~4,000 lines, 170+ tests. Feb 8 — quantum simulation 306 tests, 11 improvements. Feb 6 — exotic quantum-classical 8 modules, 99 tests.

### 5n. ruvector-core Advanced Features (R90)

**R90 deep-read confirmed a quality gradient within ruvector-core**: core algorithms (HNSW, SIMD) 90-98%, advanced_features/ module 85-93%, advanced/ module 60-90%. Total 4 files, ~2,104 LOC, avg 80-87% real.

**product_quantization.rs (88-92%, 551 LOC)**: Complete Product Quantization implementation resolving H1. k-means++ initialization (distance-weighted D² random sampling), Lloyd's algorithm (assignment step + centroid update), Asymmetric Distance Computation (ADC) with LookupTable (query-to-centroid distances computed once at creation, distance() sums via table lookup). encode() finds nearest centroid via exhaustive scan — O(k × subspace_dim) per subspace, no SIMD acceleration. Minor logic bug in k-means++ fallback (tautological condition line 384, harmless). Test suite covers creation, training, encoding, lookup table accuracy, compression ratio.

**conformal_prediction.rs (88-93%, 505 LOC)**: Valid split-conformal prediction (Vovk et al. 2005). calibrate() computes nonconformity scores from held-out calibration set, compute_threshold() sets (1-alpha) quantile with correct finite-sample Bonferroni-style correction — ceil((1-alpha)*(n+1)/n) formula. Three nonconformity measures: distance threshold, inverse rank (1/(rank+1)), normalized distance with per-query average normalization. predict() implements all three and returns sets of candidates exceeding threshold. adaptive_top_k() delegates to predict().results.len() (pragmatic). 7 tests using mock search functions.

**hypergraph.rs (85-90%, 551 LOC)**: Genuine bipartite hypergraph index. Correct incidence representation via entity_to_hyperedges (HashMap<VectorId, HashSet<String>>) and hyperedge_to_entities (HashMap<String, HashSet<VectorId>>). k_hop_neighbors() implements BFS over hyperedge-mediated paths correctly (node→hyperedge→all other nodes in hyperedge, not pairwise edges). CausalMemory computes utility function U = alpha*similarity + beta*causal_uplift - gamma*latency_penalty, with causal_uplift using log1p of co-occurrence counts (prevents outlier domination). Temporal index with four granularities (hourly/daily/monthly/yearly) via floor division. Cites HyperGraphRAG (NeurIPS 2025). Tests verify 2-hop reachability and causal utility queries.

**tda.rs (60-70%, 497 LOC) — MISLABELED (CRITICAL C25)**: Named "Topological Data Analysis" but implements ZERO canonical TDA algorithms. No Vietoris-Rips complex, no boundary operators, no Betti numbers, no persistence diagrams. Implements: kNN graph construction (all-pairs O(n²) epsilon-neighborhood), connected components (recursive DFS), clustering coefficient (triangle counting via shared neighbors), degeneracy detection (covariance matrix then diagonal-element singular value approximation — NOT a real SVD, invalid for non-axis-aligned manifolds), persistence approximation (component count at 5 fixed scales [0.1, 0.5, 1.0, 2.0, 5.0] — not birth/death pairs). mode_collapse detection (coefficient of variation of pairwise distances) is a reasonable heuristic. This is an embedding quality analyzer, not TDA.

**Quality gradient confirmed (R90)**:
```
ruvector-core algorithms (HNSW, SIMD): 92-98% — production-ready
ruvector-core advanced_features/ (PQ, conformal): 88-93% — production-ready
ruvector-core advanced/ (hypergraph): 85-90% — production-ready
ruvector-core advanced/ (tda.rs mislabeled): 60-70% — functional but misleading
ruvector-graph distributed protocols: 40-55% — correct design, no transport
ruvector-graph distributed transport: 15-20% — stubs only
```

### 5o. MinCut-Gated-Transformer (R91+R93+R94+R96)

**MOST NOVEL CRATE confirmed — ~84% weighted avg across 27 DEEP files (R91+R93+R94+R96).** The ruvector-mincut-gated-transformer crate implements a quantized INT8 transformer inference engine with a novel energy-based gating mechanism derived from minimum-cut graph theory.

**Core novelty mechanism — energy_gate.rs (88-93%, R93):** The "MOST NOVEL" rating (R34) traces to this file. Implements a genuine 3-component Energy-Based Transformer (EBT) energy function: sigmoid lambda decay (1/(1+lambda/150)), boundary edge penalty, and partition entropy (ln(k)/ln(10)). Central-difference finite gradient computation drives System-2 iterative refinement via gradient descent on the controllable lambda variable. Dominant gradient component selects intervention type. Low-confidence states fall back to rule-based GatePolicy — a pragmatic hybrid. Cites Gladstone 2025. The scale-sensitivity of hardcoded normalization constants (lambda/150, boundary/100, max 10 partitions) is the main engineering gap.

**Early exit — early_exit.rs (88-92%, R93):** Implements CoherenceEarlyExit, an alternative to LayerSkip (Elhoushi et al. 2024). Uses precomputed mincut lambda signals from GatePacket to decide when to terminate early and begin speculative token generation. Novel 40/40/20 confidence weighting: lambda-strength + stability + boundary dispersion. Adaptive exit layer adjusts based on lambda stability. Does not directly import energy_gate.rs — dependency is mediated by transformer forward pass.

**Computational kernels (88-93% avg, R93):**
- **qgemm.rs (88-93%):** Production quantized GEMM. Real AVX2 (correct widening madd_epi16, prefetch pipelining) and NEON (dual accumulators, vaddvq horizontal reduction). Asymmetric per-row scales matching llama.cpp convention. No cache-blocking/tiling limits throughput on large matrices. SIMD dispatch is compile-time only (no runtime detection — H43).
- **ffn.rs (88-92%):** Standard GELU FFN (Vaswani 2017), NOT SwiGLU/GeGLU despite crate name. Genuine SIMD GELU: AVX2 Padé tanh (27+x²)/(27+9x²), NEON Newton-Raphson reciprocal. Allocation-free forward pass. Single-scale approximation bug (H47).
- **spike_driven.rs (88-93%):** Genuine neuromorphic spike-driven attention following Yao et al. 2023. Real LIF neuron rate coding with membrane accumulate-and-reset, correct STDP temporal coincidence detection, refractory period enforcement. 12 tests. However, spike_value_contribution() uses saturating_mul, contradicting "multiplication-free" claim (H37). Zero integration with energy_gate.rs.
- **q15.rs (88-92%):** Correct Q15 fixed-point newtype (transparent u16). Widening multiply, direction-aware LERP, 11 tests. Exported but UNUSED within crate — rope.rs duplicates the constant as raw i16 (type mismatch H41).

**Attention variants:**
- **sparse_attention.rs (72-80%, R93):** Novel design thesis — mincut lambda values control attention density (cites MInference NeurIPS 2024). GatePacket API with lambda, boundary_concentration_q15, partition_count. Well-reasoned adaptive density formula. **BUT**: partition boundaries are PLACEHOLDER (uniform stride, H39) and density parameter is dead code (H40). Functionally equivalent to fixed-stride BigBird local attention.
- **speculative.rs (88-92%, R91):** EAGLE-style tree speculative decoding with novel lambda-guided confidence. Sequential path verification (not parallel tree forward).

**KV cache management (72-87% avg, R91+R93+R96):**
- **manager.rs (72-78%, R93):** AdaptiveKVCache with 3-tier Hot/Warm/Archive. PARALLEL to legacy.rs (zero imports between them). Pure FIFO eviction (H44). SQuat/KVQuant quantizers declared but dead (H45). Rematerialization policy complete but never triggered (H46). Presets match LLaMA-2/3 and GPT-4 parameters.
- **kvquant.rs (78-85%, R93):** Pre-RoPE KVQuant (Hooper et al. 2024) — quantize keys BEFORE RoPE for lower dynamic range. Correct deferred RoPE formula. 3-bit packing broken (falls back to 4-bit, H48). Per-layer calibration API lies (flattens to single pair).
- **kivi.rs (72-78%, R96):** KIVI paper's bit-packing primitive. GENUINE FWHT (butterfly, self-inverse, power-of-2 enforced). CORE DEFECT: per-channel quantization claim FALSE — global min/max regardless of QuantScheme (H60c). PerGroup discards group_size (H60c). SIMD dequantize TODO (H60d). Complementary to quantized_store.rs (tier/graduation logic) — provides low-level bit-packing.
- **squat.rs (78-84%, R96):** SQuAt 2024 paper. Hadamard basis + Gram-Schmidt math CORRECT. 2/4-bit packing COMPLETE. calibrate() STUB — ignores calibration data (H60b). No integration with sibling kv_cache files.
- **policy.rs (82-87%, R96):** Three-struct architecture (MemoryTracker/TierPolicy/RematerializationPolicy) genuine. Age-based graduation only — NO H2O/attention-sink/StreamingLLM (H60j). Evaluate/tracker pressure divergence (H60k). Cost model inversion (older tokens appear cheaper to recompute). No direct hot_buffer.rs integration.
- **hot_buffer.rs (82-87%, R96):** FIFO ring buffer for recent tokens in "FP16" (actually f32 — test acknowledges). Separate high-precision tier. pop_oldest() BUG — no read cursor (H60e). Dual-API trap (push vs push_head+advance — H60e). Zero policy.rs integration. keys()/values() correctly reconstruct chronological order for wrapped ring buffer.
- **legacy.rs (82-88%, R91):** RotateKV (IJCAI 2025) with FWHT. Per-head scale stomping bug (H28). No eviction (H29).
- **tier.rs (90-93%, R97):** Clean 3-tier definitions (Hot/Warm/Archive = FP16/4-bit/2-bit). Age-based not access-frequency tier assignment (H68). 9 unit tests.

**R96 additions — arena + trace:**

**arena.rs (92%, R96):** GENUINE bump allocator built specifically for MinCut inference weight layout. Single Vec<u8> backing store with manual offset tracking. 64-byte cache-line alignment via bit-mask arithmetic. WeightRef stores u32 offset+size (serialization-safe). calculate_arena_size encodes QKV+FFN layout (ignores _heads param — GQA/MQA unsupported). SAFETY comments on all unsafe blocks are accurate. **CRITICAL**: multiple &mut slices from same Vec trigger aliasing UB (H60a). reset() leaves stale weights readable via new alloc slices. 10 unit tests.

**trace.rs (88-92%, R96):** Gate-decision diagnostics with zero heap allocation. Fixed-size [T;64] stack arrays as circular rolling buffer. Records GateDecision/GateReason/lambda/tier from packets.rs. Tier 0-3 classification consistent with packets.rs semantics. lambda_trend() splits window in half and compares means (simple monotone detection, not regression). Feature gate claim documentation-only — compiles unconditionally (H60i). No serialization output path. 6 genuine unit tests.

**Quantization pipeline:** q15.rs (u16) and rope.rs (i16) use the same Q15_SCALE constant (32768.0) but are **architecturally disconnected** due to signed/unsigned divergence. The quantized inference path would flow: q15.rs format → qgemm.rs INT8 matmul → ffn.rs GELU → output, but q15.rs newtype is never imported by any sibling.

**R94 additions — infrastructure + routing (7 files, ~3,534 LOC, ~83% weighted avg):**

**packets.rs (90-95%, BEST in session):** Pure type-definition coherence interface connecting energy gate → spike scheduler → transformer kernel. GatePacket carries lambda, boundary_edges, partition_count. GateDecision is a 5-level hierarchy: Allow → ReduceScope → FlushKv → FreezeWrites → QuarantineUpdates. QuarantineUpdates is architecturally novel — compute runs but all state changes discarded, enabling speculative inference. SpikePacket bridges LIF+STDP output to sparse attention. InferInput supports dual modes (tokens or quantized embeddings). InferStats tracks qgemm_calls, tokens_skipped (MoD), early_exit_layer — confirming subsystem integration. All repr(C) with consistent Q15 fixed-point for cross-language ABI.

**state.rs (88-92%):** Zero-alloc RuntimeState manages all inference buffers as a single contiguous Vec with computed BufferLayout offsets: Q → K → V → attn_scores → FFN → residual → norm → K_cache → V_cache, all 64-byte aligned. KV ring buffer with per-layer write indices and valid lengths. Inference-only — no checkpoint, no safetensors, no version tagging. Unsafe accessor inconsistency: 6 of 9 buffer accessors use unbounded `[start..]` slicing (UB risk if layout is wrong, H57).

**quantized_store.rs (88-92%):** Two-tier KIVI quantized KV cache — 4-bit warm tier, 2-bit archive tier. Correct asymmetric quantization (PerChannel keys, PerToken values per KIVI paper). Warm→archive graduation via dequantize+requantize (cascaded quantization error, no error diffusion). Orthogonal to kvquant.rs — imports kivi.rs only. Silent no-op on warm overflow (H54).

**mod_routing.rs (72-78%):** MoD (Mixture-of-Depths) routing, NOT MoE. Deterministic lambda-delta signal from GatePacket drives per-token skip/compute decisions. |λ_delta| < threshold → stable → more skipping; |λ_delta| >= threshold → unstable → more compute. 50% FLOPs hard cap via layer_capacity_ratio. BUT: route_unstable_tokens() and route_stable_tokens() are functionally identical (H52), and boundary detection uses regular stride intervals, not actual mincut edge IDs (H53).

**window.rs (82-87%):** Causal sliding window attention — each token attends to W previous tokens. NOT Longformer despite docstring claim (no global/CLS tokens, no two-pattern attention). Scalar attention kernels (no SIMD in Q@K^T or V weighted sum — only qkv_projection delegates to qgemm_i8). Feature-gated SparseMask bridge to sparse_attention.rs under `cfg(feature = "sparse_attention")`. Correct numerical-stable softmax (max subtraction + inv_sum). Asymmetric Q vs KV layout fragility.

**metrics.rs (78-82%):** QualityTracker tracks PPL-degradation and task-accuracy scores in a rolling 1,000-entry VecDeque. Correct sliding-window subtract-on-evict pattern (floating-point drift risk is acceptable at window size 1000). Docstring falsely claims "cache hit rates per tier" but has zero hit-rate tracking (H59). tier_metrics() returns hardcoded min/max/std_dev. boundary_adjustment_factor() and should_adapt() are dead code from manager.rs's perspective.

**quant4.rs (72-78%):** Plain RTN (round-to-nearest) 4-bit quantization with absmax scaling. NOT GPTQ/AWQ — no Hessian-based error minimization, no activation-aware reordering. Fully scalar (no SIMD) despite being in kernel/ directory alongside SIMD-heavy qgemm.rs (H55). BlockInt4Weights is write-only (no dequantize_row, no GEMV — H56). Wastes 12.5% INT4 range (never uses -8 slot).

**R97 additions — config.rs, spike.rs, kv_cache/tier.rs (3 files, ~1,040 LOC, ~90% weighted avg):**

**config.rs (88-92%, 369 LOC):** Well-structured serde config for transformer inference. TransformerConfig covers: num_layers, hidden_size, num_heads, head_dim, seq_len_max, window_normal/degraded/critical (3-tier degraded mode hierarchy). GatePolicy with Q15 fixed-point ratios for allow/reduce/flush/freeze thresholds and conservative/permissive presets. Both derive Serialize/Deserialize (fully TOML-compatible). Correct kv_cache_bytes() formula uses layers×seq_len_max×hidden but omits head dimension multiplier for multi-head (H67 adjacent). validate() checks window_normal > 0 but misses window_degraded > 0. No from_file() or from_toml() constructor (only serde_json::from_str). Tests: 3 covering baseline_config, micro_config, invalid_config. **Coverage gap (H67)**: NO fields for energy gate EBT weights, spike neuron parameters (tau_m, tau_s, threshold), quantization bit-width selection, KV cache tier boundaries (Hot/Warm/Archive capacities), or MoD routing thresholds — five out of nine DEEP subsystems are unconfigurable through this file.

**spike.rs (88-92%, 366 LOC):** SpikeScheduler is a TOKEN DISPATCH LAYER, not a spiking neuron implementation (H69). Reads SpikePacket from packets.rs (coherence with R94). Q15 rate-tiering: rate_q15 above high_threshold → Tier 0 (compute), above medium_threshold → Tier 1 (reduced), above skip_threshold → Tier 2 (skip), else → inactive. FNV-1a novelty hashing (offset_basis=0xcbf29ce484222325) for compute_input_signature() and compute_embedding_signature(). build_sparse_mask() populates boolean mask Vec<bool> from SpikePacket.top_indices() with bounds-checking. no_std compatible (extern crate alloc). 7 tests covering all scheduling branches. Zero spiking neuron dynamics — no membrane_potential, no spike threshold, no STDP weight updates. Contrast with spike_driven.rs (R93, 88-93%) which has genuine LIF+STDP. The two files are complementary: spike_driven.rs generates spikes, spike.rs dispatches based on them.

**kv_cache/tier.rs (90-93%, 305 LOC):** Pure definitions file — enums, structs, and pure functions only (no state, no mutable operations). CacheTier enum: Hot (FP16, 16-bit), Warm (4-bit KIVI), Archive (2-bit KIVI/SQuat/KVQuant). TierConfig fields: hot_capacity, warm_capacity, adaptive bool, quality_threshold (0.95 default). Three presets: default (hot=64, warm=512), long_context (hot=64, warm=1024), extreme_context (hot=32, warm=2048). TierBoundary.tier_for_position() computes age = current_len - position - 1 and delegates to tier_for_age() — **age-based (positional), NOT access-frequency** (H68). requires_dequantization() returns true for Warm and Archive — every attention step for non-hot tokens requires dequant (performance cost). TierCounts.memory_bytes() correct per-tier byte math (Hot=2B/elem, Warm=0.5B/elem+4B scale, Archive=0.25B/elem+4B scale). 9 unit tests covering tier_bits, compression ratios, boundary defaults, tier_for_age edge cases. Perfectly consistent with quantized_store.rs two-tier KIVI (R94, 88-92%) — tier.rs provides the type system, quantized_store.rs implements the logic.

**R98 additions — lib.rs (crate root) and norm.rs (kernel):**

**lib.rs (90-95% EXCELLENT, 261 LOC, R98):** Crate root serving as the public interface for the entire MinCut-Gated-Transformer crate. 23 public module declarations: 18 unconditional (kernel/qgemm, kernel/ffn, energy_gate, early_exit, packets, state, mod_routing, window, metrics, quant4, arena, trace, spike, config, speculative, quantized_store, norm, transformer) + 5 feature-gated (spike_attention, spectral_pe, sparse_attention, energy_gate_extra, kv_cache/*). 60+ exported items via explicit re-exports: MincutGatedTransformer, TransformerConfig, GatePolicy, GatePacket, GateDecision, SpikePacket, InferInput, InferStats, CacheTier, TierConfig, plus all KV cache types and ArenaAllocator. 10 feature flags with semantic groupings: default=[sliding_window], full=[simd,trace,spike_attention,spectral_pe,sparse_attention,energy_gate,kv_cache], novel=[spike_attention,spectral_pe,sparse_attention,energy_gate]. Prelude module re-exports 43 items. Dual KV cache re-export: legacy manager::AdaptiveKVCache (backward compat) + new kv_cache::* types (three-tier). Excellent rustdoc: opening doc comment cites Gladstone 2025 + energy gate concept, includes working code example demonstrating TransformerConfig → MincutGatedTransformer flow. **ARCHITECTURAL CONFIRMATION**: ALL 22+ DEEP-analyzed modules from R93-R97 are publicly accessible — no hidden silos, no dead modules (unlike lib_simple.rs in other crates which excluded algorithms). 3 HIGH findings: (a) "novel" feature flag is non-additive (resets full features), (b) transformer.rs module declared but NOT in file registry (unread DEEP target), (c) feature combinations create combinatorial build matrix (untested combinations).

**norm.rs (55-65%, 213 LOC, R98):** LayerNorm and RMSNorm implementations for the MinCut transformer. Math is correct: LayerNorm uses standard two-pass algorithm (mean pass, then variance pass) with epsilon=1e-5; RMSNorm uses single-pass root-mean-square normalization. In-place variants (layernorm_inplace, rmsnorm_inplace) modify output slice directly. f32_to_i8() and i8_to_f32() quantization helpers for the INT8 inference path. ALL implementations are pure scalar — no SIMD intrinsics anywhere in the file, no `#[target_feature]`, no unsafe blocks, no AVX2/NEON. This contrasts sharply with sibling kernel files: qgemm.rs has real AVX2+NEON widening multiply-accumulate, ffn.rs has Padé tanh + Newton-Raphson GELU. **Dead code anti-pattern (H73)**: `#[cfg(feature = "rmsnorm")]` block contains byte-identical body to the unconditional block — the feature provides no behavioral difference. 6 passing tests. WEAKEST MinCut kernel by far.

**Quality gradient (consistent with broader ruvector pattern, updated R98):**
```
Crate interface (lib.rs):                                   90-95%
Novel algorithms (energy gate, early exit, speculative):    88-93%
Computational kernels (qgemm, ffn, spike_driven, q15):      88-93%
Coherence types (packets, tier.rs):                         90-95%
Runtime state (state, quantized_store, spike.rs, config):   88-92%
Attention variants (window):                                 82-87%
Infrastructure (manager, kvquant, sparse_attention,
               metrics, mod_routing, quant4):                72-82%
Normalization kernel (norm.rs):                              55-65%
```

MinCut crate now has 24 DEEP files with ~84% weighted average.

### 5p. ruvector Non-MinCut LLM Extensions (R91)

Five files (3,935 LOC) from ruvector that implement LLM inference primitives as standalone modules independent of ruvllm. These are lower-level building blocks (speculative decoding, RoPE, KV cache, mmap, compression) that can be composed into inference pipelines.

**Quality gradient (R91, weighted avg ~83%):**
```
rope.rs:             88-92% — production-ready RoPE (independent from ruvllm/kernels/rope.rs)
speculative.rs:      88-92% — EAGLE tree decoding (sequential verification, not parallel)
mmap.rs:             88-92% — production memmap2 for GNN training (not HNSW)
kv_cache/legacy.rs:  82-88% — RotateKV with scale stomping bug; no eviction
compress.rs:         55-65% — MISLABELED + fake f16; binary quantization correct
```

**mmap.rs (918 LOC, 88-92%)**: Production memmap2 file-backed memory mapping for gradient accumulation in GNN training. AtomicBitmap with CAS-based lock-free bit set/clear operations (dirty page tracking). Linux madvise(MADV_WILLNEED) for prefetching. MmapGradientAccumulator wraps the mmap region with RwLock granularity for concurrent readers. `#![cfg(not(wasm32), feature = "mmap")]` compile-time gating is clean. Seventeen tests cover bitmap ops, mmap creation, and gradient accumulation. **No HNSW integration** — only GNN training callsites. Pin count fields allocated but pin_count never used (no eviction guard, H30).

**compress.rs (679 LOC, 55-65%) — CRITICAL C28+C29**: Named "graph compression" but implements embedding/tensor quantization across 5 temperature tiers. HotCompressor (identity pass-through), WarmCompressor (fake f16: val*1000 → i16, CRITICAL C29), CoolCompressor (Q8 scalar quantization, correct), ColdCompressor (labelled PQ but uses trivial linear interpolation centroids, not k-means, HIGH H32), ArchiveCompressor (binary quantization by sign — correct). PQ4 outlier handling is genuine: z-score detection (> 3σ), sparse storage for outliers vs packed storage for inliers. 12 unit tests, none covering overflow or precision edge cases for fake f16.

**speculative.rs (788 LOC, 88-92%)**: EAGLE-style tree speculative decoding. Builds speculative draft trees with branching factor controlled by lambda-guided confidence (novel: lambda derived from mincut boundary signal, not standard confidence scoring). Standard textbook rejection sampling: accept token i if u ≤ min(1, p_target(x_i) / p_draft(x_i)). Tree attention mask is correctly computed. **Limitation (H31)**: path verification is sequential along the accepted prefix — no batch forward pass over all tree branches simultaneously. The architectural innovation is in tree construction; EAGLE's parallel verification speedup is not realized. Logit-processing layer only — no model weights or KV cache objects embedded; composable with external model runners. 9 genuine tests.

**rope.rs (777 LOC, 88-92%)**: Correct RoPE rotary embeddings per Su et al. 2021. NTK-aware scaling uses the CodeLlama/Qwen formula (scale = (max_seq_len / base_seq_len)^(d/(d-2)) per frequency pair). Partial YaRN: base frequency scaling and frequency bands (ramp_up) implemented; the attention scale factor (√(1 + 0.1·log(scale))) from the YaRN paper is absent. Q15 fixed-point quantized path for edge inference. 11 substantive tests covering rotation correctness, NTK scaling, and quantization round-trips. No false SIMD claims. Independent from ruvllm/kernels/rope.rs (R35, 95%) — that file targets Apple Silicon NEON; this one is platform-agnostic.

**kv_cache/legacy.rs (773 LOC, 82-88%)**: Implements RotateKV rotation (IJCAI 2025) using Fast Walsh-Hadamard Transform for key diversity before caching. 2-bit quantization uses correct bit-packing (4 values per byte). 4-bit quantization uses correct 2-nibble packing. **Scale stomping bug (H28)**: per-head min/max scales recomputed on each append, discarding prior scale; dequantization of full history silently corrupted after first update. No eviction or capacity limit (H29) — unbounded growth. 15 genuine tests, but none cover multi-token dequantization correctness across appends (would expose H28). Labeled "legacy" but no replacement file observed.

**R91 ruvector LLM extensions verdict**: rope.rs and speculative.rs are production-quality standalone modules. mmap.rs is solid infrastructure for GNN. kv_cache/legacy.rs has a correctness bug and lacks eviction. compress.rs is mislabeled, has fake f16, and should not be used for real graph or tensor compression without significant fixes.

### 5p. Hyperbolic Geometry (R92+R99)

**FIRST DEEP READS on 63-file crate family — confirmed GENUINE at 88-95%.** Three distinct hyperbolic geometry implementations discovered.

**ruvector-hyperbolic-hnsw (5 files DEEP, ~2,837 LOC, 87-93% weighted — CRATE COMPLETE as of R99):** Complete Poincaré ball HNSW implementation. hnsw.rs (88-93%) is a NATIVE implementation distinct from ruvector-core's hnsw_rs wrapper. Uses fused_norms() for 3x memory bandwidth reduction, tangent-space two-phase search (prune via Euclidean in tangent space, then exact Poincaré for top-N), DualSpaceIndex (Poincaré + Euclidean with RRF fusion). poincare.rs (90-95%) is the mathematical foundation — all Möbius operations verified correct (Ganea et al. 2018), comprehensive numerical stability (EPS guards, clamping, fast_acosh). shard.rs (82-85%) implements hyperbolic-aware radius partitioning (bins by ||x|| in Poincaré ball, which correlates with hierarchy depth), canary deployment pattern, Spearman correlation for hierarchy validation. Transport-absent by design (no sockets, designed for separate coordination layer). WASM wrapper (88-92%) is the 17th GENUINE WASM — 7 real math operations, ShardedIndex, WasmTangentCache, zero stubs.

**R99 — tangent.rs (88-92%, 349 LOC):** Completes the ruvector-hyperbolic-hnsw crate. TangentPruner implements the two-phase pruning strategy referenced in hnsw.rs: TangentCache precomputes Fréchet mean centroid (correct iterative Riemannian GD via poincare.rs log_map/exp_map), then TangentPruner.prune() (1) logs all candidates into tangent space at that centroid, (2) filters by cheap Euclidean distance, (3) re-ranks survivors with exact Poincaré geodesic distance, returning top-N. Design note: search() is O(N) linear scan over the full vector store — this is by design, as the pruner is meant to receive candidates from the HNSW layer at the caller level. However no caller currently wires HNSW traversal output into TangentPruner.search() (H77). Dead import: norm_squared from poincare.rs (H78). Two genuine tests covering full pipeline construction and single round-trip. All imports from poincare.rs and error.rs are consistent — crate is internally coherent throughout.

**ruvector-hyperbolic-hnsw crate quality summary (5 files, all DEEP):**
```
poincare.rs    90-95%  — math foundation, all formulas correct
hnsw.rs        88-93%  — native HNSW, tangent-space two-phase pruning
lib.rs (wasm)  88-92%  — 17th GENUINE WASM
tangent.rs     88-92%  — Fréchet centroid cache, two-phase pruner
shard.rs       82-85%  — radius partitioning, transport-absent
Weighted avg:  ~88%
```

**ruvector-attention hyperbolic module (1 file DEEP + prior coverage, ~580 LOC):** lorentz_cascade.rs (90-93%) is a genuine Lorentz model (hyperboloid) attention mechanism — one of the most mathematically rigorous files in the ecosystem. Uses Minkowski inner product ⟨x,y⟩_L = -x₀y₀ + Σxᵢyᵢ (correct signature), Busemann function scoring B_ξ(x) = log(-⟨x,ξ⟩_L) for O(d) hierarchical attention, multi-curvature cascade (each head at different log-spaced curvature for multi-scale hierarchy capture), Einstein midpoint centroid, correct log/exp maps on the hyperboloid. Inference-only (curvatures/focal directions fixed, no learning mechanism — H35).

**ruvector-postgres hyperbolic module (2 files DEEP, R98):** poincare.rs (88-92%, 268 LOC) and lorentz.rs (87-92%, 258 LOC) are now DEEP-read. Both are pure Rust library files — NOT direct pgx entry points. SQL exposure is indirect via operators.rs `#[pg_extern]` re-exports (DEEP R96). poincare.rs implements correct Poincare ball math (Mobius addition, exp/log maps, curvature scaling). lorentz.rs implements correct Lorentz/Minkowski model with bidirectional coordinate transforms. CRITICAL H70: lorentz.rs distance() accepts off-hyperboloid points silently. See Section 5d-ii for full SQL hyperbolic analysis.

**Three distinct HNSW implementations now all DEEP-confirmed:** ruvector-core (98-100% hnsw_rs wrapper), micro-hnsw-wasm (60-70% novel no_std), hyperbolic-hnsw (88-93% native Poincaré, CRATE COMPLETE R99). Quality ordering: hyperbolic-hnsw > ruvector-core (wrapping) > micro-hnsw-wasm (novel but untested).

### 5p-ii. GNN Bindings — napi-rs and WASM (R99)

**Verdict: GENUINE inference-only bindings at 88-94% — 18th GENUINE WASM confirmed.** Two files complete the FFI surface for ruvector-gnn: a Node.js binding (napi-rs) and a WASM binding (wasm_bindgen). Both share the same inference-only API surface and both genuinely delegate to ruvector_gnn core crate algorithms.

**gnn-node/lib.rs (88-92%, 428 LOC, R99):** Production napi-rs bindings using all four attribute macros: `#[napi]` (function export), `#[napi(constructor)]` (class init), `#[napi(factory)]` (named constructor), `#[napi(object)]` (data-transfer object). GnnGraph class exposes: forward(), compress(), decompress(), differentiable_search(), hierarchical_forward(). CompressionTier enum (None/Half/PQ8/PQ4/Binary) with u8 discriminant, correct defaults. Error handling is solid: Status::InvalidArg returned for range violations (e.g., k parameter validation), .map_err() with Status::GenericFailure for all Rust error conversions — zero unwrap()/panic!() in error paths. Imports 6 real symbols from ruvector-gnn core crate (compress, layer, search modules). **Performance note**: hierarchical_forward() deserializes layer configs from JSON strings on every call (H79) — serde overhead per invocation. No training APIs exported (H80).

**gnn-wasm/lib.rs (90-94%, 415 LOC, R99) — 18th GENUINE WASM:** All exports via `#[wasm_bindgen]` delegate to real ruvector_gnn core algorithms via serde_wasm_bindgen JsValue deserialization. Same inference API as gnn-node: forward(), compress(), decompress(), differentiable_search(), hierarchical_forward(). Additional: inline cosineSimilarity() JavaScript-callable helper with correct numerics — dot(a,b) / (‖a‖·‖b‖) with 1e-8 epsilon guard (95% correct). console_error_panic_hook::init() properly called via Once on first construction (correct initialization pattern). Tests are browser-targeted (#[cfg(target_arch = "wasm32")]) — construction-only, no round-trip coverage (M2). No training APIs exported (H80).

**GNN bindings verdict**: Both FFI layers are genuine wrappers around ruvector_gnn core. The "inference-only" gap (H80) is an architectural limitation — training code exists in the core crate (scheduler.rs, replay.rs, confirmed R94) but is deliberately excluded from the public FFI surface. This is a design choice (deploy-only edge inference), not a facade. The 18th GENUINE WASM brings the ecosystem WASM ratio to 18 genuine / 13 theatrical = 58% genuine.

### 5q. AIDefence Security Module (R92)

**Verdict: PARTIALLY REAL (82-88%) — genuine regex-based security, not AI.** README-REALITY-CHECK previously classified this as UNCOVERED (16 files, 0 DEEP). R92 resolves: the "AI" in AIDefence is misleading.

**AIDefenceGuard.ts (763 LOC, 82-88%):** 28 hand-crafted regex patterns for prompt injection (direct override, role manipulation, system prompt extraction, jailbreak keywords, code injection, data exfiltration), 8 jailbreak-specific patterns (DAN, unlimited mode, GPT-4 jailbreak), 6 PII categories (email, phone, SSN, credit card, IP, API keys), Unicode homoglyph normalization (8 Cyrillic lookalikes — genuine attack defense), control character sanitization, response-side injection compliance (6 response patterns). Clean middleware factory with Zod schema validation. **Not AI**: zero ML models, zero classifiers, zero embeddings. behaviorAnalysis() is a 4-feature stub (length/punct/caps/digits) with hardcoded 2.0 threshold and ephemeral in-memory baseline. enablePolicyVerification config flag has ZERO implementation (ghost feature). The declared `aidefence@^2.1.1` npm dependency is never imported (dead dependency).

**Test suite (280 LOC):** 37 tests across 10 describe blocks exercise regex patterns correctly but miss behavioral analysis entirely (0 test cases), PII false positive rates untested, config impact on blocking behavior untested. Tests reveal the module works as designed — the design is just rule-based, not AI.

**Integration: STANDALONE.** Zero imports from AgentDB, hooks, MCP, claude-flow, or any ruvector internal package. Only external dependency: Zod. Not excluded from npm publish (contrary to earlier speculation) — exported via root barrel, just missing `./security` subpath in exports map (design omission).

### 5r. CUDA-WASM Flash Attention (R92)

**MISPLACED CPU code — 88-92% algorithm quality, 0% CUDA.** flash_attention.rs (528 LOC) in cuda-wasm/src/runtime/ is a pure CPU Flash Attention v2 reference implementation. Zero cuBLAS, zero device memory allocation, zero GPU kernels, zero WASM bindgen. All computation on heap Vec<f32>.

Textbook-correct algorithm: outer Q-tile loop, inner KV-tile loop, online softmax (Milakov & Gimelshein 2018) with running m_i/l_i and correction factor rescaling, correct final normalization with logsumexp, causal masking via kv_limit + inner guard, SRAM budget formula matching paper Section 3.1: (bq×d + 2×bkv×d + bq×bkv)×4. FLOPs counter: 4×seq_q×seq_kv×d (verified by test). forward_multi_head is serial CPU loop over (batch × heads).

7 comprehensive tests: correctness vs naive reference (tolerance 0.1), causal NaN check, memory savings monotonicity, multi-head shapes, FLOPs exact match, dimension error, single-token identity.

**Role**: Almost certainly the algorithmic ground truth / reference implementation used to validate the 997-LOC GPU version in ruvector-mincut-gated-transformer. The "pure math" layer before GPU dispatch is wired.

## 6. Cross-Domain Dependencies

- **memory-and-learning domain**: ReasoningBank implementations (4 distinct), SONA, EWC++, embeddings, attention mechanisms
- **agentdb-integration domain**: AgentDB controllers, vector-quantization, LearningSystem, AttentionService
- **agentic-flow domain**: ReasoningBank, EmbeddingService, IntelligenceStore, learning-service
- **claude-flow-cli domain**: LocalReasoningBank (only one that runs), ruvector/ modules, model-router, semantic-router
- **sublinear-time-solver domain**: sparse.rs (BEST matrix code), consciousness integration

## 7. Knowledge Gaps

- **76 crates total** — only ~23 crates deep-read across 196 files (R91: +5 files)
- **ruvllm remaining** — 309 files, ~197K LOC unread
- **ruvector-graph distributed** — ADDRESSED by R90 (all 5 distributed files DEEP). Verdict: transport-absent protocol. Remaining gap: MVCC, optimizer, hybrid features.
- **ruvector-core advanced_features/** — ADDRESSED by R90 (PQ + conformal confirmed genuine). Remaining: other files in module.
- **ruvector-core advanced/** — ADDRESSED by R90 (hypergraph + tda.rs). Remaining: other files.
- **ruvector LLM extensions** — SUBSTANTIALLY ADDRESSED by R91+R93-R98. MinCut-Gated-Transformer now has 24 DEEP files. Remaining: transformer.rs (declared in lib.rs, unread), and any files under the `full` feature flag not yet reached.
- **npm packages** — 50+ packages, most unread
- **WASM targets** — 15+ WASM crates, micro-hnsw-wasm + hyperbolic-hnsw-wasm deep-read (R92)
- **router crate** — 4 crates (router-core, router-cli, router-ffi, router-wasm)
- **delta framework** — 5 crates (delta-core, delta-wasm, delta-index, delta-graph, delta-consensus)
- **cluster/replication** — distributed system crates
- **examples** — 34+ example projects
- **benchmark validation** — no standard ANN-Benchmark results (SIFT1M, GIST1M, Deep1M)

## 8. Session Log

### Initial (2026-02-09): Repository overview + deep dive of 11 crates
11 largest/most complex crates analyzed. Established completeness spectrum from 95% (temporal-tensor) to 30-35% (ruvector-graph). "2 million lines" claim debunked — actual ~400K Rust.

### C (2026-02-14): ruvector-core + ruvector-gnn deep-reads
Phase C Rust source examination. CRITICAL placeholder embeddings discovered. HNSW deletions broken. Real SIMD in simd_intrinsics.rs. ruvector-gnn custom hybrid confirmed.

### B (2026-02-14): ruvector-attention correction
66 files, ~9,200 LOC. Initial 45% completeness was WRONG — 18+ real implementations found. SIMD/Rayon features are no-ops but algorithms are genuine.

### R13 (2026-02-14): SONA + ruvector-core phase
~40 files across SONA, ruvector-gnn, ruvector-core. SONA 85% production-ready. Hash-based embeddings confirmed in Rust source.

### R22 (2026-02-15): ruvllm LLM inference + postgres SIMD/indexes
3 ruvllm files (8,824 LOC), 3 postgres files (6,291 LOC). BitNet backend 92-95%, SIMD 95-98% BEST IN ECOSYSTEM. HNSW neighbor connections EMPTY (CRITICAL). IVFFlat mutations stubbed.

### R28 (2026-02-15): sparse.rs sublinear matrix code
1 file, 964 LOC. 95% real — 4 sparse formats (CSR/CSC/COO/Graph), no_std. BEST matrix code in ecosystem.

### R34 (2026-02-15): ruvllm infrastructure + postgres SPARQL + mincut
10 files, ~8,300 LOC. memory_pool.rs 95% BEST systems code. SPARQL executor COMPLETE (corrects "no executor" verdict for graph domain). mincut wrapper 90% genuine arXiv:2512.13105.

### R35 (2026-02-15): ruvllm backends cluster
23 files, 26,454 LOC, 5-agent swarm. Architecture-complete, persistence-incomplete (SYSTEMIC). Kernel extensions 90.3% exceptional. Speculative decoding SLOWER than vanilla (CRITICAL). Scheduler 90-92% BEST serving code.

### R36 (2026-02-15): HNSW patches + nervous-system + neuro-divergent + cognitum + postgres healing
28 files, 26,569 LOC, 5-agent swarm. cognitum-gate-kernel 93% EXCEPTIONAL. HNSW patches 87% with CRITICAL FFI unsafe + no integrity validation. micro-hnsw-wasm neuromorphic features UNVALIDATED. Postgres healing 76% — real learning, stub execution.

### R37 (2026-02-15): ruvllm claude_flow bridge + training + ruQu + prime-radiant + temporal-tensor
25 files, 30,960 LOC, 5-agent swarm. Fourth ReasoningBank discovered. micro_lora.rs BEST learning code. ruQu QEC 89% genuine. prime-radiant 89% sheaf-theoretic coherence. temporal-tensor updated to 93% production-ready. Hash-based embeddings in Rust training confirmed systemic.

### R39 (2026-02-15): ruQu completion (tile.rs + planner.rs)
2 files, 3,603 LOC. Completes ruQu picture — 91.3% weighted real across all 7 files. HIGHEST QUALITY MULTI-FILE CRATE in ecosystem. Genuine quantum error correction throughout.

### R44 (2026-02-15): Edge-Net P2P Transport
3 files, 3,498 LOC, ~63 findings. **MAJOR REVERSAL**: p2p.rs (92-95%) has production libp2p with Gossipsub/Kademlia/RequestResponse, NOISE encryption, direct RAC integration via broadcast_rac_event() — edge-net IS a distributed system. advanced.rs (72%) is a MISNOMER — zero networking, actually ML primitives (SNN 95%, Raft 85%, HDC 93%, hash embeddings 8th occurrence). swarm.rs (72%) has excellent cryptographic protocol (Ed25519+AES-256-GCM) but 0% GUN network transport (all publish = stubs). Two parallel P2P architectures: edge-net uses libp2p (production), edge uses GUN relays (stubs).

### R54 (2026-02-16): ruQu Quantum Extended + Edge AI
8 files, ~10,080 LOC, ~170 findings. DEEP: 970→978. **Cluster A (ruQu Extended)**: filters.rs (82-86%) MISNAMED — coherence quality gate NOT quantum filtering, three-filter pipeline (structural/shift/evidence), 14 tests, zero connection to decoder.rs. fabric.rs (93-96%) production 256-tile WASM fabric orchestrator, Blake3 audit trails, surface code topology generator, 23 tests. **Cluster B (ruqu-core Extended)**: mitigation.rs (95-98%) three complete NISQ-era strategies (ZNE, Measurement Error, CDR), Richardson exact extrapolation, tensor-product calibration, 40+ tests at 1e-12. transpiler.rs (95-98%) complete 3-phase quantum circuit transpiler, 3 real hardware backends (IBM/IonQ/Rigetti), 44 tests, BEST-IN-CLASS. subpoly_decoder.rs (35-40%) **FALSE SUBPOLYNOMIAL** — 3rd instance. O(n²) greedy under "provable O(d^{2-ε})" claims. Zero citations. noise.rs (96-98%) BEST-IN-CLASS Kraus operator formalism, 4 noise channels + hardware calibration, 498 test lines. **Cluster C (Edge AI)**: lora.rs (90-95%) complete edge LoRA with dual SIMD (AVX2+WASM128), Q4/Q8 quantization, LRU adapter pool, independent from micro_lora.rs. federated.rs (95-98%) BEST federated learning — Byzantine-robust, differential privacy, TopK compression, reputation-weighted FedAvg. **Key findings**: ruQu contains TWO unrelated systems (QEC + coherence gate). Near-complete QC pipeline confirmed (noise→mitigation→transpiler→surface_code→decoder). Edge AI stack production-grade at 93-96% weighted. FALSE subpolynomial pattern now confirmed in 3 files across project.

### R52 (2026-02-16): Algorithmic Infrastructure Deep-Dive
6 files, ~10,271 LOC, 118 findings (26C, 30H, 31M, 31I). DEEP: 955→970. **Cluster A (HNSW patches)**: hnsw.rs (98-100%) is vendored upstream hnsw_rs v0.3.3, NOT a patch — zero ruvector modifications. CORRECTS R36 "fork" assessment. hnswio.rs (95-98%) BEST-IN-CLASS persistence — dual-file format, hybrid mmap, 4 versions, zero-copy. No postgres/AgentDB integration. All SIMD delegated to anndists crate. **Cluster B (Graph query)**: SPARQL parser.rs (93-95%) PRODUCTION W3C SPARQL 1.1 — all 4 query forms, property paths, 33+ functions, proper AST. Total SPARQL module 7,421 LOC. ruvector confirmed as multi-model DB (Cypher parser + SPARQL parser+executor). index_bench.rs (42%) THEATRICAL — HNSW search is brute-force O(n), zero postgres despite location. New facade category: "algorithmic mislabeling". **Cluster C (Subpoly+SIMD)**: subpolynomial/mod.rs (45-50%) FALSE complexity claims — invalid arXiv citation, O(log n) not O(n^{o(1)}), deterministic open problem. Same R39 false sublinearity pattern. simd.rs (92-95%) COMPLETE independent SIMD for NN inference — real AVX2/WASM/SSE4.1, Q4/Q8 quantization, numerically stable. TWO independent SIMD codebases in ruvector (core=distance, edge-net=inference).

### R90 (2026-02-17): ruvector Blind Spot — Distributed Graph + Core Advanced Features
9 files, ~4,959 LOC, 50 findings. DEEP: 1,323→1,332. Two clusters addressed long-standing gaps. **Cluster A (ruvector-graph distributed, 5 files, ~2,855 LOC)**: Establishes new "transport-absent distributed protocol" pattern class — algorithm state machines are correctly designed throughout, but all network sends are replaced with debug log comments. shard.rs (70-80%, BEST) has genuine EdgeCutMinimizer (multilevel Kernighan-Lin) and real xxh3/blake3 hashing. gossip.rs (45-55%) has correct SWIM type system and state tracking but no socket I/O. federation.rs (40-50%) real scatter-gather and merge logic but execute_on_cluster always returns empty. coordinator.rs (30-35%) 2PC state machine frozen at Active, naive string-based query planner. rpc.rs (15-20%) all 4 methods stubs, gRPC feature-gated out of default build. **Cluster B (ruvector-core advanced features, 4 files, ~2,104 LOC)**: product_quantization.rs (88-92%) RESOLVES H1 — complete k-means++ + Lloyd's + ADC LookupTable. conformal_prediction.rs (88-93%) valid split-conformal with Vovk et al. quantile, 3 nonconformity measures. hypergraph.rs (85-90%) genuine bipartite incidence, k-hop BFS, causal utility function, cites HyperGraphRAG (NeurIPS 2025). tda.rs (60-70%) MISLABELED (C25, 11th mislabeled file) — graph metrics masquerading as persistent homology. Quality gradient confirmed: core algorithms 92-98% > advanced_features 88-93% > graph distributed protocols 40-55% > graph transport 15-20%.

### R91 (2026-02-17): ruvector LLM Extensions — mmap, compress, speculative, rope, kv_cache/legacy
5 files, ~3,935 LOC, 9 findings (2C, 5H, positives). DEEP: 1,332→1,337. Cluster covers standalone LLM inference primitives in ruvector (distinct from ruvllm crate). mmap.rs (88-92%) genuine memmap2 + lock-free AtomicBitmap for GNN training; no HNSW integration; pin count unused (no eviction, H30). compress.rs (55-65%) is the 12th MISLABELED FILE (C28) — "graph compression" is embedding/tensor quantization; fake IEEE f16 by ×1000 fixed-point (C29); PQ codebook is trivial linear interpolation not k-means (H32); binary quantization correct. speculative.rs (88-92%) EAGLE-style tree decoding with novel lambda-guided confidence from mincut signal; textbook rejection sampling correct; sequential path verification negates parallel tree benefit (H31). rope.rs (88-92%) faithful RoPE + NTK-aware scaling + partial YaRN; Q15 path; independent from ruvllm/kernels/rope.rs. kv_cache/legacy.rs (82-88%) RotateKV + FWHT + correct 2/4-bit quantization; per-head scale stomping bug corrupts multi-token dequantization (H28); no eviction policy (H29). Weighted avg ~83%. Cumulative mislabeled files: 12.

### R92 (2026-02-17): Reality-Check Uncovered Areas — AIDefence + Hyperbolic HNSW + CUDA Flash Attention
9 files, ~4,760 LOC, 112 findings. DEEP: 1,339→1,348. Three UNCOVERED verdicts from README-REALITY-CHECK resolved. **Cluster A (AIDefence)**: AIDefenceGuard.ts (82-88%) PARTIALLY REAL — 28 genuine regex patterns, zero ML, behavioral analysis is 4-feature stub, policy verification ghost feature (C31), declared `aidefence` npm dep never imported. Tests MOCKED — exercise regex only. aidefence-integration.ts SIMULATION-ONLY. **Cluster B (Hyperbolic HNSW, 88-95%)**: First DEEP reads on 63-file crate. hnsw.rs (88-93%) NATIVE implementation (not hnsw_rs), Poincaré ball distance, tangent-space two-phase pruning, DualSpaceIndex RRF. poincare.rs (90-95%) ALL formulas correct (Ganea 2018). lorentz_cascade.rs (90-93%) GENUINE Lorentz model attention — Minkowski metric, Busemann functions, multi-curvature cascade. WASM lib.rs (88-92%) = 17th GENUINE WASM. shard.rs (82-85%) confirms transport-absent pattern with hyperbolic-aware radius partitioning. **Cluster C (CUDA Flash Attention)**: flash_attention.rs (88-92% algorithm, 0% CUDA) — CPU reference impl MISPLACED in cuda-wasm crate (C30). Updated WASM count: 17 genuine, 13 theatrical.

### R93 (2026-02-17): MinCut-Gated-Transformer Core — FFN, Quantization, KV Cache, Attention Variants, Energy Gate
9 files, ~5,421 LOC, 69 findings (0C, 15H, positives). DEEP: 1,348→1,357. **R34 "MOST NOVEL" CONFIRMED**: energy_gate.rs (88-93%) is a genuine 3-component EBT energy function with central-difference gradient and System-2 refinement — not a renamed threshold. Computational kernels (qgemm.rs 88-93%, ffn.rs 88-92%, spike_driven.rs 88-93%, q15.rs 88-92%) all genuine with real AVX2+NEON SIMD. early_exit.rs (88-92%) implements novel lambda-stability-driven early exit as LayerSkip alternative with 40/40/20 confidence weighting. Infrastructure layers lower: manager.rs (72-78%) pure FIFO eviction, sparse_attention.rs (72-80%) mincut-aware design with placeholder partitions, kvquant.rs (78-85%) genuine pre-RoPE KVQuant but 3-bit broken. Key architectural finding: Q15 u16/i16 type mismatch between q15.rs and rope.rs means quantized paths disconnected. FFN is vanilla GELU despite "Gated-Transformer" name — gating comes from energy/mincut mechanism. Weighted avg ~86%. Quality gradient: novel algorithms + kernels (88-93%) > infrastructure (72-80%).

### R94 (2026-02-17): MinCut Infrastructure + GNN Training Components
9 files, ~4,569 LOC, ~70 findings (0C, 9H). DEEP: 1,357→1,366. **Cluster A (MinCut infrastructure, 7 files, ~3,534 LOC, ~83%)**: packets.rs (90-95%) BEST — pure coherence interface with novel QuarantineUpdates isolation mode, consistent Q15 repr(C). state.rs (88-92%) genuine zero-alloc buffer layout, inference-only. quantized_store.rs (88-92%) two-tier KIVI (4-bit/2-bit), correct per KIVI paper, orthogonal to kvquant.rs. mod_routing.rs (72-78%) MoD (NOT MoE) — deterministic lambda-delta routing with duplicate functions (H52) and stride heuristic boundary detection (H53). window.rs (82-87%) causal sliding window (NOT Longformer despite docstring), scalar attention kernels. metrics.rs (78-82%) PPL tracker not cache hit-rates (H59), dead code paths. quant4.rs (72-78%) RTN not GPTQ/AWQ, fully scalar, BlockInt4 write-only (H56). **Cluster B (GNN training, 2 files, ~1,035 LOC, ~86%)**: scheduler.rs (82-88%) 5 correct LR algorithms (CosineAnnealing SGDR, WarmupLinear, ReduceOnPlateau), zero GNN integration. replay.rs (88-92%) correct Vitter/Knuth reservoir sampling, Welford online stats, false KL claim (H60). Both fall GENUINE side of R91 bimodal split. Quality gradient holds: coherence types (90-95%) > runtime state (88-92%) > attention (82-87%) > infrastructure (72-82%).

### R96 (2026-02-17): MinCut KV Cache Layer + SQL Attention + MinCut Trace
8 files, ~3,545 LOC, 95 findings (1C, 20H, 32M, 42I). DEEP: 1,366→1,374. **Cluster A (MinCut KV cache, 5 files, ~2,256 LOC, ~83%)**: arena.rs (92%) genuine bump allocator with 64B cache-line alignment purpose-built for QKV+FFN weight layout; aliasing UB hazard from multiple &mut slices off same Vec (H60a). squat.rs (78-84%) GENUINE SQuAt 2024 paper — Hadamard+Gram-Schmidt math correct, 2/4-bit packing complete, but calibrate() STUB ignoring calibration data (H60b). kivi.rs (72-78%) GENUINE FWHT but per-channel quantization FALSE — global min/max regardless of QuantScheme (H60c), SIMD dequantize TODO (H60d); complementary primitive to quantized_store.rs. policy.rs (82-87%) age-based tier graduation only (H60j), RematerializationPolicy pressure divergence (H60k). hot_buffer.rs (82-87%) FIFO "FP16" (actually f32); pop_oldest() BUG — no read cursor (H60e); dual push API trap. **Cluster B (SQL Attention, ruvector-postgres, 2 files, ~814 LOC)**: operators.rs (88-92%) GENUINE pgx extension — 5 #[pg_extern] functions CORRECTING R91 AttentionService.ts skepticism; only 3/10 types SQL-dispatchable (H60f); 6 genuine pg_test tests. flash.rs (72-78%) genuine online softmax algorithm but NOT a pgx function (H60g); false O(sqrt(N)) space claim (H60h); block_size_q dead_code stub. **Cluster C (MinCut trace, 1 file, ~413 LOC)**: trace.rs (88-92%) gate-decision diagnostics with stack-allocated [T;64] circular buffer; feature gate claim documentation-only (H60i); 6 genuine tests. MinCut-Gated-Transformer crate now 27 DEEP files.

### R97 (2026-02-17): Prime-Radiant Hyperbolic + SQL Attention + MinCut Remaining
9 files, ~3,055 LOC, 93 findings (0C, 9H, 22M, 51I). DEEP: 1,366→1,375. Three clusters. **Cluster A (Prime-Radiant Hyperbolic, 3 files, ~1,048 LOC)**: hyperbolic/mod.rs (88-92%) genuine Poincare geometry, brute-force O(n) search, disconnected from sheaf substrate. hyperbolic/energy.rs (82-87%) pure data container, curvature stored but unused, merge() silently corrupts cross-curvature data (H63). hyperbolic/adapter.rs (78-83%) Poincare-only despite "adapter" name (H61), HNSW documented stub (H62), exp_map uses Euclidean addition not Mobius addition (H64). Contrast with ruvector-hyperbolic-hnsw (R92, 88-93%): dedicated crate has native HNSW, no exp_map bug. **Cluster B (SQL Attention, ruvector-postgres, 3 files, ~967 LOC)**: multi_head.rs (88-92%) genuine Rayon parallel MHA, no W_Q/W_K/W_V projections. scaled_dot.rs (90-93%) correct QK^T/sqrt(d_k), real simsimd SIMD. mod.rs (82-87%) genuine pgrx PostgresEnum but inflated "39 mechanisms" — only 10 enum variants (H66). **Cluster C (MinCut remaining, 3 files, ~1,040 LOC)**: spike.rs (88-92%) scheduling layer (NOT neuron dynamics), Q15 rate-tiering, FNV-1a novelty hashing. config.rs (88-92%) covers only ~40% subsystem surface — five of nine DEEP subsystems unconfigurable (H67). kv_cache/tier.rs (90-93%) clean 3-tier definitions, age-based not access-frequency tier assignment (H68). Weighted avg ~89%.

### R98 (2026-02-17): SQL Hyperbolic + MinCut Crate Root + norm.rs + Prime-Radiant Hyperbolic Module Complete
6 files, ~1,385 LOC, 7 HIGH findings (H70-H76). DEEP: 1,375→1,381. Four clusters. **Cluster A (SQL Hyperbolic, ruvector-postgres, 2 files, ~526 LOC)**: poincare.rs (88-92%) genuine Poincare ball math — distance, Mobius addition, exp/log maps with correct curvature scaling and EPSILON/MAX_NORM guards, 13 tests; SQL via operators.rs (pure Rust library layer). lorentz.rs (87-92%) correct Lorentz/Minkowski model with bidirectional Poincare↔Lorentz coordinate transforms and SIMD dot product; CRITICAL H70 (no manifold validation — off-hyperboloid points accepted silently), H71 (no pgx annotations, no direct SQL exposure). SQL hyperbolic arc now complete. **Cluster B (MinCut crate root, 1 file, 261 LOC)**: lib.rs (90-95% EXCELLENT) — 23 public modules, 60+ exported items, 10 feature flags, 43-item prelude; confirms ALL 22+ DEEP modules from R93-R97 publicly accessible, no hidden silos; dual KV cache re-export (legacy+new). MinCut crate now 24 DEEP files. **Cluster C (MinCut norm.rs, 1 file, 213 LOC)**: norm.rs (55-65% WEAKEST MinCut kernel) — correct LayerNorm+RMSNorm math but ALL pure scalar (H72), RMSNorm feature-flag duplication is dead code (H73); 6 tests. Quality gradient now spans 55-95% within the crate. **Cluster D (Prime-Radiant Hyperbolic, 2 files, ~385 LOC)**: depth.rs (78-82%) correct Poincare depth formula but curvature param never modulates (H74), hardcoded level thresholds (H75). config.rs (75-82%) HNSW params dead config (H76), Euclidean defaults. Prime-Radiant Hyperbolic MODULE COMPLETE: 5/5 files DEEP, weighted avg ~81%.

### R99 (2026-02-17): Hyperbolic HNSW Crate Complete + GNN Bindings (node + wasm)
3 files, ~1,192 LOC, 27 findings (0C, 4H, 3M, 20I). DEEP: 1,381→1,384. Two clusters. **Cluster A (ruvector-hyperbolic-hnsw, 1 file, 349 LOC)**: tangent.rs (88-92%) implements TangentPruner — the two-phase Poincare pruning strategy referenced in hnsw.rs (R92). TangentCache computes Fréchet mean centroid correctly via iterative Riemannian GD using poincare.rs log_map/exp_map. Pruning: tangent-space Euclidean filter reduces arcosh call count, then exact Poincare re-rank for final top-N. All poincare.rs imports consistent — crate is purely Poincare ball (no model mixing). search() is O(N) linear scan by design (H77) — designed to receive HNSW candidates from caller, but no caller wires HNSW output in. Dead import: norm_squared (H78). 2 genuine tests. **ruvector-hyperbolic-hnsw CRATE NOW COMPLETE** (5/5 DEEP files, ~88% weighted avg, internally consistent). **Cluster B (GNN bindings, 2 files, ~843 LOC)**: gnn-node/lib.rs (88-92%) genuine napi-rs with #[napi]/#[napi(constructor)]/#[napi(factory)]/#[napi(object)] macros, solid error handling (Status::InvalidArg for ranges, .map_err + Status::GenericFailure), imports 6 real core symbols. hierarchical_forward() JSON per-call serde overhead (H79). gnn-wasm/lib.rs (90-94%) **18th GENUINE WASM** — all exports delegate to real ruvector_gnn core via serde_wasm_bindgen, correct inline cosineSimilarity() (95%), proper console_error_panic_hook init. Both crates are inference-only: training APIs (train/backprop/update_weights) are absent from FFI surface despite existing in core crate (H80). WASM ratio now 18 genuine / 13 theatrical = 58% genuine.

### R100 (2026-02-17): Attention Hyperbolic Module + Hyperbolic-HNSW Library Root + MinCut Attention/Error + Postgres Hyperbolic Mod
12 files, ~874 LOC across 5 clusters. DEEP: 1,384→1,396. **Cluster A (ruvector-attention hyperbolic, 4 files, ~620 LOC, 90-93% avg)**: hyperbolic_attention.rs (88-92%) fully working Poincare-ball dot-product attention with Fréchet mean aggregation; adaptive_curvature dead field (H81). mixed_curvature.rs (90-94%) MOST ARCHITECTURALLY NOVEL — [euclidean || hyperbolic] embedding split with independent attention paths and ad-hoc softmax blend (H82); unenforced API contract (M5). poincare.rs (93-96%) best-quality Gyrovector math in the attention crate — all formulas correct with comprehensive numerical guards; fixed frechet_mean learning rate may oscillate at high curvature (M4). mod.rs (100% wiring) confirms 4-submodule re-export with lorentz_cascade as first-class hyperbolic mechanism. **ruvector-attention hyperbolic module now 4/4 DEEP**. **Cluster B (ruvector-hyperbolic-hnsw library root, 2 files, ~254 LOC)**: lib.rs (90-93%) exposes 5 submodules with tangent-space pruning and per-shard curvature; no shard bounds check (H84). error.rs (95%+) clean 8-variant thiserror enum with programmatic recovery fields — OutsideBall carries norm+curvature for retry logic. **Cluster C (MinCut KV cache module root, 1 file, 98 LOC)**: kv_cache/mod.rs (~90%) confirms ADR-004 Three-Tier Adaptive KV cache (FP16 hot → 4-bit KIVI warm → 2-bit archive) with backward-compat legacy re-export; pure module root, all logic in children. **Cluster D (MinCut error + attention, 3 files, ~197 LOC)**: error.rs (~92%) hard no-panic contract with is_recoverable()/is_config_error() — HIGH quality. attention/linear.rs (15-20%) **LINEAR ATTENTION PLACEHOLDER** — docstring cites Katharopoulos 2020 but NO forward pass, NO ELU+1, hidden behind feature gate (H83); deflates O(n) attention claims. attention/mod.rs (~88%) feature-gated architecture confirms default = SlidingWindowAttention only; 4 academic citations; MinCut sparse path real but non-default. kernel/mod.rs (~90%) pure re-export root for bench_utils/norm/qgemm/quant4; benchmark utilities built into kernel. **Cluster E (ruvector-postgres hyperbolic/mod.rs, 1 file, 31 LOC)**: mod.rs (~92%) clean module root with correct constants (DEFAULT_CURVATURE=-1.0, EPSILON=1e-8, MAX_NORM=1.0-1e-5); operators.rs kept internal (not pub use-d). Completes SQL hyperbolic arc started in R98. MinCut crate now has ~30 DEEP files.

### R101 (2026-02-17): v4-priority Clearance + Postgres Healing + Postgres GNN Layer
9 files, ~2,618 LOC, 72 findings (0C, 10H). DEEP: ~1,421→~1,430. **v4-priority domain CLEARED.** Three arc completions: postgres hyperbolic/ (4/4 DEEP, 92-95% — operators.rs adds 8 pg_extern functions; zero manifold validation at any layer H85), healing/ (7/7 DEEP, 88-93% — functions.rs 17 SQL functions, mod.rs OnceLock singleton H87, OutcomeTracker diverging state H88), MinCut kernel/ (bench_utils.rs 80-85% last file — pure scaffolding, zero SIMD invocations, three timer bugs). **Postgres GNN layer OPENED**: TWO parallel GNN ecosystems confirmed — ruvector-postgres/gnn is self-contained reimplementation (82-92%), NOT ruvector-gnn composition. ruvector_message_pass is FACADE (H90). gcn.rs missing self-loops (H93). No trained weight loading (H94). ruvector-gnn error.rs (95%+) and mmap_fixed.rs AtomicBitmap genuine but MmapManager ABSENT (H86).
